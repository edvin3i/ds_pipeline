<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" type="text/javascript"
  data-document-language="true" charset="UTF-8" data-domain-script="3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5">
</script>
<script type="text/javascript">
  function OptanonWrapper() {
    var event = new Event('bannerLoaded');
    window.dispatchEvent(event);
  }
</script>
<script src="https://images.nvidia.com/aem-dam/Solutions/ot-js/ot-custom.js" type="text/javascript">
</script>

  <meta charset="utf-8">
<meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/">
<meta content="The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs." name="description">
<meta content="CUDA RDMA, CUDA RDMA GPU, CUDA RDMA GPU direct, CUDA RDMA DMA, CUDA RDMA GPUDirect transfer, CUDA RDMA systems, CUDA RDMA GPU memory, CUDA RDMA free callback, CUDA RDMA link kernel module, CUDA RDMA references" name="keywords">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>GPUDirect RDMA</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css">
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css">
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css">
      <link rel="stylesheet" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css">
      <link rel="stylesheet" href="../_static/omni-style.css" type="text/css">
      <link rel="stylesheet" href="../_static/api-styles.css" type="text/css">
    <link rel="shortcut icon" href="../_static/favicon.ico">
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/mermaid-init.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/design-tabs.js"></script>
        <script src="../_static/geoip/geoip.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html">
    <link rel="search" title="Search" href="search.html">
    <link rel="prev" title="Contents" href="contents.html">
 
<script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>
 

<link rel="stylesheet" href="../../../../common-libs/common.css">
<script src="../../../../common-libs/common.js"></script>
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="contents.html">
            <img src="../_static/Logo_and_CUDA.png" class="logo" alt="Logo">
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs">
    <input type="hidden" name="check_keywords" value="yes">
    <input type="hidden" name="area" value="default">
  </form>
</div>
        </div>
<div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current">
<a class="current reference internal" href="index.html#">1. Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-gpudirect-rdma-works">1.1. How GPUDirect RDMA Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#standard-dma-transfer">1.2. Standard DMA Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#gpudirect-rdma-transfers">1.3. GPUDirect RDMA Transfers</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#changes-in-cuda-6-0">1.4. Changes in CUDA 6.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#changes-in-cuda-7-0">1.5. Changes in CUDA 7.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#changes-in-cuda-8-0">1.6. Changes in CUDA 8.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#changes-in-cuda-10-1">1.7. Changes in CUDA 10.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#changes-in-cuda-11-2">1.8. Changes in CUDA 11.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#changes-in-cuda-11-4">1.9. Changes in CUDA 11.4</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#changes-in-cuda-12-2">1.10. Changes in CUDA 12.2</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="index.html#design-considerations">2. Design Considerations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#lazy-unpinning-optimization">2.1. Lazy Unpinning Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#registration-cache">2.2. Registration Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#unpin-callback">2.3. Unpin Callback</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#supported-systems">2.4. Supported Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#pci-bar-sizes">2.5. PCI BAR sizes</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tokens-usage">2.6. Tokens Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#synchronization-and-memory-ordering">2.7. Synchronization and Memory Ordering</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="index.html#how-to-perform-specific-tasks">3. How to Perform Specific Tasks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#displaying-gpu-bar-space">3.1. Displaying GPU BAR space</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#pinning-gpu-memory">3.2. Pinning GPU memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#unpinning-gpu-memory">3.3. Unpinning GPU memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#handling-the-free-callback">3.4. Handling the free callback</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#buffer-id-tag-check-for-a-registration-cache">3.5. Buffer ID Tag Check for A Registration Cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#linking-a-kernel-module-against-nvidia-ko">3.6. Linking a Kernel Module against nvidia.ko</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-nvidia-peermem">3.7. Using nvidia-peermem</a></li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="index.html#references">4. References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#basics-of-uva-cuda-memory-management">4.1. Basics of UVA CUDA Memory Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#userspace-api">4.2. Userspace API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#kernel-api">4.3. Kernel API</a></li>
<li class="toctree-l2">
<a class="reference internal" href="index.html#porting-to-tegra">4.4. Porting to Tegra</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#changing-the-allocator">4.4.1. Changing the allocator</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#modification-to-kernel-api">4.4.2. Modification to Kernel API</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#other-highlights">4.4.3. Other highlights</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="index.html#notices">5. Notices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#notice">5.1. Notice</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#opencl">5.2. OpenCL</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#trademarks">5.3. Trademarks</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="contents.html">GPUDirect RDMA</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">


  

<li>
<a href="../index.html" class="icon icon-home"></a> »</li>
  
<li>
<span class="section-number">1. </span>Overview</li>

      <li class="wy-breadcrumbs-aside">
      </li>
<li class="wy-breadcrumbs-aside">


  <span>v12.6 |</span>



  <a href="../pdf/GPUDirect_RDMA.pdf" class="reference external">PDF</a>



  <span>|</span>



  <a href="https://developer.nvidia.com/cuda-toolkit-archive" class="reference external">Archive</a>


  <span> </span>
</li>

  </ul>
  <hr>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p class="rubric-h1 rubric">Developing a Linux Kernel Module using GPUDirect RDMA</p>
<p>The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs.</p>
<section id="overview">
<h1>
<span class="section-number">1. </span>Overview<a class="headerlink" href="index.html#overview" title="Permalink to this headline"></a>
</h1>
<p>GPUDirect RDMA is a technology introduced in Kepler-class GPUs and CUDA 5.0 that enables a direct path for data exchange between the GPU and a third-party peer device using standard features of PCI Express. Examples of third-party devices are: network interfaces, video acquisition devices, storage adapters.</p>
<p>GPUDirect RDMA is available on both Tesla and Quadro GPUs.</p>
<p>A number of limitations can apply, the most important being that the two devices must share the same upstream PCI Express root complex. Some of the limitations depend on the platform used and could be lifted in current/future products.</p>
<p>A few straightforward changes must be made to device drivers to enable this functionality with a wide range of hardware devices. This document introduces the technology and describes the steps necessary to enable an GPUDirect RDMA connection to NVIDIA GPUs on Linux.</p>
<figure class="align-center" id="overview-gpudirect-rdma-within-linux-device-driver-model">
<img alt="GPUDirect RDMA within the Linux Device Driver Model" src="_images/gpudirect-rdma-within-linux-device-driver-model.png">
<figcaption>
<p><span class="caption-text">GPUDirect RDMA within the Linux Device Driver Model</span><a class="headerlink" href="index.html#overview-gpudirect-rdma-within-linux-device-driver-model" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<section id="how-gpudirect-rdma-works">
<span id="id1"></span><h2>
<span class="section-number">1.1. </span>How GPUDirect RDMA Works<a class="headerlink" href="index.html#how-gpudirect-rdma-works" title="Permalink to this headline"></a>
</h2>
<p>When setting up GPUDirect RDMA communication between two peers, all physical addresses are the same from the PCI Express devices’ point of view. Within this physical address space are linear windows called PCI BARs. Each device has six BAR registers at most, so it can have up to six active 32bit BAR regions. 64bit BARs consume two BAR registers. The PCI Express device issues reads and writes to a peer device’s BAR addresses in the same way that they are issued to system memory.</p>
<p>Traditionally, resources like BAR windows are mapped to user or kernel address space using the CPU’s MMU as memory mapped I/O (MMIO) addresses. However, because current operating systems don’t have sufficient mechanisms for exchanging MMIO regions between drivers, the NVIDIA kernel driver exports functions to perform the necessary address translations and mappings.</p>
<p>To add GPUDirect RDMA support to a device driver, a small amount of address mapping code within the kernel driver must be modified. This code typically resides near existing calls to <code class="docutils literal notranslate"><span class="pre">get_user_pages()</span></code>.</p>
<p>The APIs and control flow involved with GPUDirect RDMA are very similar to those used with standard DMA transfers.</p>
<p>Refer to <a class="reference internal" href="index.html#supported-systems"><span class="std std-ref">Supported Systems</span></a> and <a class="reference internal" href="index.html#pci-bar-sizes"><span class="std std-ref">PCI BAR sizes</span></a> for more hardware details.</p>
</section>
<section id="standard-dma-transfer">
<span id="id2"></span><h2>
<span class="section-number">1.2. </span>Standard DMA Transfer<a class="headerlink" href="index.html#standard-dma-transfer" title="Permalink to this headline"></a>
</h2>
<p>First, we outline a standard DMA Transfer initiated from userspace. In this scenario, the following components are present:</p>
<ul class="simple">
<li><p>Userspace program</p></li>
<li><p>Userspace communication library</p></li>
<li><p>Kernel driver for the device interested in doing DMA transfers</p></li>
</ul>
<p>The general sequence is as follows:</p>
<ol class="arabic simple">
<li><p>The userspace program requests a transfer via the userspace communication library. This operation takes a pointer to data (a virtual address) and a size in bytes.</p></li>
<li><p>The communication library must make sure the memory region corresponding to the virtual address and size is ready for the transfer. If this is not the case already, it has to be handled by the kernel driver (next step).</p></li>
<li><p>The kernel driver receives the virtual address and size from the userspace communication library. It then asks the kernel to translate the virtual address range to a list of physical pages and make sure they are ready to be transferred to or from. We will refer to this operation as pinning the memory.</p></li>
<li><p>The kernel driver uses the list of pages to program the physical device’s DMA engine(s).</p></li>
<li><p>The communication library initiates the transfer.</p></li>
<li><p>After the transfer is done, the communication library should eventually clean up any resources used to pin the memory. We will refer to this operation as unpinning the memory.</p></li>
</ol>
</section>
<section id="gpudirect-rdma-transfers">
<h2>
<span class="section-number">1.3. </span>GPUDirect RDMA Transfers<a class="headerlink" href="index.html#gpudirect-rdma-transfers" title="Permalink to this headline"></a>
</h2>
<p>For the communication to support GPUDirect RDMA transfers some changes to the sequence above have to be introduced. First of all, two new components are present:</p>
<ul class="simple">
<li><p>Userspace CUDA library</p></li>
<li><p>NVIDIA kernel driver</p></li>
</ul>
<p>As described in <a class="reference internal" href="index.html#basics-of-uva-cuda-memory-management"><span class="std std-ref">Basics of UVA CUDA Memory Management</span></a>, programs using the CUDA library have their address space split between GPU and CPU virtual addresses, and the communication library has to implement two separate paths for them.</p>
<p>The userspace CUDA library provides a function that lets the communication library distinguish between CPU and GPU addresses. Moreover, for GPU addresses it returns additional metadata that is required to uniquely identify the GPU memory represented by the address. Refer to  <a class="reference internal" href="index.html#userspace-api"><span class="std std-ref">Userspace API</span></a> for details.</p>
<p>The difference between the paths for CPU and GPU addresses is in how the memory is pinned and unpinned. For CPU memory this is handled by built-in Linux Kernel functions (<code class="docutils literal notranslate"><span class="pre">get_user_pages()</span></code> and <code class="docutils literal notranslate"><span class="pre">put_page()</span></code>). However, in the GPU memory case the pinning and unpinning has to be handled by functions provided by the NVIDIA Kernel driver. See <a class="reference internal" href="index.html#pinning-gpu-memory"><span class="std std-ref">Pinning GPU memory</span></a> and <a class="reference internal" href="index.html#unpinning-gpu-memory"><span class="std std-ref">Unpinning GPU memory</span></a> for details.</p>
<p>Some hardware caveats are explained in <a class="reference internal" href="index.html#supported-systems"><span class="std std-ref">Supported Systems</span></a> and <a class="reference internal" href="index.html#pci-bar-sizes"><span class="std std-ref">PCI BAR sizes</span></a>.</p>
</section>
<section id="changes-in-cuda-6-0">
<h2>
<span class="section-number">1.4. </span>Changes in CUDA 6.0<a class="headerlink" href="index.html#changes-in-cuda-6-0" title="Permalink to this headline"></a>
</h2>
<p>In this section we briefly list the changes that are available in CUDA 6.0:</p>
<ul class="simple">
<li><p>CUDA peer-to-peer tokens are no longer mandatory. For memory buffers owned by the calling process (which is typical) tokens can be replaced by zero (0) in the kernel-mode function <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code>. This new feature is meant to make it easier for existing third party software stacks to adopt RDMA for GPUDirect.</p></li>
<li><p>As a consequence of the change above, a new API <code class="docutils literal notranslate"><span class="pre">cuPointerSetAttribute()</span></code> has been introduced. This API must be used to register any buffer for which no peer-to-peer tokens are used. It is necessary to ensure correct synchronization behavior of the CUDA API when operation on memory which may be read by RDMA for GPUDirect. Failing to use it in these cases may cause data corruption. See changes in <a class="reference internal" href="index.html#tokens-usage"><span class="std std-ref">Tokens Usage</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> has been extended to return a globally unique numeric identifier, which in turn can be used by lower-level libraries to detect buffer reallocations happening in user-level code (Refer to  <a class="reference internal" href="index.html#userspace-api"><span class="std std-ref">Userspace API</span></a>). It provides an alternative method to detect reallocations when intercepting CUDA allocation and deallocation APIs is not possible.</p></li>
<li><p>The kernel-mode memory pinning feature has been extended to work in combination with Multi-Process Service (MPS).</p></li>
</ul>
<p>Caveats as of CUDA 6.0:</p>
<ul class="simple">
<li><p>CUDA Unified Memory is not explicitly supported in combination with GPUDirect RDMA. While the page table returned by <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> is valid for managed memory buffers and provides a mapping of GPU memory at any given moment in time, the GPU device copy of that memory may be incoherent with the writable copy of the page which is not on the GPU. Using the page table in this circumstance may result in accessing stale data, or data loss, because of a DMA write access to device memory that is subsequently overwritten by the Unified Memory run-time. <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> may be used to determine if an address is being managed by the Unified Memory runtime.</p></li>
<li><p>Every time a device memory region is pinned, new GPU BAR space is allocated unconditionally, even when pinning overlapping or duplicate device memory ranges, i.e. there is no attempt at reusing mappings. This behavior has been changed since CUDA 7.0.</p></li>
</ul>
</section>
<section id="changes-in-cuda-7-0">
<h2>
<span class="section-number">1.5. </span>Changes in CUDA 7.0<a class="headerlink" href="index.html#changes-in-cuda-7-0" title="Permalink to this headline"></a>
</h2>
<p>In this section we briefly list the changes that are available in CUDA 7.0:</p>
<ul class="simple">
<li><p>On the IBM POWER8 platform, GPUDirect RDMA is not supported, though it is not explicitly disabled.</p></li>
<li><p>GPUDirect RDMA is not guaranteed to work on any given ARM64 platform.</p></li>
<li><p>Management of GPU BAR mappings has been improved with respect to CUDA 6.0. Now when a device memory region is pinned, GPU BAR space might be shared with pre-existing mappings. This is the case for example when pinning overlapping or duplicate device memory ranges. As a consequence, when unpinning a region, its whole BAR space will not be returned if even only a subset of its BAR space is shared.</p></li>
<li><p>The new <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttributes()</span></code> API has been introduced. It can be useful when retrieving multiple attributes for the same buffer, e.g. in MPI when examining a new buffer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cudaPointerGetAttributes()</span></code> is now faster since it leverages <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttributes()</span></code> internally.</p></li>
<li><p>A new sample code, <code class="docutils literal notranslate"><span class="pre">samples/7_CUDALibraries/cuHook</span></code>, has been added in CUDA 6.5. It can be used as a template for implementing an interception framework for CUDA memory de/allocation APIs.</p></li>
</ul>
</section>
<section id="changes-in-cuda-8-0">
<h2>
<span class="section-number">1.6. </span>Changes in CUDA 8.0<a class="headerlink" href="index.html#changes-in-cuda-8-0" title="Permalink to this headline"></a>
</h2>
<p>In this section we briefly list the changes that are available in CUDA 8.0:</p>
<ul class="simple">
<li><p>The nvidia_p2p_page_table struct has been extended to include a new member, without breaking binary compatibility. The minor version in the NVIDIA_P2P_PAGE_TABLE_VERSION macro has been updated accordingly.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_dma_mapping</span></code> structure, the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_dma_map_pages()</span></code> and <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_dma_unmap_pages()</span></code> APIs, the <code class="docutils literal notranslate"><span class="pre">NVIDIA_P2P_DMA_MAPPING_VERSION</span></code> macro have been introduced. These APIs can be used by third party device drivers to map and unmap the GPU BAR pages into their device’s I/O address space. The main use case is on platforms where the I/O addresses of PCIe resources, used for PCIe peer-to-peer transactions, are different from the physical addresses used by the CPU to access those same resources. See this <a class="reference external" href="https://github.com/Mellanox/nv_peer_memory/commit/a313b8beab5403339c0740afa3bea720b92dc2b7">link</a> for an example of code using these new APIs.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE</span></code> and <code class="docutils literal notranslate"><span class="pre">NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE</span></code> macros have been introduced. These are meant to be called by third-party device drivers to check for runtime binary compatibility, for example in case of changes to the data structure’s layout.</p></li>
<li><p>On the IBM POWER8 platform, when using the above APIs, GPUDirect RDMA is reported to work correctly restricted to the case where the GPU and the third party device are connected through a supported PCIe switch.</p></li>
</ul>
</section>
<section id="changes-in-cuda-10-1">
<h2>
<span class="section-number">1.7. </span>Changes in CUDA 10.1<a class="headerlink" href="index.html#changes-in-cuda-10-1" title="Permalink to this headline"></a>
</h2>
<p>GPUDirect RDMA is supported on Jetson AGX Xavier platform. Refer to the <a class="reference internal" href="index.html#porting-to-tegra"><span class="std std-ref">Porting to Tegra</span></a> section for details.</p>
</section>
<section id="changes-in-cuda-11-2">
<h2>
<span class="section-number">1.8. </span>Changes in CUDA 11.2<a class="headerlink" href="index.html#changes-in-cuda-11-2" title="Permalink to this headline"></a>
</h2>
<p>GPUDirect RDMA is supported on Drive AGX Xavier Linux based platform. Refer to the <a class="reference internal" href="index.html#porting-to-tegra"><span class="std std-ref">Porting to Tegra</span></a> section for details.</p>
</section>
<section id="changes-in-cuda-11-4">
<h2>
<span class="section-number">1.9. </span>Changes in CUDA 11.4<a class="headerlink" href="index.html#changes-in-cuda-11-4" title="Permalink to this headline"></a>
</h2>
<p>Added a new a kernel module, <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code>, which provides Mellanox InfiniBand-based HCAs (Host Channel Adapters) direct peer-to-peer read and write access to the NVIDIA GPU’s video memory. Refer to <a class="reference internal" href="index.html#using-nvidia-peermem"><span class="std std-ref">Using nvidia-peermem</span></a> for details.</p>
<p>GPUDirect RDMA is supported on Jetson Orin platform. Refer to the <a class="reference internal" href="index.html#porting-to-tegra"><span class="std std-ref">Porting to Tegra</span></a> section for details.</p>
<p><strong>Known Issue:</strong></p>
<p>Currently, there is no service to automatically load <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code>. Users need to load the module manually.</p>
</section>
<section id="changes-in-cuda-12-2">
<h2>
<span class="section-number">1.10. </span>Changes in CUDA 12.2<a class="headerlink" href="index.html#changes-in-cuda-12-2" title="Permalink to this headline"></a>
</h2>
<p>In drivers released from the R515 up to the R535 branches, except for newer R525 and R535 releases mentioned below, there is a race bug which may show up as a kernel null-pointer dereference. This happens when the GPU invokes the (hereby I/O) kernel driver invalidation callback, the one which was registered during the call to <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages</span></code>, concurrently with the I/O driver calling <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages</span></code>.
The race bug does not affect the persistent mapping case, as in that case an invalidation callback is not supported nor needed.</p>
<p>The bug fix required the following API change:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages</span></code> no longer accepts a NULL callback pointer.</p></li>
<li><p>Instead, <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages_persistent</span></code> and <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages_persistent</span></code> have been introduced and should be used instead when requesting a persistent mapping.</p></li>
<li><p>The use of those new persistent APIs can be guarded by the <code class="docutils literal notranslate"><span class="pre">NVIDIA_P2P_CAP_GET_PAGES_PERSISTENT_API</span></code> preprocessor macro, for example when writing portable drivers.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module has been updated accordingly.</p></li>
<li><p>Although deprecated when running GPU drivers from the R470 branch and newer, customers still using the off-tree <code class="docutils literal notranslate"><span class="pre">nv_peer_mem</span> <span class="pre">module</span></code> (<a class="reference external" href="https://github.com/Mellanox/nv_peer_memory">https://github.com/Mellanox/nv_peer_memory</a>) and needing the persistent mapping feature will have to switch to <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code>.</p></li>
</ul>
<p>Note that I/O drivers, which do not need persistent mappings, do not require source code changes.</p>
<p>The API changes described above are deployed in the R535 branch, specifically in release 535.14 and later, and have also been back-ported to the R525 branch, for TeslaRD3 (525.105.17) and later.</p>
</section>
</section>
<section id="design-considerations">
<span id="id3"></span><h1>
<span class="section-number">2. </span>Design Considerations<a class="headerlink" href="index.html#design-considerations" title="Permalink to this headline"></a>
</h1>
<p>When designing a system to utilize GPUDirect RDMA, there a number of considerations which should be taken into account.</p>
<section id="lazy-unpinning-optimization">
<span id="id4"></span><h2>
<span class="section-number">2.1. </span>Lazy Unpinning Optimization<a class="headerlink" href="index.html#lazy-unpinning-optimization" title="Permalink to this headline"></a>
</h2>
<p>Pinning GPU device memory in BAR is an expensive operation, taking up to milliseconds. Therefore the application should be designed in a way to minimize that overhead.</p>
<p>The most straightforward implementation using GPUDirect RDMA would pin memory before each transfer and unpin it right after the transfer is complete. Unfortunately, this would perform poorly in general, as pinning and unpinning memory are expensive operations. The rest of the steps required to perform an RDMA transfer, however, can be performed quickly without entering the kernel (the DMA list can be cached and replayed using MMIO registers/command lists).</p>
<p>Hence, lazily unpinning memory is key to a high performance RDMA implementation. What it implies, is keeping the memory pinned even after the transfer has finished. This takes advantage of the fact that it is likely that the same memory region will be used for future DMA transfers thus lazy unpinning saves pin/unpin operations.</p>
<p>An example implementation of lazy unpinning would keep a set of pinned memory regions and only unpin some of them (for example the least recently used one) if the total size of the regions reached some threshold, or if pinning a new region failed because of BAR space exhaustion (refer to <a class="reference internal" href="index.html#pci-bar-sizes"><span class="std std-ref">PCI BAR sizes</span></a>).</p>
</section>
<section id="registration-cache">
<span id="id5"></span><h2>
<span class="section-number">2.2. </span>Registration Cache<a class="headerlink" href="index.html#registration-cache" title="Permalink to this headline"></a>
</h2>
<p>Communication middleware often employs an optimization called a registration cache, or pin-down cache, to minimize pinning overhead. Typically it already exists for host memory, implementing lazy unpinning, LRU de-registration, etc. For networking middleware, such caches are usually implemented in user-space, as they are used in combination with hardware capable of user-mode message injection. CUDA UVA memory address layout enables GPU memory pinning to work with these caches by taking into account just a few design considerations. In the CUDA environment, this is even more important as the amount of memory which can be pinned may be significantly more constrained than for host memory.</p>
<p>As the GPU BAR space is typically mapped using 64KB pages, it is more resource efficient to maintain a cache of regions rounded to the 64KB boundary. Even more so, as two memory areas which are in the same 64KB boundary would allocate and return the same BAR mapping.</p>
<p>Registration caches usually rely on the ability to intercept deallocation events happening in the user application, so that they can unpin the memory and free important HW resources, e.g. on the network card. To implement a similar mechanism for GPU memory, an implementation has two options:</p>
<ul class="simple">
<li><p>Instrument all CUDA allocation and deallocation APIs.</p></li>
<li><p>Use a tag check function to track deallocation and reallocation. Refer to <a class="reference internal" href="index.html#invalidating-based-on-buffer-id"><span class="std std-ref">Buffer ID Tag Check for A Registration Cache</span></a>.</p></li>
</ul>
<p>There is a sample application, <code class="docutils literal notranslate"><span class="pre">7_CUDALibraries/cuHook</span></code>, showing how to intercept calls to CUDA APIs at run-time, which can be used to detect GPU memory de/allocations.</p>
<p>While intercepting CUDA APIs is beyond the scope of this document, an approach to performing tag checks is available starting with CUDA 6.0. It involves the usage of the <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_BUFFER_ID</span></code> attribute in <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> (or <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttributes()</span></code> if more attributes are needed) to detect memory buffer deallocations or reallocations. The API will return a different ID value in case of reallocation or an error if the buffer address is no longer valid. Refer to <a class="reference internal" href="index.html#userspace-api"><span class="std std-ref">Userspace API</span></a> for API usage.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using tag checks introduces an extra call into the CUDA API on each memory buffer use, so this approach is most appropriate when the additional latency is not a concern.</p>
</div>
</section>
<section id="unpin-callback">
<span id="id6"></span><h2>
<span class="section-number">2.3. </span>Unpin Callback<a class="headerlink" href="index.html#unpin-callback" title="Permalink to this headline"></a>
</h2>
<p>When a third party device driver pins the GPU pages with <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> it must also provide a callback function that the NVIDIA driver will call if it needs to revoke access to the mapping. <strong>This callback occurs synchronously</strong>, giving the third party driver the opportunity to clean up and remove any references to the pages in question (i.e., wait for outstanding DMAs to complete). <strong>The user callback function may block for a few milliseconds</strong>, although it is recommended that the callback complete as quickly as possible. Care has to be taken not to introduce deadlocks as waiting within the callback for the GPU to do anything is not safe.</p>
<p>The callback must call <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_free_page_table()</span></code> (not <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages()</span></code>) to free the memory pointed to by <code class="docutils literal notranslate"><span class="pre">page_table</span></code>. The corresponding mapped memory areas will only be unmapped by the NVIDIA driver after returning from the callback.</p>
<p>Note that the callback will be invoked in two scenarios:</p>
<ul class="simple">
<li><p>If the userspace program explicitly deallocates the corresponding GPU memory, e.g. <code class="docutils literal notranslate"><span class="pre">cuMemFree</span></code>, <code class="docutils literal notranslate"><span class="pre">cuCtxDestroy</span></code>, etc. before the third party kernel driver has a chance to unpin the memory with <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages()</span></code>.</p></li>
<li><p>As a consequence of an early exit of the process.</p></li>
</ul>
<p>In the latter case there can be tear-down ordering issues between closing the file descriptor of the third party kernel driver and that of the NVIDIA kernel driver. In the case the file descriptor for the NVIDIA kernel driver is closed first, the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages()</span></code> callback will be invoked.</p>
<p>A proper software design is important as the NVIDIA kernel driver will protect itself from reentrancy issues with locks before invoking the callback. The third party kernel driver will almost certainly take similar actions, so dead-locking or live-locking scenarios may arise if careful consideration is not taken.</p>
</section>
<section id="supported-systems">
<span id="id7"></span><h2>
<span class="section-number">2.4. </span>Supported Systems<a class="headerlink" href="index.html#supported-systems" title="Permalink to this headline"></a>
</h2>
<p><strong>General remarks</strong></p>
<p>Even though the only theoretical requirement for GPUDirect RDMA to work between a third-party device and an NVIDIA GPU is that they share the same root complex, there exist bugs (mostly in chipsets) causing it to perform badly, or not work at all in certain setups.</p>
<p>We can distinguish between three situations, depending on what is on the path between the GPU and the third-party device:</p>
<ul class="simple">
<li><p>PCIe switches only</p></li>
<li><p>single CPU/IOH</p></li>
<li><p>CPU/IOH &lt;-&gt; QPI/HT &lt;-&gt; CPU/IOH</p></li>
</ul>
<p>The first situation, where there are only PCIe switches on the path, is optimal and yields the best performance. The second one, where a single CPU/IOH is involved, works, but yields worse performance ( especially peer-to-peer read bandwidth has been shown to be severely limited on some processor architectures ). Finally, the third situation, where the path traverses a QPI/HT link, may be extremely performance-limited or even not work reliably.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>lspci can be used to check the PCI topology:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">lspci</span><span class="w"> </span><span class="o">-</span><span class="n">t</span><span class="w"></span>
</pre></div>
</div>
</div>
<p><strong>Platform support</strong></p>
<p>For IBM POWER8 platform, GPUDirect RDMA and P2P are not supported, but are not explicitly disabled. They may not work at run-time.</p>
<p>GPUDirect RDMA is supported on Jetson AGX Xavier platform starting from CUDA 10.1 and on Drive AGX Xavier Linux based platforms from CUDA 11.2. Refer to <a class="reference internal" href="index.html#porting-to-tegra"><span class="std std-ref">Porting to Tegra</span></a> for details. On ARM64, the necessary peer-to-peer functionality depends on both the hardware and the software of the particular platform. So while GPUDirect RDMA is not explicitly disabled on non-Jetson and non-Drive platforms, there are no guarantees that it will be fully functional.</p>
<p><strong>IOMMUs</strong></p>
<p>GPUDirect RDMA currently relies upon all physical addresses being the same from the different PCI devices’ point of view. This makes it incompatible with IOMMUs performing any form of translation other than 1:1, hence they must be disabled or configured for pass-through translation for GPUDirect RDMA to work.</p>
</section>
<section id="pci-bar-sizes">
<span id="id8"></span><h2>
<span class="section-number">2.5. </span>PCI BAR sizes<a class="headerlink" href="index.html#pci-bar-sizes" title="Permalink to this headline"></a>
</h2>
<p>PCI devices can ask the OS/BIOS to map a region of physical address space to them. These regions are commonly called BARs. NVIDIA GPUs currently expose multiple BARs, and some of them can back arbitrary device memory, making GPUDirect RDMA possible.
The maximum BAR size available for GPUDirect RDMA differs from GPU to GPU. For example, currently the smallest available BAR size on Kepler class GPUs is 256 MB. Of that, 32MB are currently reserved for internal use. These sizes may change.</p>
<p>On some Tesla-class GPUs a large BAR feature is enabled, e.g. BAR1 size is set to 16GB or larger. Large BARs can pose a problem for the BIOS, especially on older motherbords, related to compatibility support for 32bit operating systems. On those motherboards the bootstrap can stop during the early POST phase, or the GPU may be misconfigured and so unusable. If this appears to be occuring it might be necessary to enable some special BIOS feature to deal with the large BAR issue. Please consult your system vendor for more details regarding large BAR support.</p>
</section>
<section id="tokens-usage">
<span id="id9"></span><h2>
<span class="section-number">2.6. </span>Tokens Usage<a class="headerlink" href="index.html#tokens-usage" title="Permalink to this headline"></a>
</h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Starting in CUDA 6.0, tokens should be considered deprecated, though they are still supported.</p>
</div>
<p>As can be seen in <a class="reference internal" href="index.html#userspace-api"><span class="std std-ref">Userspace API</span></a> and <a class="reference internal" href="index.html#kernel-api"><span class="std std-ref">Kernel API</span></a>, one method for pinning and unpinning memory requires two tokens in addition to the GPU virtual address.</p>
<p>These tokens, <code class="docutils literal notranslate"><span class="pre">p2pToken</span></code> and <code class="docutils literal notranslate"><span class="pre">vaSpaceToken</span></code>, are necessary to uniquely identify a GPU VA space. A process identifier alone does not identify a GPU VA space.</p>
<p>The tokens are consistent within a single CUDA context (i.e., all memory obtained through <code class="docutils literal notranslate"><span class="pre">cudaMalloc()</span></code> within the same CUDA context will have the same <code class="docutils literal notranslate"><span class="pre">p2pToken</span></code> and <code class="docutils literal notranslate"><span class="pre">vaSpaceToken</span></code>). However, a given GPU virtual address need not map to the same context/GPU for its entire lifetime. As a concrete example:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"></span>
<span class="n">ptr0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cudaMalloc</span><span class="p">();</span><span class="w"></span>
<span class="n">cuPointerGetAttribute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">return_data</span><span class="p">,</span><span class="w"> </span><span class="n">CU_POINTER_ATTRIBUTE_P2P_TOKENS</span><span class="p">,</span><span class="w"> </span><span class="n">ptr0</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Returns [p2pToken = 0xabcd, vaSpaceToken = 0x1]</span>
<span class="n">cudaFree</span><span class="p">(</span><span class="n">ptr0</span><span class="p">);</span><span class="w"></span>
<span class="n">cudaSetDevice</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="n">ptr1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cudaMalloc</span><span class="p">();</span><span class="w"></span>
<span class="n">assert</span><span class="p">(</span><span class="n">ptr0</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">ptr1</span><span class="p">);</span><span class="w"></span>
<span class="c1">// The CUDA driver is free (although not guaranteed) to reuse the VA,</span>
<span class="c1">// even on a different GPU</span>
<span class="n">cuPointerGetAttribute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">return_data</span><span class="p">,</span><span class="w"> </span><span class="n">CU_POINTER_ATTRIBUTE_P2P_TOKENS</span><span class="p">,</span><span class="w"> </span><span class="n">ptr0</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Returns [p2pToken = 0x0123, vaSpaceToken = 0x2]</span>
</pre></div>
</div>
<p>That is, the same address, when passed to <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute</span></code>, may return different tokens at different times during the program’s execution. Therefore, the third party communication library must call <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> for every pointer it operates on.</p>
<p><strong>Security implications</strong></p>
<p>The two tokens act as an authentication mechanism for the NVIDIA kernel driver. If you know the tokens, you can map the address space corresponding to them, and the NVIDIA kernel driver doesn’t perform any additional checks. The 64bit <code class="docutils literal notranslate"><span class="pre">p2pToken</span></code> is randomized to prevent it from being guessed by an adversary.</p>
<p>When no tokens are used, the NVIDIA driver limits the <a class="reference internal" href="index.html#kernel-api"><span class="std std-ref">Kernel API</span></a> to the process which owns the memory allocation.</p>
</section>
<section id="synchronization-and-memory-ordering">
<span id="sync-behavior"></span><h2>
<span class="section-number">2.7. </span>Synchronization and Memory Ordering<a class="headerlink" href="index.html#synchronization-and-memory-ordering" title="Permalink to this headline"></a>
</h2>
<p>GPUDirect RDMA introduces a new independent GPU data flow path exposed to third party devices and it is important to understand how these devices interact with the GPU’s relaxed memory model.</p>
<ul class="simple">
<li><p>Properly registering a BAR mapping of CUDA memory is required for that mapping to remain consistent with CUDA APIs operations on that memory.</p></li>
<li><p>Only CUDA synchronization and work submission APIs provide memory ordering of GPUDirect RDMA operations.</p></li>
</ul>
<p><strong>Registration for CUDA API Consistency</strong></p>
<p>Registration is necessary to ensure the CUDA API memory operations visible to a BAR mapping happen before the API call returns control to the calling CPU thread. This provides a consistent view of memory to a device using GPUDirect RDMA mappings when invoked after a CUDA API in the thread. This is a strictly more conservative mode of operation for the CUDA API and disables optimizations, thus it may negatively impact performance.</p>
<p>This behavior is enabled on a per-allocation granularity either by calling <code class="docutils literal notranslate"><span class="pre">cuPointerSetAttribute()</span></code> with the <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_SYNC_MEMOPS</span></code> attribute, or p2p tokens are retrieved for a buffer when using the legacy path. Refer to <a class="reference internal" href="index.html#userspace-api"><span class="std std-ref">Userspace API</span></a> for more details.</p>
<p>An example situation would be Read-after-Write dependency between a <code class="docutils literal notranslate"><span class="pre">cuMemcpyDtoD()</span></code> and subsequent GPUDirect RDMA read operation on the destination of the copy. As an optimization the device-to-device memory copy typically returns asynchronously to the calling thread after queuing the copy to the GPU scheduler. However, in this circumstance that will lead to inconsistent data read via the BAR mapping, so this optimization is disabled an the copy completed before the CUDA API returns.</p>
<p><strong>CUDA APIs for Memory Ordering</strong></p>
<p>Only CPU initiated CUDA APIs provide ordering of GPUDirect memory operations as observed by the GPU. That is, despite a third party device having issued all PCIE transactions, a running GPU kernel or copy operation may observe stale data or data that arrives out-of-order until a subsequent CPU initiated CUDA work submission or synchronization API. To ensure that memory updates are visible to CUDA kernels or copies, an implementation should ensure that all writes to the GPU BAR happen before control is returned to the CPU thread which will invoke the dependent CUDA API.</p>
<p>An example situation for a network communication scenario is when a network RDMA write operation is completed by the third party network device and the data is written to the GPU BAR mapping. Though reading back the written data either through GPU BAR or a CUDA memory copy operation, will return the newly written data, a concurrently running GPU kernel to that network write might observe stale data, the data partially written, or the data written out-of-order.</p>
<p>In short, a GPU kernel is wholly inconsistent with concurrent RDMA for GPUDirect operations and accessing the memory overwritten by the third party device in such a situation would be considered a data race. To resolve this inconsistency and remove the data race the DMA write operation must complete with respect to the CPU thread which will launch the dependent GPU kernel.</p>
</section>
</section>
<section id="how-to-perform-specific-tasks">
<h1>
<span class="section-number">3. </span>How to Perform Specific Tasks<a class="headerlink" href="index.html#how-to-perform-specific-tasks" title="Permalink to this headline"></a>
</h1>
<section id="displaying-gpu-bar-space">
<span id="display-bar-space"></span><h2>
<span class="section-number">3.1. </span>Displaying GPU BAR space<a class="headerlink" href="index.html#displaying-gpu-bar-space" title="Permalink to this headline"></a>
</h2>
<p>Starting in CUDA 6.0 the NVIDIA SMI utility provides the capability to dump BAR1 memory usage. It can be used to understand the application usage of BAR space, the primary resource consumed by GPUDirect RDMA mappings.</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span>$ nvidia-smi -q
...
      BAR1 Memory Usage
         Total                       : 256 MiB
         Used                        : 2 MiB
         Free                        : 254 MiB
...
</pre></div>
</div>
<p>GPU memory is pinned in fixed size chunks, so the amount of space reflected here might be unexpected. In addition, a certain amount of BAR space is reserved by the driver for internal use, so not all available memory may be usable via GPUDirect RDMA. Note that the same ability is offered programmatically through the <code class="docutils literal notranslate"><span class="pre">nvmlDeviceGetBAR1MemoryInfo()</span></code> NVML API.</p>
</section>
<section id="pinning-gpu-memory">
<span id="id10"></span><h2>
<span class="section-number">3.2. </span>Pinning GPU memory<a class="headerlink" href="index.html#pinning-gpu-memory" title="Permalink to this headline"></a>
</h2>
<ol class="arabic">
<li>
<p>Correct behavior requires using <code class="docutils literal notranslate"><span class="pre">cuPointerSetAttribute()</span></code> on the memory address to enable proper synchronization behavior in the CUDA driver. Refer to <a class="reference internal" href="index.html#sync-behavior"><span class="std std-ref">Synchronization and Memory Ordering</span></a>.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">pin_buffer</span><span class="p">(</span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">address</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">CUresult</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cuPointerSetAttribute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">flag</span><span class="p">,</span><span class="w"> </span><span class="n">CU_POINTER_ATTRIBUTE_SYNC_MEMOPS</span><span class="p">,</span><span class="w"> </span><span class="n">address</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">CUDA_SUCCESS</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">status</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// GPU path</span>
<span class="w">        </span><span class="n">pass_to_kernel_driver</span><span class="p">(</span><span class="n">address</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// CPU path</span>
<span class="w">        </span><span class="c1">// ...</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>This is required so that the GPU memory buffer is treated in a special way by the CUDA driver, so that CUDA memory transfers are guaranteed to always be synchronous with respect to the host. Refer to <a class="reference internal" href="index.html#userspace-api"><span class="std std-ref">Userspace API</span></a> for details on <code class="docutils literal notranslate"><span class="pre">cuPointerSetAttribute()</span></code>.</p>
</li>
<li>
<p>In the kernel driver, invoke <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code>.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="c1">// for boundary alignment requirement</span>
<span class="cp">#define GPU_BOUND_SHIFT   16</span>
<span class="cp">#define GPU_BOUND_SIZE    ((u64)1 &lt;&lt; GPU_BOUND_SHIFT)</span>
<span class="cp">#define GPU_BOUND_OFFSET  (GPU_BOUND_SIZE-1)</span>
<span class="cp">#define GPU_BOUND_MASK    (~GPU_BOUND_OFFSET)</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">kmd_state</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">nvidia_p2p_page_table_t</span><span class="w"> </span><span class="o">*</span><span class="n">page_table</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="p">};</span><span class="w"></span>

<span class="kt">void</span><span class="w"> </span><span class="nf">kmd_pin_memory</span><span class="p">(</span><span class="k">struct</span><span class="w"> </span><span class="nc">kmd_state</span><span class="w"> </span><span class="o">*</span><span class="n">my_state</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">address</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">// do proper alignment, as required by NVIDIA kernel driver</span>
<span class="w">    </span><span class="n">u64</span><span class="w"> </span><span class="n">virt_start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">address</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">GPU_BOUND_MASK</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">pin_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">address</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">virt_start</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">size</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="o">-</span><span class="n">EINVAL</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nvidia_p2p_get_pages</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">virt_start</span><span class="p">,</span><span class="w"> </span><span class="n">pin_size</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">my_state</span><span class="o">-&gt;</span><span class="n">page_table</span><span class="p">,</span><span class="w"> </span><span class="n">free_callback</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">my_state</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// Succesfully pinned, page_table can be accessed</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// Pinning failed</span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Note how the start address is aligned to a 64KB boundary before calling the pinning functions.</p>
<p>If the function succeeds the memory has been pinned and the <code class="docutils literal notranslate"><span class="pre">page_table</span></code> entries can be used to program the device’s DMA engine. Refer to <a class="reference internal" href="index.html#kernel-api"><span class="std std-ref">Kernel API</span></a> for details on <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code>.</p>
</li>
</ol>
</section>
<section id="unpinning-gpu-memory">
<span id="id11"></span><h2>
<span class="section-number">3.3. </span>Unpinning GPU memory<a class="headerlink" href="index.html#unpinning-gpu-memory" title="Permalink to this headline"></a>
</h2>
<p>In the kernel driver, invoke <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages()</span></code>.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">unpin_memory</span><span class="p">(</span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">address</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">nvidia_p2p_page_table_t</span><span class="w"> </span><span class="o">*</span><span class="n">page_table</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">nvidia_p2p_put_pages</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">address</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">page_table</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Refer to <a class="reference internal" href="index.html#kernel-api"><span class="std std-ref">Kernel API</span></a> for details on <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages()</span></code>.</p>
<p>Starting CUDA 6.0 zeros should be used as the token parameters. Note that <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages()</span></code> must be called from within the same process context as the one from which the corresponding <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> has been issued.</p>
</section>
<section id="handling-the-free-callback">
<span id="handling-free-callback"></span><h2>
<span class="section-number">3.4. </span>Handling the free callback<a class="headerlink" href="index.html#handling-the-free-callback" title="Permalink to this headline"></a>
</h2>
<ol class="arabic">
<li><p>The NVIDIA kernel driver invokes <code class="docutils literal notranslate"><span class="pre">free_callback(data)</span></code> as specified in the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> call if it needs to revoke the mapping. Refer to <a class="reference internal" href="index.html#kernel-api"><span class="std std-ref">Kernel API</span></a> and <a class="reference internal" href="index.html#unpin-callback"><span class="std std-ref">Unpin Callback</span></a> for details.</p></li>
<li>
<p>The callback waits for pending transfers and then cleans up the page table allocation.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">free_callback</span><span class="p">(</span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">   </span><span class="n">my_state</span><span class="w"> </span><span class="o">*</span><span class="n">state</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="n">wait_for_pending_transfers</span><span class="p">(</span><span class="n">state</span><span class="p">);</span><span class="w"></span>
<span class="w">   </span><span class="n">nvidia_p2p_free_pages</span><span class="p">(</span><span class="n">state</span><span class="o">-&gt;</span><span class="n">page_table</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>The NVIDIA kernel driver handles the unmapping so <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages()</span></code> should not be called.</p></li>
</ol>
</section>
<section id="buffer-id-tag-check-for-a-registration-cache">
<span id="invalidating-based-on-buffer-id"></span><h2>
<span class="section-number">3.5. </span>Buffer ID Tag Check for A Registration Cache<a class="headerlink" href="index.html#buffer-id-tag-check-for-a-registration-cache" title="Permalink to this headline"></a>
</h2>
<p>Remember that a solution built around Buffer ID tag checking is not recommended for latency sensitive implementations. Instead, instrumentation of CUDA allocation and deallocation APIs to provide callbacks to the registration cache is recommended, removing tag checking overhead from the critical path.</p>
<ol class="arabic">
<li>
<p>The first time a device memory buffer is encountered and recognized as not yet pinned, the pinned mapping is created and the associated buffer ID is retrieved and stored together in the cache entry. The <code class="docutils literal notranslate"><span class="pre">cuMemGetAddressRange()</span></code> function can be used to obtain the size and starting address for the whole allocation, which can then be used to pin it. As <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> will need a pointer aligned to 64K, it is useful to directly align the cached address. Also, as the BAR space is currently mapped in chunks of 64KB, it is more resource efficient to round the whole pinning to 64KB.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="c1">// struct buf represents an entry of the registration cache</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">buf</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">CUdeviceptr</span><span class="w"> </span><span class="n">pointer</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">size_t</span><span class="w">      </span><span class="n">size</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">CUdeviceptr</span><span class="w"> </span><span class="n">aligned_pointer</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">size_t</span><span class="w">      </span><span class="n">aligned_size</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w">         </span><span class="n">is_pinned</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint64_t</span><span class="w">    </span><span class="n">id</span><span class="p">;</span><span class="w"> </span><span class="c1">// buffer id obtained right after pinning</span>
<span class="p">};</span><span class="w"></span>
</pre></div>
</div>
</li>
<li>
<p>Once created, every time a registration cache entry will be used it must be first checked for validity. One way to do this is to use the Buffer ID provided by CUDA as a tag to check for deallocation or reallocation.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">buf_is_gpu_pinning_valid</span><span class="p">(</span><span class="k">struct</span><span class="w"> </span><span class="nc">buf</span><span class="o">*</span><span class="w"> </span><span class="n">buf</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">buffer_id</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">retcode</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">assert</span><span class="p">(</span><span class="n">buf</span><span class="o">-&gt;</span><span class="n">is_pinned</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="c1">// get the current buffer id</span>
<span class="w">    </span><span class="n">retcode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cuPointerGetAttribute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">buffer_id</span><span class="p">,</span><span class="w"> </span><span class="n">CU_POINTER_ATTRIBUTE_BUFFER_ID</span><span class="p">,</span><span class="w"> </span><span class="n">buf</span><span class="o">-&gt;</span><span class="n">pointer</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">CUDA_ERROR_INVALID_VALUE</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">retcode</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// the device pointer is no longer valid</span>
<span class="w">        </span><span class="c1">// it could have been deallocated</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">ERROR_INVALIDATED</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">CUDA_SUCCESS</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">retcode</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="c1">// handle more serious errors here</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">ERROR_SERIOUS</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">buf</span><span class="o">-&gt;</span><span class="n">id</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">buffer_id</span><span class="p">)</span><span class="w"></span>
<span class="w">        </span><span class="c1">// the original buffer has been deallocated and the cached mapping should be invalidated and the buffer re-pinned</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">ERROR_INVALIDATED</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>When the buffer identifier changes the corresponding memory buffer has been reallocated so the corresponding kernel-space page table will not be valid anymore. In this case the kernel-space <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> callback would have been invoked. Thus the Buffer IDs provide a tag to keep the pin-down cache consistent with the kernel-space page table without requiring the kernel driver to up-call into the user-space.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">CUDA_ERROR_INVALID_VALUE</span></code> is returned from <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code>, the program should assume that the memory buffer has been deallocated or is otherwise not a valid GPU memory buffer.</p>
</li>
<li>
<p>In both cases, the corresponding cache entry must be invalidated.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="c1">// in the registration cache code</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">buf</span><span class="o">-&gt;</span><span class="n">is_pinned</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">buf_is_gpu_pinning_valid</span><span class="p">(</span><span class="n">buf</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">regcache_invalidate_entry</span><span class="p">(</span><span class="n">buf</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">pin_buffer</span><span class="p">(</span><span class="n">buf</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="linking-a-kernel-module-against-nvidia-ko">
<span id="linking-kernel-module-against-nvidia-ko"></span><h2>
<span class="section-number">3.6. </span>Linking a Kernel Module against nvidia.ko<a class="headerlink" href="index.html#linking-a-kernel-module-against-nvidia-ko" title="Permalink to this headline"></a>
</h2>
<ol class="arabic">
<li>
<p>Run the extraction script:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span>./NVIDIA-Linux-x86_64-&lt;version&gt;.run -x
</pre></div>
</div>
<p>This extracts the NVIDA driver and kernel wrapper.</p>
</li>
<li>
<p>Navigate to the output directory:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span>cd &lt;output directory&gt;/kernel/
</pre></div>
</div>
</li>
<li>
<p>Within this directory, build the NVIDIA module for your kernel:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span>make module
</pre></div>
</div>
<p>After this is done, the <code class="docutils literal notranslate"><span class="pre">Module.symvers</span></code> file under your kernel build directory contains symbol information for <code class="docutils literal notranslate"><span class="pre">nvidia.ko</span></code>.</p>
</li>
<li>
<p>Modify your kernel module build process with the following line:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span>KBUILD_EXTRA_SYMBOLS := &lt;path to kernel build directory&gt;/Module.symvers
</pre></div>
</div>
</li>
</ol>
</section>
<section id="using-nvidia-peermem">
<span id="id12"></span><h2>
<span class="section-number">3.7. </span>Using nvidia-peermem<a class="headerlink" href="index.html#using-nvidia-peermem" title="Permalink to this headline"></a>
</h2>
<p>The NVIDIA GPU driver package provides a kernel module, <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code>, which provides NVIDIA InfiniBand based HCAs (Host Channel Adapters) direct peer-to-peer read and write access to the NVIDIA GPU’s video memory. It allows GPUDirect RDMA-based applications to use GPU computing power with the RDMA interconnect without needing to copy data to host memory.</p>
<p>This capability is supported with NVIDIA ConnectX®-3 VPI or newer adapters. It works with both InfiniBand and RoCE (RDMA over Converged Ethernet) technologies.</p>
<p>NVIDIA OFED (Open Fabrics Enterprise Distribution), or MLNX_OFED, introduces an API between the InfiniBand Core and peer memory clients such as NVIDIA GPUs. The <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> module registers the NVIDIA GPU with the InfiniBand subsystem by using peer-to-peer APIs provided by the NVIDIA GPU driver.</p>
<p>The kernel must have the required support for RDMA peer memory either through additional patches to the kernel or via MLNX_OFED as a prerequisite for loading and using <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code>.</p>
<p>It is possible that the <code class="docutils literal notranslate"><span class="pre">nv_peer_mem</span></code> module from the GitHub project may be installed and loaded on the system. Installation of <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> will not affect the functionality of the existing <code class="docutils literal notranslate"><span class="pre">nv_peer_mem</span></code> module. But, to load and use <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code>, users must disable the <code class="docutils literal notranslate"><span class="pre">nv_peer_mem</span></code> service. Additionally, it is encouraged to uninstall the <code class="docutils literal notranslate"><span class="pre">nv_peer_mem</span></code> package to avoid any conflict with <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> since only one module can be loaded at any time.</p>
<p>To stop the <code class="docutils literal notranslate"><span class="pre">nv_peer_mem</span></code> service:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span># service nv_peer_mem stop&lt;/screen&gt;
</pre></div>
</div>
<p>Check if <code class="docutils literal notranslate"><span class="pre">nv_peer_mem.ko</span></code> is still loaded after stopping the service:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span># lsmod | grep nv_peer_mem
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">nv_peer_mem.ko</span></code> is still loaded, unload it using:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span># rmmod nv_peer_mem
</pre></div>
</div>
<p>Uninstall the <code class="docutils literal notranslate"><span class="pre">nv_peer_mem</span></code> package:</p>
<p><strong>For DEB-based OS:</strong></p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span># dpkg -P nvidia-peer-memory
</pre></div>
</div>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span># dpkg -P nvidia-peer-memory-dkms
</pre></div>
</div>
<p><strong>For RPM-based OS:</strong></p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span># rpm -e nvidia_peer_memory
</pre></div>
</div>
<p>After ensuring kernel support and installing the GPU driver, <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> can be loaded with the following command with root privileges in a terminal window:</p>
<div class="highlight-text notranslate">
<div class="highlight"><pre><span></span># modprobe nvidia-peermem
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note: If the NVIDIA GPU driver is installed before MLNX_OFED, the GPU driver must be uninstalled and installed again to make sure <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> is compiled with the RDMA APIs that are provided by MLNX_OFED.</p>
</div>
</section>
</section>
<section id="references">
<span id="id13"></span><h1>
<span class="section-number">4. </span>References<a class="headerlink" href="index.html#references" title="Permalink to this headline"></a>
</h1>
<section id="basics-of-uva-cuda-memory-management">
<span id="id14"></span><h2>
<span class="section-number">4.1. </span>Basics of UVA CUDA Memory Management<a class="headerlink" href="index.html#basics-of-uva-cuda-memory-management" title="Permalink to this headline"></a>
</h2>
<p>Unified virtual addressing (UVA) is a memory address management system enabled by default in CUDA 4.0 and later releases on Fermi and Kepler GPUs running 64-bit processes. The design of UVA memory management provides a basis for the operation of GPUDirect RDMA. On UVA-supported configurations, when the CUDA runtime initializes, the virtual address (VA) range of the application is partitioned into two areas: the CUDA-managed VA range and the OS-managed VA range. All CUDA-managed pointers are within this VA range, and the range will always fall within the first 40 bits of the process’s VA space.</p>
<figure class="align-center" id="basics-of-uva-cuda-memory-management-cuda-va-space-addressing">
<img alt="CUDA VA Space Addressing" src="_images/cuda-va-space-addressing.png">
<figcaption>
<p><span class="caption-text">CUDA VA Space Addressing</span><a class="headerlink" href="index.html#basics-of-uva-cuda-memory-management-cuda-va-space-addressing" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Subsequently, within the CUDA VA space, addresses can be subdivided into three types:</p>
<dl class="simple">
<dt>GPU</dt>
<dd>
<p>A page backed by GPU memory. This will not be accessible from the host and the VA in question will never have a physical backing on the host. Dereferencing a pointer to a GPU VA from the CPU will trigger a segfault.</p>
</dd>
<dt>CPU</dt>
<dd>
<p>A page backed by CPU memory. This will be accessible from both the host and the GPU at the same VA.</p>
</dd>
<dt>FREE</dt>
<dd>
<p>These VAs are reserved by CUDA for future allocations.</p>
</dd>
</dl>
<p>This partitioning allows the CUDA runtime to determine the physical location of a memory object by its pointer value within the reserved CUDA VA space.</p>
<p>Addresses are subdivided into these categories at page granularity; all memory within a page is of the same type. Note that GPU pages may not be the same size as CPU pages. The CPU pages are usually 4KB and the GPU pages on Kepler-class GPUs are 64KB. GPUDirect RDMA operates exclusively on GPU pages (created by <code class="docutils literal notranslate"><span class="pre">cudaMalloc()</span></code>) that are within this CUDA VA space.</p>
</section>
<section id="userspace-api">
<span id="id15"></span><h2>
<span class="section-number">4.2. </span>Userspace API<a class="headerlink" href="index.html#userspace-api" title="Permalink to this headline"></a>
</h2>
<p><strong>Data structures</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="k">typedef</span><span class="w"> </span><span class="k">struct</span><span class="w"> </span><span class="nc">CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_st</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="kt">long</span><span class="w"> </span><span class="n">p2pToken</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">vaSpaceToken</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span><span class="n">CUDA_POINTER_ATTRIBUTE_P2P_TOKENS</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p><strong>Function cuPointerSetAttribute()</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="n">CUresult</span><span class="w"> </span><span class="nf">cuPointerSetAttribute</span><span class="p">(</span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">CUpointer_attribute</span><span class="w"> </span><span class="n">attribute</span><span class="p">,</span><span class="w"> </span><span class="n">CUdeviceptr</span><span class="w"> </span><span class="n">pointer</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>In GPUDirect RDMA scope, the interesting usage is when <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_SYNC_MEMOPS</span></code> is passed as the <code class="docutils literal notranslate"><span class="pre">attribute</span></code>:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">flag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"></span>
<span class="n">cuPointerSetAttribute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">flag</span><span class="p">,</span><span class="w"> </span><span class="n">CU_POINTER_ATTRIBUTE_SYNC_MEMOPS</span><span class="p">,</span><span class="w"> </span><span class="n">pointer</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p><strong>Parameters</strong></p>
<dl class="simple">
<dt>data [in]</dt>
<dd>
<p>A pointer to a <code class="docutils literal notranslate"><span class="pre">unsigned</span> <span class="pre">int</span></code> variable containing a boolean value.</p>
</dd>
<dt>attribute [in]</dt>
<dd>
<p>In GPUDirect RDMA scope should always be <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_SYNC_MEMOPS</span></code>.</p>
</dd>
<dt>pointer [in]</dt>
<dd>
<p>A pointer.</p>
</dd>
</dl>
<p><strong>Returns</strong></p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">CUDA_SUCCESS</span></code></dt>
<dd>
<p>if pointer points to GPU memory and the CUDA driver was able to set the new behavior for the whole device memory allocation.</p>
</dd>
<dt>anything else</dt>
<dd>
<p>if pointer points to CPU memory.</p>
</dd>
</dl>
<p>It is used to explicitly enable a strictly synchronizing behavior on the whole memory allocation pointed to by <code class="docutils literal notranslate"><span class="pre">pointer</span></code>, and by doing so disabling all data transfer optimizations which might create problems with concurrent RDMA and CUDA memory copy operations. This API has CUDA synchronizing behavior, so it should be considered expensive and possibly invoked only once per buffer.</p>
<p><strong>Function cuPointerGetAttribute()</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="n">CUresult</span><span class="w"> </span><span class="nf">cuPointerGetAttribute</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">CUpointer_attribute</span><span class="w"> </span><span class="n">attribute</span><span class="p">,</span><span class="w"> </span><span class="n">CUdeviceptr</span><span class="w"> </span><span class="n">pointer</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>This function has two different attributes related to GPUDirect RDMA: <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_P2P_TOKENS</span></code> and <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_BUFFER_ID</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>CU_POINTER_ATTRIBUTE_P2P_TOKENS has been deprecated in CUDA 6.0</p>
</div>
<p>When <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_P2P_TOKENS</span></code> is passed as the <code class="docutils literal notranslate"><span class="pre">attribute</span></code>, <code class="docutils literal notranslate"><span class="pre">data</span></code> is a pointer to <code class="docutils literal notranslate"><span class="pre">CUDA_POINTER_ATTRIBUTE_P2P_TOKENS</span></code>:</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="n">CUDA_POINTER_ATTRIBUTE_P2P_TOKENS</span><span class="w"> </span><span class="n">tokens</span><span class="p">;</span><span class="w"></span>
<span class="n">cuPointerGetAttribute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tokens</span><span class="p">,</span><span class="w"> </span><span class="n">CU_POINTER_ATTRIBUTE_P2P_TOKENS</span><span class="p">,</span><span class="w"> </span><span class="n">pointer</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>In this case, the function returns two tokens for use with the <a class="reference internal" href="index.html#kernel-api"><span class="std std-ref">Kernel API</span></a>.</p>
<p><strong>Parameters</strong></p>
<dl class="simple">
<dt>data [out]</dt>
<dd>
<p>Struct <code class="docutils literal notranslate"><span class="pre">CUDA_POINTER_ATTRIBUTE_P2P_TOKENS</span></code> with the two tokens.</p>
</dd>
<dt>attribute [in]</dt>
<dd>
<p>In GPUDirect RDMA scope should always be <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_P2P_TOKENS</span></code>.</p>
</dd>
<dt>pointer [in]</dt>
<dd>
<p>A pointer.</p>
</dd>
</dl>
<p><strong>Returns</strong></p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">CUDA_SUCCESS</span></code></dt>
<dd>
<p>if pointer points to GPU memory.</p>
</dd>
<dt>anything else</dt>
<dd>
<p>if pointer points to CPU memory.</p>
</dd>
</dl>
<p>This function may be called at any time, including before CUDA initialization, and it has CUDA synchronizing behavior, as in <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_SYNC_MEMOPS</span></code>, so it should be considered expensive and should be invoked only once per buffer.</p>
<p>Note that values set in <code class="docutils literal notranslate"><span class="pre">tokens</span></code> can be different for the same <code class="docutils literal notranslate"><span class="pre">pointer</span></code> value during a lifetime of a user-space program. Refer to <a class="reference internal" href="index.html#tokens-usage"><span class="std std-ref">Tokens Usage</span></a> for a concrete example.</p>
<p>Note that for security reasons the value set in <code class="docutils literal notranslate"><span class="pre">p2pToken</span></code> will be randomized, to prevent it from being guessed by an adversary.</p>
<p>In CUDA 6.0, a new attribute has been introduced that is useful to detect memory reallocations.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_BUFFER_ID</span></code> is passed as the <code class="docutils literal notranslate"><span class="pre">attribute</span></code>, <code class="docutils literal notranslate"><span class="pre">data</span></code> is expected to point to a 64bit unsigned integer variable, like <code class="docutils literal notranslate"><span class="pre">uint64_t</span></code>.</p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">buf_id</span><span class="p">;</span><span class="w"></span>
<span class="n">cuPointerGetAttribute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">buf_id</span><span class="p">,</span><span class="w"> </span><span class="n">CU_POINTER_ATTRIBUTE_BUFFER_ID</span><span class="p">,</span><span class="w"> </span><span class="n">pointer</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p><strong>Parameters</strong></p>
<dl class="simple">
<dt>data [out]</dt>
<dd>
<p>A pointer to a 64 bits variable where the buffer id will be stored.</p>
</dd>
<dt>attribute [in]</dt>
<dd>
<p>The <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_BUFFER_ID</span></code> enumerator.</p>
</dd>
<dt>pointer [in]</dt>
<dd>
<p>A pointer to GPU memory.</p>
</dd>
</dl>
<p><strong>Returns</strong></p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">CUDA_SUCCESS</span></code></dt>
<dd>
<p>if pointer points to GPU memory.</p>
</dd>
<dt>anything else</dt>
<dd>
<p>if pointer points to CPU memory.</p>
</dd>
</dl>
<p>Some general remarks follow:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> and <code class="docutils literal notranslate"><span class="pre">cuPointerSetAttribute()</span></code> are CUDA driver API functions only.</p></li>
<li><p>In particular, <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> is not equivalent to <code class="docutils literal notranslate"><span class="pre">cudaPointerGetAttributes()</span></code>, as the required functionality is only present in the former function. This in no way limits the scope where GPUDirect RDMA may be used as <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> is compatible with the CUDA Runtime API.</p></li>
<li><p>No runtime API equivalent to <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> is provided. This is so as the additional overhead associated with the CUDA runtime API to driver API call sequence would introduce unneeded overhead and <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute()</span></code> can be on the critical path, e.g. of communication libraries.</p></li>
<li><p>Whenever possible, we suggest to combine multiple calls to <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttribute</span></code> by using <code class="docutils literal notranslate"><span class="pre">cuPointerGetAttributes</span></code>.</p></li>
</ul>
<p><strong>Function ``cuPointerGetAttributes()``</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="n">CUresult</span><span class="w"> </span><span class="nf">cuPointerGetAttributes</span><span class="p">(</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">numAttributes</span><span class="p">,</span><span class="w"> </span><span class="n">CUpointer_attribute</span><span class="w"> </span><span class="o">*</span><span class="n">attributes</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">CUdeviceptr</span><span class="w"> </span><span class="n">ptr</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>This function can be used to inspect multiple attributes at once. The one most probably related to GPUDirect RDMA are <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_BUFFER_ID</span></code>, <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_MEMORY_TYPE</span></code> and <code class="docutils literal notranslate"><span class="pre">CU_POINTER_ATTRIBUTE_IS_MANAGED</span></code>.</p>
</section>
<section id="kernel-api">
<span id="id16"></span><h2>
<span class="section-number">4.3. </span>Kernel API<a class="headerlink" href="index.html#kernel-api" title="Permalink to this headline"></a>
</h2>
<p>The following declarations can be found in the <code class="docutils literal notranslate"><span class="pre">nv-p2p.h</span></code> header that is distributed in the NVIDIA Driver package. Please refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values of the functions described below.</p>
<p><strong>Preprocessor macros</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE()</span></code> and <code class="docutils literal notranslate"><span class="pre">NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE()</span></code> preprocessor macros are meant to be called by third-party device drivers to check for runtime binary compatibility.</p>
<p><strong>Structure nvidia_p2p_page</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="k">typedef</span><span class="w"></span>
<span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_page</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">physical_address</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">union</span><span class="w"> </span><span class="nc">nvidia_p2p_request_registers</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="k">struct</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">            </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">wreqmb_h</span><span class="p">;</span><span class="w"></span>
<span class="w">            </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">rreqmb_h</span><span class="p">;</span><span class="w"></span>
<span class="w">            </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">rreqmb_0</span><span class="p">;</span><span class="w"></span>
<span class="w">            </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">reserved</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span><span class="w"></span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="n">fermi</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="n">registers</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span><span class="n">nvidia_p2p_page_t</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>In the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_page</span></code> structure only the <code class="docutils literal notranslate"><span class="pre">physical_address</span></code> field is relevant to GPUDirect RDMA.</p>
<p><strong>Structure nvidia_p2p_page_table</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="k">typedef</span><span class="w"></span>
<span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_page_table</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">version</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">page_size</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_page</span><span class="w"> </span><span class="o">**</span><span class="n">pages</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">entries</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint8_t</span><span class="w"> </span><span class="o">*</span><span class="n">gpu_uuid</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span><span class="n">nvidia_p2p_page_table_t</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">version</span></code> field of the page table should be checked by using <code class="docutils literal notranslate"><span class="pre">NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE()</span></code> before accessing the other fields.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">page_size</span></code> field is encoded according to the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_page_size_type</span></code> enum.</p>
<p><strong>Structure nvidia_p2p_dma_mapping</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="k">typedef</span><span class="w"></span>
<span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_dma_mapping</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint32_t</span><span class="w">  </span><span class="n">version</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">enum</span><span class="w"> </span><span class="n">nvidia_p2p_page_size_type</span><span class="w"> </span><span class="n">page_size_type</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint32_t</span><span class="w">  </span><span class="n">entries</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="kt">uint64_t</span><span class="w"> </span><span class="o">*</span><span class="n">dma_addresses</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span><span class="n">nvidia_p2p_dma_mapping_t</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>The version field of the dma mapping should be passed to <code class="docutils literal notranslate"><span class="pre">NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE()</span></code> before accessing the other fields.</p>
<p><strong>Function nvidia_p2p_get_pages()</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">nvidia_p2p_get_pages</span><span class="p">(</span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">p2p_token</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">va_space_token</span><span class="p">,</span><span class="w"></span>
<span class="w">                </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">virtual_address</span><span class="p">,</span><span class="w"></span>
<span class="w">                </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"></span>
<span class="w">                </span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_page_table</span><span class="w"> </span><span class="o">**</span><span class="n">page_table</span><span class="p">,</span><span class="w"></span>
<span class="w">                </span><span class="kt">void</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">free_callback</span><span class="p">)(</span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">),</span><span class="w"></span>
<span class="w">                </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>This function makes the pages underlying a range of GPU virtual memory accessible to a third-party device.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This is an expensive operation and should be performed as infrequently as possible - refer to <a class="reference internal" href="index.html#lazy-unpinning-optimization"><span class="std std-ref">Lazy Unpinning Optimization</span></a>.</p>
</div>
<p><strong>Function nvidia_p2p_put_pages()</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">nvidia_p2p_put_pages</span><span class="p">(</span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">p2p_token</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">va_space_token</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">virtual_address</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_page_table</span><span class="w"> </span><span class="o">*</span><span class="n">page_table</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>This function releases a set of pages previously made accessible to a third-party device. Warning: it is not meant to be called from within the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> callback.</p>
<p><strong>Function nvidia_p2p_free_page_table()</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">nvidia_p2p_free_page_table</span><span class="p">(</span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_page_table</span><span class="w"> </span><span class="o">*</span><span class="n">page_table</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>This function frees a third-party P2P page table and is meant to be invoked during the execution of the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> callback.</p>
<p><strong>Function nvidia_p2p_dma_map_pages()</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">nvidia_p2p_dma_map_pages</span><span class="p">(</span><span class="k">struct</span><span class="w"> </span><span class="nc">pci_dev</span><span class="w"> </span><span class="o">*</span><span class="n">peer</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_page_table</span><span class="w"> </span><span class="o">*</span><span class="n">page_table</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_dma_mapping</span><span class="w"> </span><span class="o">**</span><span class="n">dma_mapping</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>This function makes the physical pages retrieved using <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> accessible to a third-party device.</p>
<p>It is required on platforms where the I/O addresses of PCIe resources, used for PCIe peer-to-peer transactions, are different from the physical addresses used by the CPU to access those same resources.</p>
<p>On some platforms, this function relies on a correct implementation of the <code class="docutils literal notranslate"><span class="pre">dma_map_resource()</span></code> Linux kernel function.</p>
<p><strong>Function nvidia_p2p_dma_unmap_pages()</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">nvidia_p2p_dma_unmap_pages</span><span class="p">(</span><span class="k">struct</span><span class="w"> </span><span class="nc">pci_dev</span><span class="w"> </span><span class="o">*</span><span class="n">peer</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_page_table</span><span class="w"> </span><span class="o">*</span><span class="n">page_table</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_dma_mapping</span><span class="w"> </span><span class="o">*</span><span class="n">dma_mapping</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>This function unmaps the physical pages previously mapped to the third-party device by <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_dma_map_pages()</span></code>.</p>
<p>It is not meant to be called from within the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> invalidation callback.</p>
<p><strong>Function nvidia_p2p_free_dma_mapping()</strong></p>
<div class="highlight-c notranslate">
<div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">nvidia_p2p_free_dma_mapping</span><span class="p">(</span><span class="k">struct</span><span class="w"> </span><span class="nc">nvidia_p2p_dma_mapping</span><span class="w"> </span><span class="o">*</span><span class="n">dma_mapping</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>This function is meant to be called from within the <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> invalidation callback.</p>
<p>Note that the deallocation of the I/O mappings may be deferred, for example after returning from the invalidation callback.</p>
</section>
<section id="porting-to-tegra">
<span id="id17"></span><h2>
<span class="section-number">4.4. </span>Porting to Tegra<a class="headerlink" href="index.html#porting-to-tegra" title="Permalink to this headline"></a>
</h2>
<p>GPUDirect RDMA is supported on Jetson AGX Xavier platform from CUDA 10.1, on DRIVE AGX Xavier Linux based platforms from CUDA 11.2 and on Jetson Orin platform from CUDA 11.4. From this point onwards, this document will collectively refer Jetson and Drive as Tegra. Owing to hardware and software specific divergence of Tegra vis-a-vis Linux-Desktop, already developed applications needs to be slightly modified in order to port them to Tegra. The following sub-sections (4.4.1-4.4.3) briefs over the necessary changes.</p>
<section id="changing-the-allocator">
<h3>
<span class="section-number">4.4.1. </span>Changing the allocator<a class="headerlink" href="index.html#changing-the-allocator" title="Permalink to this headline"></a>
</h3>
<p>GPUDirect RDMA on Desktop allows applications to operate exclusively on GPU pages allocated using <code class="docutils literal notranslate"><span class="pre">cudaMalloc()</span></code>. On Tegra, applications will have to change the memory allocator from <code class="docutils literal notranslate"><span class="pre">cudaMalloc()</span></code> to <code class="docutils literal notranslate"><span class="pre">cudaHostAlloc()</span></code>. Applications can either:</p>
<ol class="arabic simple">
<li><p>Treat the returned pointer as if it is a device pointer, provided that the iGPU supports UVA or <code class="docutils literal notranslate"><span class="pre">cudaDevAttrCanUseHostPointerForRegisteredMem</span></code> device attribute is a non-zero value when queried using <code class="docutils literal notranslate"><span class="pre">cudaDeviceGetAttribute()</span></code> for iGPU.</p></li>
<li><p>Get the device pointer corresponding to the host memory allocated using <code class="docutils literal notranslate"><span class="pre">cudaHostGetDevicePointer()</span></code>. Once the application has the device pointer, all the rules that are applicable to the standard GPUDirect solution also apply to Tegra.</p></li>
</ol>
</section>
<section id="modification-to-kernel-api">
<h3>
<span class="section-number">4.4.2. </span>Modification to Kernel API<a class="headerlink" href="index.html#modification-to-kernel-api" title="Permalink to this headline"></a>
</h3>
<p>The declarations under Tegra API column of the following table can be found in the nv-p2p.h header that is distributed in the NVIDIA Driver package. Refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values. The table below represents the Kernel API changes on Tegra vis-a-vis Desktop.</p>
<table class="table-no-stripes docutils align-default">
<colgroup>
<col style="width: 53%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="row-odd">
<th class="head"><p>Desktop API</p></th>
<th class="head"><p>Tegra API</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even">
<td><p>int nvidia_p2p_get_pages(uint64_t p2p_token, uint32_t va_space_token, uint64_t virtual_address, uint64_t length, struct nvidia_p2p_page_table **page_table, void ( *free_callback)(void *data), void *data);</p></td>
<td><p>int nvidia_p2p_get_pages(u64 virtual_address, u64 length, struct nvidia_p2p_page_table **page_table, void (*free_callback)(void *data), void *data);</p></td>
</tr>
<tr class="row-odd">
<td><p>int nvidia_p2p_put_pages(uint64_t p2p_token, uint32_t va_space_token, uint64_t virtual_address, struct nvidia_p2p_page_table *page_table);</p></td>
<td><p>int nvidia_p2p_put_pages(struct nvidia_p2p_page_table *page_table);</p></td>
</tr>
<tr class="row-even">
<td><p>int nvidia_p2p_dma_map_pages(struct pci_dev *peer, struct nvidia_p2p_page_table *page_table, struct nvidia_p2p_dma_mapping **dma_mapping);</p></td>
<td><p>int nvidia_p2p_dma_map_pages(struct device *dev, struct nvidia_p2p_page_table *page_table, struct nvidia_p2p_dma_mapping **dma_mapping, enum dma_data_direction direction);</p></td>
</tr>
<tr class="row-odd">
<td><p>int nvidia_p2p_dma_unmap_pages(struct pci_dev *peer, struct nvidia_p2p_page_table *page_table, struct nvidia_p2p_dma_mapping *dma_mapping);</p></td>
<td><p>int nvidia_p2p_dma_unmap_pages(struct nvidia_p2p_dma_mapping *dma_mapping);</p></td>
</tr>
<tr class="row-even">
<td><p>int nvidia_p2p_free_page_table(struct nvidia_p2p_page_table *page_table);</p></td>
<td><p>int nvidia_p2p_free_page_table(struct nvidia_p2p_page_table *page_table);</p></td>
</tr>
<tr class="row-odd">
<td><p>int nvidia_p2p_free_dma_mapping(struct nvidia_p2p_dma_mapping *dma_mapping);</p></td>
<td><p>int nvidia_p2p_free_dma_mapping(struct nvidia_p2p_dma_mapping *dma_mapping);</p></td>
</tr>
</tbody>
</table>
</section>
<section id="other-highlights">
<h3>
<span class="section-number">4.4.3. </span>Other highlights<a class="headerlink" href="index.html#other-highlights" title="Permalink to this headline"></a>
</h3>
<ol class="arabic">
<li><p>The length of the requested mapping and base address must be a multiple of 4KB, failing which leads to an error.</p></li>
<li><p>Unlike the Desktop version, callback registered at <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_get_pages()</span></code> will always be triggered when <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_put_pages()</span></code> is invoked. It is the reponsibilty of the kernel driver to free the page_table allocated by calling <code class="docutils literal notranslate"><span class="pre">nvidia_p2p_free_page_table()</span></code>. Note that, similar to the Desktop version, the callback will also triggered in scenarios explained in <a class="reference internal" href="index.html#unpin-callback"><span class="std std-ref">Unpin Callback</span></a>.</p></li>
<li>
<p>Since <code class="docutils literal notranslate"><span class="pre">cudaHostAlloc()</span></code> can be allocated with <code class="docutils literal notranslate"><span class="pre">cudaHostAllocWriteCombined</span></code> flag or default flag, applications are expected to excercise caution when mapping the memory to userspace, for example using standard linux <code class="docutils literal notranslate"><span class="pre">mmap()</span></code>. In this regard:</p>
<ol class="loweralpha simple">
<li><p>When GPU memory is allocated as writecombined, the userspace mapping should also be done as writecombined by passing the <code class="docutils literal notranslate"><span class="pre">vm_page_prot</span></code> member of <code class="docutils literal notranslate"><span class="pre">vm_area_struct</span></code> to the standard linux interface: <code class="docutils literal notranslate"><span class="pre">`pgprot_writecombine()</span></code> &lt;<a class="reference external" href="https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/pgtable.h#L403">https://elixir.bootlin.com/linux/latest/source/arch/arm64/include/asm/pgtable.h#L403</a>&gt;`__.</p></li>
<li><p>When GPU memory is allocated as default, no modifcations to the <code class="docutils literal notranslate"><span class="pre">vm_page_prot</span></code> member of <code class="docutils literal notranslate"><span class="pre">vm_area_struct</span></code> should be done.</p></li>
</ol>
<p>Incompatible combination of map and allocation attributes will lead to undefined behavior.</p>
</li>
</ol>
</section>
</section>
</section>
<section id="notices">
<h1>
<span class="section-number">5. </span>Notices<a class="headerlink" href="index.html#notices" title="Permalink to this headline"></a>
</h1>
<section id="notice">
<h2>
<span class="section-number">5.1. </span>Notice<a class="headerlink" href="index.html#notice" title="Permalink to this headline"></a>
</h2>
<p>This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.</p>
<p>NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.</p>
<p>Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.</p>
<p>NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.</p>
<p>NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.</p>
<p>NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.</p>
<p>No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.</p>
<p>Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.</p>
<p>THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.</p>
</section>
<section id="opencl">
<h2>
<span class="section-number">5.2. </span>OpenCL<a class="headerlink" href="index.html#opencl" title="Permalink to this headline"></a>
</h2>
<p>OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.</p>
</section>
<section id="trademarks">
<h2>
<span class="section-number">5.3. </span>Trademarks<a class="headerlink" href="index.html#trademarks" title="Permalink to this headline"></a>
</h2>
<p>NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr>

  <div role="contentinfo">
<img src="../_static/NVIDIA-LogoBlack.svg" class="only-light">
<img src="../_static/NVIDIA-LogoWhite.svg" class="only-dark">

<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>

<p>
  Copyright © 2012-2024, NVIDIA Corporation &amp; affiliates. All rights reserved.
</p>

    <p>
      <span class="lastupdated">Last updated on Aug 1, 2024.
      </span></p>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>
 
<script type="text/javascript">if (typeof _satellite !== "undefined"){_satellite.pageBottom();}</script>
 


<script type="text/javascript">_satellite.pageBottom();</script>
</body>
</html>
