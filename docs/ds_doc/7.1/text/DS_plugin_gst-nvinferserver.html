

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" type="text/javascript"
  data-document-language="true" charset="UTF-8" data-domain-script="3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5">
</script>
<script type="text/javascript">
  function OptanonWrapper() {
    var event = new Event('bannerLoaded');
    window.dispatchEvent(event);
  }
</script>
<script src="https://images.nvidia.com/aem-dam/Solutions/ot-js/ot-custom.js" type="text/javascript">
</script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Gst-nvinferserver &#8212; DeepStream documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css%3Fv=a746c00c.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/nvidia-sphinx-theme.css%3Fv=eb367b29.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css%3Fv=7abaf8bc.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css%3Fv=95c83b7e.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js%3Fdigest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js%3Fdigest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js%3Fv=22d9b4cb"></script>
    <script src="../_static/doctools.js%3Fv=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js%3Fv=dc90522c"></script>
    <script src="../_static/design-tabs.js%3Fv=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'text/DS_plugin_gst-nvinferserver';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '../versions1.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '7.1';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="../_static/version-patch.js%3Fv=c24f8c5d"></script>
    <link rel="icon" href="../_static/Nvidia.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gst-nvtracker" href="DS_plugin_gst-nvtracker.html" />
    <link rel="prev" title="Gst-nvinfer" href="DS_plugin_gst-nvinfer.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Sep 15, 2025"/>

    <script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js" ></script>
    


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="DS_plugin_gst-nvinferserver.html#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="DeepStream documentation - Home"/>
    <script>document.write(`<img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark" alt="DeepStream documentation - Home"/>`);</script>
  
  
    <p class="title logo__title">DeepStream documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="DeepStream documentation - Home"/>
    <script>document.write(`<img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark" alt="DeepStream documentation - Home"/>`);</script>
  
  
    <p class="title logo__title">DeepStream documentation</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">


<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Overview.html">Welcome to the DeepStream Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Migration_guide.html">Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Quickstart.html">Quickstart Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_docker_containers.html">Docker Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Samples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_C_Sample_Apps.html">C/C++ Sample Apps Source Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Python_Sample_Apps.html">Python Sample Apps and Bindings Source Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_deepstream.html">DeepStream Reference Application - deepstream-app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_test5.html">DeepStream Reference Application - deepstream-test5 app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_nmos.html">DeepStream Reference Application - deepstream-nmos app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_github.html">DeepStream Reference Application on GitHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_sample_configs_streams.html">Sample Configurations and Streams</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_sample_custom_gstream.html">Implementing a Custom GStreamer Plugin with OpenCV Integration Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TAO toolkit Integration with DeepStream</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_TAO_integration.html">TAO Toolkit Integration with DeepStream</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials and How-to's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Custom_Manual.html">DeepStream-3D Custom Apps and Libs Tutorials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Performance.html">Performance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Accuracy</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Accuracy.html">Accuracy Tuning Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Custom Model</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_using_custom_model.html">Using a Custom Model with DeepStream</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Key Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Sensor_Fusion.html">DeepStream-3D Sensor Fusion Multi-Modal Application and Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Camera_BEVFusion.html">DeepStream-3D Multi-Modal BEVFusion Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Camera_V2XFusion.html">DeepStream-3D Multi-Modal V2XFusion Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Smart_video.html">Smart Video Record</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_IoT.html">IoT</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_on_the_fly_model.html">On the Fly Model Update</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_NTP_Timestamp.html">NTP Timestamp in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_AVSync.html">AV Sync in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_RestServer.html">DeepStream With REST API Sever</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Action.html">DeepStream 3D Action Recognition App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Depth_Camera.html">DeepStream 3D Depth Camera App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Lidar_Inference.html">DeepStream 3D Lidar Inference App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_library_nvdsnmos.html">Networked Media Open Specifications (NMOS) in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_postprocessing_plugin.html">Gst-nvdspostprocess in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Can_Orientation.html">DeepStream Can Orientation App</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Application Migration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Application_migration.html">Application Migration to DeepStream 7.1 from DeepStream 7.0</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Plugin Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="DS_plugin_Intro.html">GStreamer Plugin Overview</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_metadata.html">MetaData in the DeepStream SDK</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdspreprocess.html">Gst-nvdspreprocess (Alpha)</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvinfer.html">Gst-nvinfer</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="DS_plugin_gst-nvinferserver.html#">Gst-nvinferserver</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvtracker.html">Gst-nvtracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreammux.html">Gst-nvstreammux</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreammux2.html">Gst-nvstreammux New</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreamdemux.html">Gst-nvstreamdemux</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmultistreamtiler.html">Gst-nvmultistreamtiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsosd.html">Gst-nvdsosd</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsmetautils.html">Gst-nvdsmetautils</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsvideotemplate.html">Gst-nvdsvideotemplate</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsaudiotemplate.html">Gst-nvdsaudiotemplate</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideoconvert.html">Gst-nvvideoconvert</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdewarper.html">Gst-nvdewarper</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvof.html">Gst-nvof</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvofvisual.html">Gst-nvofvisual</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvsegvisual.html">Gst-nvsegvisual</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideo4linux2.html">Gst-nvvideo4linux2</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvjpegdec.html">Gst-nvjpegdec</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvimagedec.html">Gst-nvimagedec</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvjpegenc.html">Gst-nvjpegenc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvimageenc.html">Gst-nvimageenc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmsgconv.html">Gst-nvmsgconv</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmsgbroker.html">Gst-nvmsgbroker</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsanalytics.html">Gst-nvdsanalytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsudpsrc.html">Gst-nvdsudpsrc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsudpsink.html">Gst-nvdsudpsink</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdspostprocess.html">Gst-nvdspostprocess (Alpha)</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dfilter.html">Gst-nvds3dfilter</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dbridge.html">Gst-nvds3dbridge</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dmixer.html">Gst-nvds3dmixer</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsucx.html">Gst-NvDsUcx</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsxfer.html">Gst-nvdsxfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideotestsrc.html">Gst-nvvideotestsrc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmultiurisrcbin.html">Gst-nvmultiurisrcbin</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvurisrcbin.html">Gst-nvurisrcbin</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Troubleshooting and FAQ</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_FAQ.html">Frequently Asked Questions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream On WSL2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_on_WSL2.html">DeepStream On WSL</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_WSL2_FAQ.html">FAQ for Deepstream On WSL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream API Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_API_Guide.html">DeepStream API Guides</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Service Maker</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_intro.html">What is Deepstream Service Maker</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_cpp.html">Service Maker for C/C++ Developers</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="DS_service_maker_python.html">Service Maker for Python Developers(alpha)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_quick_start.html">Quick Start Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_into_to_flow_api.html">Introduction to Flow APIs</a></li>

<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_into_to_pipeline_api.html">Introduction to Pipeline APIs</a></li>

<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_advanced_features.html">Advanced Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_traditional_app_migration.html">Migrating Traditional Deepstream Apps to Service Maker Apps in Python</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_plugin.html">What is a Deepstream Service Maker Plugin</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deepstream Libraries</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Libraries.html">DeepStream Libraries (Developer Preview)</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph Composer</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_intro.html">Overview</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Platforms.html">Supported platforms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Getting_Started.html">Application Development Workflow</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_GraphComposer_Create_Graph.html">Creating an AI Application</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_Sample_Graphs.html">Reference graphs</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Dev_Workflow.html">Extension Development Workflow</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_Developing_Extension.html">Developing Extensions for DeepStream</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_DS_Components.html">DeepStream Components</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Internals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Internals.html">GXF Internals</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph eXecution Engine</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Graph_Runtime.html">Graph Execution Engine</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph Composer Containers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Containers.html">Graph Composer and GXF Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Component Interfaces</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Component_Interfaces.html">GXF Component Interfaces</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Application API's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_App_C++_APIs.html">GXF App C++ APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_App_Python_APIs.html">GXF App Python APIs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Runtime API's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_C++_APIs.html">GXF Core C++ APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_C_APIs.html">GXF Core C APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_Python_APIs.html">GXF Core Python APIs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Extension Manual</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="Extensionmanual_toc.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/CudaExtension.html">CudaExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/StreamSync.html">GXF Stream Sync</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/StandardExtension.html">StandardExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/Python_Codelet.html">Python Codelets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/NetworkExtension.html">NetworkExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/NvTritonExt.html">NvTritonExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/SerializationExtension.html">SerializationExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/MultimediaExtension.html">MultimediaExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/VideoEncoderExtension.html">VideoEncoderExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/VideoDecoderExtension.html">VideoDecoderExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/Behavior_Tree.html">Behavior Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/UcxExtension.html">UCX Extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/HttpExtension.html">HttpExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/GrpcExtension.html">GrpcExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/TensorrtExtension.html">TensorRTExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDs3dProcessingExt.html">NvDs3dProcessingExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsActionRecognitionExt.html">NvDsActionRecognitionExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsAnalyticsExt.html">NvDsAnalyticsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsBaseExt.html">NvDsBaseExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsCloudMsgExt.html">NvDsCloudMsgExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsConverterExt.html">NvDsConverterExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsDewarperExt.html">NvDsDewarperExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInferenceExt.html">NvDsInferenceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInferenceUtilsExt.html">NvDsInferenceUtilsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInterfaceExt.html">NvDsInterfaceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsMuxDemuxExt.html">NvDsMuxDemuxExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsOpticalFlowExt.html">NvDsOpticalFlowExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsOutputSinkExt.html">NvDsOutputSinkExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSampleExt.html">NvDsSampleExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSampleModelsExt.html">NvDsSampleModelsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSourceExt.html">NvDsSourceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTemplateExt.html">NvDsTemplateExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTrackerExt.html">NvDsTrackerExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTranscodeExt.html">NvDsTranscodeExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTritonExt.html">NvDsTritonExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsUcxExt.html">NvDsUcxExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsUdpExt.html">NvDsUdpExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsVisualizationExt.html">NvDsVisualizationExt</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Registry.html">Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Registry_CLI.html">Registry Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Composer.html">Composer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Container_Builder.html">Container Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_gxf_CLI.html">GXF Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pipetuner-guide.html">Pipetuner Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">FAQ Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_FAQ.html">FAQ</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Legal Information</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Legal.html">DeepStream End User License Agreement</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Feedback</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DeepStream_Main_Feedback_Form.html">Feedback form</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">


<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
  </div>
  
  <div id="rtd-footer-container"></div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="DS_plugin_Intro.html" class="nav-link">GStreamer Plugin Overview</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Gst-nvinferserver</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="gst-nvinferserver">
<span id="ds-plugin-nvinferserver"></span><h1>Gst-nvinferserver<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#gst-nvinferserver" title="Link to this heading">#</a></h1>
<p>The Gst-nvinferserver plugin does inferencing on input data using NVIDIA® Triton Inference Server (previously called TensorRT Inference Server) Release 2.49.0, NGC Container 24.08 for both Jetson and dGPU on x86. Refer to the following README <a class="github reference external" href="https://github.com/triton-inference-server/server/blob/r24.08/README.md">triton-inference-server/server</a>
The plugin accepts batched <code class="docutils literal notranslate"><span class="pre">NV12/RGBA</span></code> buffers from upstream. The NvDsBatchMeta structure must already be attached to the Gst Buffers.</p>
<p>The low-level library (libnvds_infer_server) operates on any of NV12 or RGBA buffers.
The Gst-nvinferserver plugin passes the input batched buffers to the low-level library and waits for the results to be available. Meanwhile, it keeps queuing input buffers to the low-level library as they are received. Once the results are available from the low-level library, the plugin translates and attaches the results back in to Gst-buffer for downstream plugins.</p>
<p>The low-level library preprocesses the transformed frames (performs color conversion and scaling, normalization and mean subtraction) and produces final <code class="docutils literal notranslate"><span class="pre">FP32/FP16/INT8/UINT8/INT16/UINT16/INT32/UINT32</span> <span class="pre">RGB/BGR/GRAY</span> <span class="pre">planar/</span></code> packed data which is passed to the Triton for inferencing. The output type generated by the low-level library depends on the network type.</p>
<p>The pre-processing function is:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">netscalefactor</span> <span class="pre">*</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">mean)</span></code></p>
</div></blockquote>
<p>Where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">x</span></code> is the input pixel value. It is an uint8 with range [0,255].</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span></code> is the corresponding mean value, read either from the mean file or as offsets[c], where c is the channel to which the input pixel belongs, and offsets is the array specified in the configuration file. It is a float.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">netscalefactor</span></code> is the pixel scaling factor specified in the configuration file. It is a float.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y</span></code> is the corresponding output pixel value. It can be of type <code class="docutils literal notranslate"><span class="pre">float</span> <span class="pre">/</span> <span class="pre">half</span> <span class="pre">/</span> <span class="pre">int8</span> <span class="pre">/</span> <span class="pre">uint8</span> <span class="pre">/</span> <span class="pre">int16</span> <span class="pre">/</span> <span class="pre">uint16</span> <span class="pre">/</span> <span class="pre">int32</span> <span class="pre">/</span> <span class="pre">uint32</span></code>.</p>
<blockquote>
<div><p>Take specific example for uint8 to int8 conversion. set <code class="docutils literal notranslate"><span class="pre">netscalefactor</span> <span class="pre">=</span> <span class="pre">1.0</span></code> and <code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">=</span> <span class="pre">[128,</span> <span class="pre">128,</span> <span class="pre">128]</span></code>. Then the function looks like:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">(1.0)</span> <span class="pre">*</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">128)</span></code></p>
</div></blockquote>
</div></blockquote>
</li>
</ul>
<p>Gst-nvinferserver currently works on the following type of networks:</p>
<ul class="simple">
<li><p>Multi-class object detection</p></li>
<li><p>Multi-label classification</p></li>
<li><p>Segmentation</p></li>
</ul>
<p>The Gst-nvinferserver plugin can work in three process modes:</p>
<ul>
<li><p>Primary mode: Operates on full frames.</p></li>
<li><p>Secondary mode: Operates on objects added in the metadata by upstream components.</p>
<blockquote>
<div><p>When the plugin is operating as a secondary classifier in <cite>async</cite> mode along with the tracker, it tries to improve performance by avoiding re-inferencing on the same objects in every frame. It does this by caching the classification output in a map with the object’s unique ID as the key. The object is inferred upon only when it is first seen in a frame (based on its object ID) or when the size (bounding box area) of the object increases by 20% or more. This optimization is possible only when the tracker is added as an upstream element.</p>
</div></blockquote>
</li>
<li><p>Preprocessed Tensor Input mode: Operates on tensors attached by upstream components.</p>
<blockquote>
<div><p>When operating in preprocessed tensor input mode, the pre-processing inside Gst-nvinferserver is completely skipped. The plugin looks for <code class="docutils literal notranslate"><span class="pre">GstNvDsPreProcessBatchMeta</span></code> attached to the input buffer and passes the tensor as is to the Tirton Inference Server without any modifications. This mode currently supports processing on full-frame and ROI. The GstNvDsPreProcessBatchMeta is attached by the Gst-nvdspreprocess plugin. This mode is enabled by adding the input_tensor_from_meta configuration message in the InferenceConfig message.</p>
</div></blockquote>
</li>
</ul>
<p>Detailed documentation of the Triton Inference Server is available at: <a class="github reference external" href="https://github.com/triton-inference-server/server/blob/r24.08/README.md">triton-inference-server/server</a></p>
<p>The plugin supports Triton features along with multiple deep-learning frameworks such as TensorRT, TensorFlow (GraphDef / SavedModel), ONNX and PyTorch on Tesla platforms. On Jetson, it also supports TensorRT and TensorFlow (GraphDef / SavedModel). TensorFlow and ONNX can be configured with TensorRT acceleration. For details, see <a class="reference external" href="https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/optimization.md#framework-specific-optimization">Framework-Specific Optimization</a>.
The plugin requires a configurable model repository root directory path where all the models need to reside. All the plugin instances in a single process must share the same model root. For details, see <a class="reference external" href="https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_repository.md">Model Repository</a>. Each model also needs a specific <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code> file in its subdirectory. For details, see <a class="reference external" href="https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/model_configuration.md">Model Configuration</a>.
The plugin supports Triton ensemble mode to enable users to perform preprocessing or postprocessing with Triton custom backend.
The plugin also supports the interface for custom functions for parsing outputs of object detectors, classifiers, and initialization of non-image input layers in cases where there is more than one input layer.
Refer to <code class="docutils literal notranslate"><span class="pre">sources/includes/nvdsinfer_custom_impl.h</span></code> for the custom method implementations for custom models.</p>
<img alt="Gst-nvinferserver" class="align-center" src="../_images/DS_plugin_gst-nvinferserver.png" />
<p>Downstream components receive a Gst Buffer with unmodified contents plus the metadata created from the inference output of the Gst-nvinferserver plugin. The plugin can be used for cascaded inferencing. That is, it can perform primary inferencing directly on input data, then perform secondary inferencing on the results of primary inferencing, and so on. This is similar with Gst-nvinfer, see more details in Gst-nvinfer.</p>
<section id="inputs-and-outputs">
<h2>Inputs and Outputs<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#inputs-and-outputs" title="Link to this heading">#</a></h2>
<p>This section summarizes the inputs, outputs, and communication facilities of the Gst-nvinferserver plugin.</p>
<ul>
<li><p>Inputs</p>
<blockquote>
<div><ul class="simple">
<li><p>Gst Buffer</p></li>
<li><p>NvDsBatchMeta (attaching NvDsFrameMeta)</p></li>
<li><p>Model repository directory path (model_repo.root)</p></li>
<li><p>gRPC endpoint URL (grpc.url)</p></li>
<li><p>Runtime model file with config.pbtxt file in model repository</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Control parameters</p>
<blockquote>
<div><blockquote>
<div><ul>
<li><p>Gst-nvinferserver gets control parameters from a configuration file. You can specify this by setting the property config-file-path. For details, see <a class="reference internal" href="DS_plugin_gst-nvinferserver.html#gst-nvinferserver-configuration-file-specifications">Gst-nvinferserver Configuration File Specifications</a>. Other control parameters that can be set through GObject properties are:</p>
<blockquote>
<div><ul class="simple">
<li><p>Batch size</p></li>
<li><p>Process mode</p></li>
<li><p>Unique id</p></li>
<li><p>Inference on GIE id and operate on class ids [secondary mode only]</p></li>
<li><p>Inference interval</p></li>
<li><p>Raw output generated callback function</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<ul class="simple">
<li><p>The parameters set through the GObject properties override the parameters in the Gst-nvinferserver configuration file.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Outputs</p>
<blockquote>
<div><ul>
<li><p>Gst Buffer</p></li>
<li><p>Depending on network type and configured parameters, one or more of:</p>
<blockquote>
<div><ul class="simple">
<li><p>NvDsObjectMeta</p></li>
<li><p>NvDsClassifierMeta</p></li>
<li><p>NvDsInferSegmentationMeta</p></li>
<li><p>NvDsInferTensorMeta</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</section>
<section id="gst-nvinferserver-configuration-file-specifications">
<h2>Gst-nvinferserver Configuration File Specifications<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#gst-nvinferserver-configuration-file-specifications" title="Link to this heading">#</a></h2>
<p>The Gst-nvinferserver configuration file uses prototxt format described in <a class="reference external" href="https://developers.google.com/protocol-buffers">https://developers.google.com/protocol-buffers</a>.</p>
<p>The protobuf message structures of this configuration file are defined by <code class="docutils literal notranslate"><span class="pre">nvdsinferserver_plugin.proto</span></code> and <code class="docutils literal notranslate"><span class="pre">nvdsinferserver_config.proto</span></code>. All the basic data-type values are set to 0 or false from protobuf’s guide. Map, arrays and oneof are set to empty by default. See more details for each message definition.</p>
<ul class="simple">
<li><p>The message PluginControl in <code class="docutils literal notranslate"><span class="pre">nvdsinferserver_plugin.proto</span></code> is the entry point for this config-file.</p></li>
<li><p>The message InferenceConfig configures the low-level settings for <code class="docutils literal notranslate"><span class="pre">libnvds_infer_server</span></code>.</p></li>
<li><p>The message PluginControl::InputControl configures the input buffers, objects filtering policy for model inference.</p></li>
<li><p>The message PluginControl::OutputControl configures inference output policy for detections and raw tensor metadata.</p></li>
<li><p>The message BackendParams configures backend input/output layers and Triton settings in InferenceConfig.</p></li>
<li><p>The message PreProcessParams configures network preprocessing information in InferenceConfig.</p></li>
<li><p>The message InputTensorFromMeta enables the preprocessed tensor input mode and configures the input tensor information in InferenceConfig.</p></li>
<li><p>The message PostProcessParams configures the output tensor parsing methods such as detection, classification, semantic segmentation and others in InferenceConfig.</p></li>
<li><p>There are also other messages (e.g. CustomLib, ExtraControl) and enum types (e.g. MediaFormat, TensorOrder, …) defined in the proto file for miscellaneous settings for InferenceConfig and PluginControl.</p></li>
</ul>
</section>
<section id="features">
<h2>Features<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#features" title="Link to this heading">#</a></h2>
<p>The following table summarizes the features of the plugin.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id3">
<caption><span class="caption-text">Gst-nvinferserver plugin features</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id3" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>dGPU</p></th>
<th class="head"><p>Jetson</p></th>
<th class="head"><p>Release</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Gst-nvinferserver Running on Host</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Running on Docker Image</p></td>
<td><p>Yes</p></td>
<td><p>Yes(DS 6.0)</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>DS Preprocessing: Network input format: RGB/BGR/Gray</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>DS Preprocessing: Network input data types FP32/FP16/UINT8/INT8/UINT16/INT16/UINT32/INT32</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>DS Preprocessing: Network input tensor orders
NCHW / NHWC</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Mem: Cuda(GPU) buf-sharing for Input Tensors</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Mem: Cuda Memory (GPU / CPU-pinned) for output tensors</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Backend: TensorRT runtime (plan engine file)</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Backend: Tensorflow Runtime CPU/GPU (graphdef/savedmodel)</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Backend: Tensorflow Runtime with TF-TRT acceleration</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Backend: ONNX Runtime</p></td>
<td><p>Yes</p></td>
<td><p>Yes(DS 6.0)</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Backend: ONNX Runtime with ONNX-TRT acceleration</p></td>
<td><p>Yes</p></td>
<td><p>Yes(DS 6.0)</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Backend: Pytorch Runtime</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Postprocessing: DS Detection / Classification/ Segmentation</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Postprocessing: DS Detection cluster method: NMS / GroupRectangle / DBSCan / None</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Postprocessing: custom parsing (NvDsInferParseCustomTfSSD)</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Postprocessing: Triton native classification</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Triton Ensemble Mode (Triton preproc/postproc) with specified media-format (RGB/BGR/Gray) with Cuda GPU buffer as inputs</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Postprocessing: Attach Triton raw tensor output in NvDsInferTensorMeta for downstream or application postprocessing</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>deepstream-app: pipeline works with PGIE / SGIE / nvtracker</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Sample App: deepstream-segmentation-test / deepstream-infer-tensor-meta-test</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-odd"><td><p>Basic LSTM features on single batch and single stream (beta version, config file might be changed in future version)</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>gRPC: Triton Server running as independent process and plugin communicates through gRPC</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 6.0</p></td>
</tr>
<tr class="row-odd"><td><p>Custom process interface <code class="docutils literal notranslate"><span class="pre">IInferCustomProcessor</span></code> for extra multi-input tensors preprocess, multi-streams LSTM loop process, custom output data postprocess(parsing and metadata attaching). Basic single-stream LSTM could be replaced by custom loop process</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 6.0</p></td>
</tr>
<tr class="row-even"><td><p>gRPC: CUDA buffer sharing with local Triton server for input tensors</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 6.2</p></td>
</tr>
<tr class="row-odd"><td><p>Postprocessing: Clip object bounding boxes to fit within the ROI</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>DS 6.3</p></td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id4">
<caption><span class="caption-text">Gst-nvinferserver plugin features message PluginControl definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id4" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs</div>
<div class="line">(Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>infer_config</p></td>
<td><p>Low-level libnvds_infer_server inference configuration settings</p></td>
<td><p>InferenceConfig</p></td>
<td><p>infer_config { … }
see details in InferenceConfig</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>input_control</p></td>
<td><p>Control plugin input buffers, objects filtering policy for inference</p></td>
<td><p>PluginControl ::InputControl</p></td>
<td><p>input_control{
process_mode: PROCESS_MODE_FULL_FRAME
}
see details in InputControl</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>output_control</p></td>
<td><p>Control plugin output metadata filtering policy after inference</p></td>
<td><p>PluginControl ::OutputControl</p></td>
<td><p>output_control { … }
see details in OutputControl</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>process_mode</p></td>
<td><p>Processing mode, selected from PluginControl::ProcessMode. In deepstream-app PGIE uses PROCESS_MODE_FULL_FRAME by default, SGIE use PROCESS_MODE_CLIP_OBJECTS by default</p></td>
<td><p>enum PluginControl::ProcessMode</p></td>
<td><p>process_mode: PROCESS_MODE_FULL_FRAME</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>operate_on_gie_id</p></td>
<td><p>Unique ID of the GIE on whose metadata (bounding boxes) this GIE is to operate on</p></td>
<td><p>int32, &gt;=0, valid gie-id.
-1, disable gie-id check, inference on all GIE Ids</p></td>
<td><p>operate_on_gie_id: 1</p></td>
<td><p>All</p>
<p>Secondary</p>
</td>
</tr>
<tr class="row-odd"><td><p>operate_on_class_ids</p></td>
<td><p>Class IDs of the parent GIE on which this GIE is to operate on</p></td>
<td><p>Comma delimited int32 array</p></td>
<td><p>operate_on_class_ids: [1, 2]
Operates on objects with class IDs 1, 2 generated by parent GIE</p></td>
<td><p>All</p>
<p>Secondary</p>
</td>
</tr>
<tr class="row-even"><td><p>interval</p></td>
<td><p>Specifies the number of consecutive, batches to be skipped for inference. default is 0</p></td>
<td><p>uint32</p></td>
<td><p>interval: 1</p></td>
<td><p>All</p>
<p>Primary</p>
</td>
</tr>
<tr class="row-odd"><td><p>async_mode</p></td>
<td><p>Enables inference on detected objects and asynchronous metadata attachments. Works only when tracker-ids are attached. Pushes buffer downstream without waiting for inference results. Attaches metadata after the inference</p></td>
<td><p>bool</p></td>
<td><p>async_mode: false</p></td>
<td><p>Classifier</p>
<p>Secondary</p>
</td>
</tr>
<tr class="row-even"><td><p>object_control</p></td>
<td><p>input object filter settings</p></td>
<td><p>PluginControl::InputObjectControl</p></td>
<td><p>object_control {
bbox_filter {
min_width: 64
min_height: 64
}
}
see details in
InputObjectControl</p></td>
<td><p>All</p>
<p>Secondary</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id5">
<caption><span class="caption-text">Gst-nvinferserver plugin message PluginControl-InputControl definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id5" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs</div>
<div class="line">(Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>process_mode</p></td>
<td><p>Processing mode, selected from PluginControl::ProcessMode. In deepstream-app PGIE uses PROCESS_MODE_FULL_FRAME by default, SGIE use PROCESS_MODE_CLIP_OBJECTS by default</p></td>
<td><p>enum PluginControl::ProcessMode</p></td>
<td><p>process_mode: PROCESS_MODE_FULL_FRAME</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>operate_on_gie_id</p></td>
<td><p>Unique ID of the GIE on whose metadata (bounding boxes) this GIE is to operate on</p></td>
<td><p>int32, &gt;=0, valid gie-id.
-1, disable gie-id check, inference on all GIE Ids</p></td>
<td><p>operate_on_gie_id: 1</p></td>
<td><p>All</p>
<p>Secondary</p>
</td>
</tr>
<tr class="row-even"><td><p>operate_on_class_ids</p></td>
<td><p>Class IDs of the parent GIE on which this GIE is to operate on</p></td>
<td><p>Comma delimited int32 array</p></td>
<td><p>operate_on_class_ids: [1, 2]
Operates on objects with class IDs 1, 2 generated by parent GIE</p></td>
<td><p>All</p>
<p>Secondary</p>
</td>
</tr>
<tr class="row-odd"><td><p>interval</p></td>
<td><p>Specifies the number of consecutive, batches to be skipped for inference. default is 0</p></td>
<td><p>uint32</p></td>
<td><p>interval: 1</p></td>
<td><p>All</p>
<p>Primary</p>
</td>
</tr>
<tr class="row-even"><td><p>async_mode</p></td>
<td><p>Enables inference on detected objects and asynchronous metadata attachments. Works only when tracker-ids are attached. Pushes buffer downstream without waiting for inference results. Attaches metadata after the inference</p></td>
<td><p>bool</p></td>
<td><p>async_mode: false</p></td>
<td><p>Classifier</p>
<p>Secondary</p>
</td>
</tr>
<tr class="row-odd"><td><p>object_control</p></td>
<td><p>input object filter settings</p></td>
<td><p>PluginControl::InputObjectControl</p></td>
<td><p>object_control {
bbox_filter {
min_width: 64
min_height: 64
}
}
see details in
InputObjectControl</p></td>
<td><p>All</p>
<p>Secondary</p>
</td>
</tr>
<tr class="row-even"><td><p>secondary_reinfer_interval</p></td>
<td><p>Re-inference interval for objects, in frames</p></td>
<td><p>uint32</p></td>
<td><p>secondary_reinfer_interval: 90</p></td>
<td><p>All</p>
<p>Secondary</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id6">
<caption><span class="caption-text">Gst-nvinferserver plugin message PluginControl-OutputControl definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id6" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs</div>
<div class="line">(Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>output_tensor_meta</p></td>
<td><p>Enable attaching Inference output tensor metadata, tensor buffer pointer for host only</p></td>
<td><p>bool</p></td>
<td><p>output_tensor_meta: false</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>detect_control</p></td>
<td><p>Specifies detection output filter policy</p></td>
<td><p>PluginControl::OutputDetectionControl</p></td>
<td><p>detect_control {
default_filter {
bbox_filter {
min_width: 32
min_height: 32
}
}
}
see details in OutputDetectionControl</p></td>
<td><div class="line-block">
<div class="line">Detector</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>classifier_type</p></td>
<td><p>The classifier type to be added in the NvDsClassifierMeta in case of classification networks</p></td>
<td><p>string</p></td>
<td><p>classifier_type: multi_class_classification</p></td>
<td><div class="line-block">
<div class="line">Classifier</div>
<div class="line">Both</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id7">
<caption><span class="caption-text">Gst-nvinferserver plugin message PluginControl-InputObjectControl definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id7" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs</div>
<div class="line">(Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bbox_filter</p></td>
<td><p>Bounding box filter</p></td>
<td><p>PluginControl::BBoxFilter</p></td>
<td><p>bbox_filter {
min_width: 32
min_height: 32
}
see details in BBoxFilter</p></td>
<td><p>All</p>
<p>Secondary</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id8">
<caption><span class="caption-text">Gst-nvinferserver plugin message PluginControl-BBoxFilter definition details for Input and Output controls</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id8" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>min_width</p></td>
<td><p>Bounding box minimum width</p></td>
<td><p>uint32</p></td>
<td><p>min_width: 64</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>min_height</p></td>
<td><p>Bounding box minimum height</p></td>
<td><p>uint32</p></td>
<td><p>min_height: 64</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>max_width</p></td>
<td><p>Bounding box maximum width, default 0, max_width is ignored</p></td>
<td><p>uint32</p></td>
<td><p>max_width: 640</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>max_height</p></td>
<td><p>Bounding box maximum height, 
default 0, max_height is ignored</p></td>
<td><p>uint32</p></td>
<td><p>max_height: 640</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id9">
<caption><span class="caption-text">Gst-nvinferserver plugin message PluginControl-OutputDetectionControl definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id9" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>default_filter</p></td>
<td><p>default detection filter for output controls</p></td>
<td><p>PluginControl::DetectClassFilter</p></td>
<td><p>default_filter {
bbox_filter {
min_width: 32
min_height: 32
}
}
see details in DetectClassFilter</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>specific_class_filters</p></td>
<td><p>specifies detection filters per class to replace default filter</p></td>
<td><p>map&lt;uint32, DetectClassFilter&gt;</p></td>
<td><p>specific_class_filters: [
{ key: 1, value {…} },
{ key: 2,
value {…} }
]</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id10">
<caption><span class="caption-text">Gst-nvinferserver plugin message PluginControl-DetectClassFilter definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id10" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bbox_filter</p></td>
<td><p>detection bounding box filter</p></td>
<td><p>PluginControl::BBoxFilter</p></td>
<td><p>bbox_filter {
min_width: 64
min_height: 64
}</p></td>
<td><div class="line-block">
<div class="line">Detection</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>roi_top_offset</p></td>
<td><p>Offset of the RoI from the top of the frame. Only objects within the RoI are output.</p></td>
<td><p>uint32</p></td>
<td><p>roi_top_offset: 128</p></td>
<td><div class="line-block">
<div class="line">Detection</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>roi_bottom_offset</p></td>
<td><p>Offset of the RoI from the bottom of the frame. Only objects within the RoI are output.</p></td>
<td><p>uint32</p></td>
<td><p>roi_bottom_offset:</p></td>
<td><div class="line-block">
<div class="line">Detection</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>border_color</p></td>
<td><p>specify border color for detection bounding boxes</p></td>
<td><p>PluginControl::Color</p></td>
<td><p>border_color { 
r: 1.0
g: 0.0
b: 0.0
a: 1.0
}</p></td>
<td><div class="line-block">
<div class="line">Detection</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>bg_color</p></td>
<td><p>specify background color for detection bounding boxes</p></td>
<td><p>PluginControl::Color</p></td>
<td><p>border_color { 
r: 0.0
g: 1.0
b: 0.0
a: 0.5
}</p></td>
<td><div class="line-block">
<div class="line">Detection</div>
<div class="line">Both</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id11">
<caption><span class="caption-text">Gst-nvinferserver plugin message PluginControl-Color definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id11" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>r</p></td>
<td><p>Red color value</p></td>
<td><p>float
Range[0.0, 1.0]</p></td>
<td><p>r: 0.5</p></td>
<td><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>g</p></td>
<td><p>Green color value</p></td>
<td><p>float. Range[0.0, 1.0]</p></td>
<td><p>g: 0.5</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>b</p></td>
<td><p>Blue color value</p></td>
<td><p>float. Range[0.0, 1.0]</p></td>
<td><p>b: 0.3</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>a</p></td>
<td><p>Alpha blending value</p></td>
<td><p>float. Range[0.0, 1.0]</p></td>
<td><p>a: 1.0</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<section id="low-level-libnvds-infer-server-so-configuration-file-specifications">
<span id="ds-plugin-gst-nvinferserver-low-level-spec"></span><h3>Low Level <code class="docutils literal notranslate"><span class="pre">libnvds_infer_server.so</span></code> Configuration File Specifications<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#low-level-libnvds-infer-server-so-configuration-file-specifications" title="Link to this heading">#</a></h3>
<p>The message InferenceConfig defines all the low-level structure fields in <code class="docutils literal notranslate"><span class="pre">nvdsinferserver_config.proto</span></code>. It has major settings for inference backend, network preprocessing and postprocessing.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id12">
<caption><span class="caption-text">Gst-nvinferserver message InferenceConfig definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id12" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><blockquote>
<div><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</div></blockquote>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>unique_id</p></td>
<td><p>Unique ID identifying metadata generated by this GIE</p></td>
<td><p>uint32, ≥0</p></td>
<td><p>unique_id: 1</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p>gpu_ids</p></td>
<td><p>Device IDs of GPU to use for pre-processing/inference (single GPU support only)</p></td>
<td><p>int32 array, ≥0</p></td>
<td><p>gpu_ids: [0]</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-even"><td><p>max_batch_size</p></td>
<td><p>Max number of frames/objects to be inferred together in a batch</p></td>
<td><p>uint32, ≥0</p></td>
<td><p>max_batch_size: 1</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p>backend</p></td>
<td><p>Inference backend settings</p></td>
<td><p>BackendParams</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">backend</span> <span class="pre">{</span>
<span class="pre">trt_is</span> <span class="pre">{</span> <span class="pre">...</span> <span class="pre">}</span>
<span class="pre">}</span></code>
see details in BackendParams</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-even"><td><p>preprocessing</p></td>
<td><p>One of preprocess or input_tensor_from_meta. Use preprocess if using frame or object processing mode and input_tensor_from_meta when using preprocessed tensor input mode</p></td>
<td><p>preprocess or input_tensor_from_meta</p></td>
<td><blockquote>
<div><p>“N/A. Refer preprocess and input_tensor_from_meta below”</p>
</div></blockquote>
</td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p>preprocess</p></td>
<td><p>Network preprocessing settings for color conversion， scale and normalization applicable when using frame or object processing mode</p></td>
<td><p>PreProcessParams</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">preprocess</span> <span class="pre">{</span>
<span class="pre">normalize</span> <span class="pre">{</span> <span class="pre">…</span> <span class="pre">}</span>
<span class="pre">}</span></code>
see details in PreProcessParams</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-even"><td><p>input_tensor_from_meta</p></td>
<td><p>Configuration for the input tensor applicable when using preprocessed tensor as input</p></td>
<td><p>InputTensorFromMeta</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_tensor_from_meta</span> <span class="pre">{</span>
<span class="pre">is_first_dim_batch</span> <span class="pre">:</span> <span class="pre">true</span>
<span class="pre">}</span></code>
see details in InputTensorFromMeta</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Preprocessed tensor input mode</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p>postprocess</p></td>
<td><p>Inference output tensor parsing methods such as detection, classification, semantic segmentation and others</p></td>
<td><p>PostProcessParams</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">postprocess</span> <span class="pre">{</span>
<span class="pre">detection</span> <span class="pre">{...}</span>
<span class="pre">}</span></code>
see details in PostProcessParams</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-even"><td><p>custom_lib</p></td>
<td><p>Specify custom lib path for custom parsing functions and preloads, optional</p></td>
<td><p>CustomLib</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">custom_lib</span> <span class="pre">{</span>
<span class="pre">path</span> <span class="pre">:</span> <span class="pre">./libcustom_parsing.so</span>
<span class="pre">}</span></code></p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p>extra</p></td>
<td><p>extra controls for inference config.</p></td>
<td><p>ExtraControl</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">extra</span> <span class="pre">{</span>
<span class="pre">output_buffer_pool_size:</span> <span class="pre">2</span>
<span class="pre">}</span></code>
see details in ExtraControl</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-even"><td><p>lstm</p></td>
<td><p>LSTM control parameters, limited on batch-size 1 and single stream</p></td>
<td><p>LstmParams
[optional]</p></td>
<td><dl>
<dt><a href="DS_plugin_gst-nvinferserver.html#id1"><span class="problematic" id="id2">``</span></a>lstm {</dt><dd><dl class="simple">
<dt>loops {</dt><dd><p>input: “init_lstm_c”
output: “output/lstm_c”
init_const { value: 0 }</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}``
See details in LstmParams</p>
</td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All </div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p>clip_object_outside_roi</p></td>
<td><p>Clip the object bounding boxes to fit within the specified ROI boundary.</p></td>
<td><p>bool</p></td>
<td><p>clip_object_outside_roi: false</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">Detector</div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id13">
<caption><span class="caption-text">Gst-nvinferserver message BackendParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id13" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><p>Network Types/Applicable to GIEs (Primary/Secondary)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>inputs</p></td>
<td><p>Backend input layer settings, optional</p></td>
<td><p>InputLayer arrays</p></td>
<td><p>see details in InputLayer</p></td>
<td><p>All/Both</p></td>
</tr>
<tr class="row-odd"><td><p>outputs</p></td>
<td><p>Backend output layer settings, optional</p></td>
<td><p>OutputLayer arrays</p></td>
<td><p>see details in OutputLayer</p></td>
<td><p>All/Both</p></td>
</tr>
<tr class="row-even"><td><p>triton</p></td>
<td><p>backend of Triton Inference Server settings</p></td>
<td><p>TritonParams</p></td>
<td><p>see details in TritonParams</p></td>
<td><p>All/Both</p></td>
</tr>
<tr class="row-odd"><td><p>output_mem_type</p></td>
<td><p>Triton native output tensor memory type</p></td>
<td><p>MemoryType select from [MEMORY_TYPE_DEFAULT, MEMORY_TYPE_CPU, MEMORY_TYPE_GPU]</p></td>
<td><p>output_mem_type: MEMORY_TYPE_CPU</p></td>
<td><p>All/Both</p></td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id14">
<caption><span class="caption-text">Gst-nvinferserver message InputLayer definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id14" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>name</p></td>
<td><p>input tensor name</p></td>
<td><p>string</p></td>
<td><p>name: “input_0”</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>dims</p></td>
<td><p>input tensor shape, optional. Only required if backend cannot figure out fixed input shapes</p></td>
<td><p>int32 array,
&gt; 0</p></td>
<td><p>dims: [299, 299, 3]</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>data_type</p></td>
<td><p>enum TensorDataType with types:</p>
<blockquote>
<div><div class="line-block">
<div class="line">TENSOR_DT_NONE,</div>
<div class="line">TENSOR_DT_FP32,</div>
<div class="line">TENSOR_DT_FP16,</div>
<div class="line">TENSOR_DT_INT8,</div>
<div class="line">TENSOR_DT_INT16,</div>
<div class="line">TENSOR_DT_INT32,</div>
<div class="line">TENSOR_DT_UINT8,</div>
<div class="line">TENSOR_DT_UINT16,</div>
<div class="line">TENSOR_DT_UINT32</div>
<div class="line">Default TENSOR_DT_NONE, </div>
</div>
</div></blockquote>
<p>usually can be deduced from Triton model config.pbtxt</p>
</td>
<td><p>TensorDataType</p></td>
<td><p>data_type:
TENSOR_DT_FP32</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id15">
<caption><span class="caption-text">Gst-nvinferserver message OutputLayer definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id15" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>name</p></td>
<td><p>output tensor name</p></td>
<td><p>string</p></td>
<td><p>name: “detection_boxes”</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>max_buffer_bytes</p></td>
<td><p>output tensor reserved buffer bytes</p></td>
<td><p>uint64</p></td>
<td><p>max_buffer_bytes: 2048</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id16">
<caption><span class="caption-text">Gst-nvinferserver message TritonParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id16" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>model_name</p></td>
<td><p>Triton inference model name</p></td>
<td><p>string</p></td>
<td><p>model_name: “ssd_inception_graphdef”</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>version</p></td>
<td><p>Triton model version number.
-1, latest version number.
&gt;0, reserved for specific version number in future version</p></td>
<td><p>int64</p></td>
<td><p>version: -1</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>model_repo</p></td>
<td><p>Triton model repository settings.
Note, all model_repo settings must be same in single process</p></td>
<td><p>TritonParams::TritonModelRepo</p></td>
<td><p>model_repo {
root: “../triton_model_repo”
log_level: 2
}
Refer the details in TritonModelRepo</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>grpc</p></td>
<td><p>Triton gRPC server settings.</p></td>
<td><p>TritonParams::TritonGrpcParams</p></td>
<td><p>grpc {
url: “localhost:8001”
enable_cuda_buffer_sharing: false
}
Refer the details in TritonGrpcParams</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id17">
<caption><span class="caption-text">Gst-nvinferserver message TritonParams-TritonModelRepo definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id17" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>root</p></td>
<td><p>Triton inference model repository directory path</p></td>
<td><p>string</p></td>
<td><p>root: “../triton_model_repo”</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>log_level</p></td>
<td><p>Triton log output levels</p></td>
<td><p>uint32;
0, ERROR;
1, WARNING;
2, INFO;
&gt;=3, VERBOSE Level</p></td>
<td><p>log_level: 1</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>strict_model_config</p></td>
<td><p>Enable Triton strict model configuration, see details in Triton <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md">Generated Model Configuration</a>. Suggest setting value true</p></td>
<td><p>bool</p></td>
<td><p>strict_model_config: true</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>tf_gpu_memory_fraction</p></td>
<td><p>TensorFlow GPU memory fraction per process. Valid for Tensorflow models only.
Default 0 means no GPU memory limitation. Suggest tuning to a proper value (e.g. in range of [0.2, 0.6]) in case Tensorflow uses up whole GPU memory</p></td>
<td><p>float,
Range (0, 1.0]</p></td>
<td><p>tf_gpu_memory_fraction: 0.6</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>tf_disable_soft_placement</p></td>
<td><p>Disable TensorFlow soft placement of operators. It’s enabled by default.</p></td>
<td><p>bool</p></td>
<td><p>tf_disable_soft_placement: false</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>min_compute_capacity</p></td>
<td><p>Specify minimal GPU compute capacity.
The default value is 6.0 on x86 and 5.0 on Jetson.</p></td>
<td></td>
<td><p>min_compute_capacity: 6.0</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>backend_dir</p></td>
<td><p>Specify Triton backend directory which store Tensorflow/Onnx/Pytorch and custom backends.
The Default value  is /opt/tritonserver/backends on X86 and opt/nvidia/deepstream/deepstream-x.x/lib/triton_backends on Jetson.</p></td>
<td><p>string</p></td>
<td><p>backend_dir: /opt/tritonserver/backends/</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>cuda_device_memory</p></td>
<td><p>Specify a list of CudaDeviceMem blocks with pre-allocated memory pool. Use Triton’s default value if list is empty.</p></td>
<td><p>message list</p></td>
<td><dl class="simple">
<dt>cuda_device_memory [ {</dt><dd><p>device: 0
memory_pool_byte_size: 2000000000</p>
</dd>
</dl>
<p>} ]</p>
</td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>CudaDeviceMem::device</p></td>
<td><p>Specify device ID</p></td>
<td><p>uint32;
&gt;= 0</p></td>
<td><p>device: 0</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>CudaDeviceMem::memory_pool_byte_size</p></td>
<td><p>Indicate pre-allocated memory pool byte size on according device for Triton runtime</p></td>
<td><p>uint64;
&gt;= 0</p></td>
<td><p>memory_pool_byte_size: 8000000000</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>pinned_memory_pool_byte_size</p></td>
<td><p>Indicate pre-allocated Pinned memory on host for Triton runtime. Use Triton’s defult value (around 256MB) if not set.</p></td>
<td><p>uint64;
&gt;= 0</p></td>
<td><p>pinned_memory_pool_byte_size: 128000000</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>backend_configs</p></td>
<td><p>A list of BackendConfig blocks for Tritonserver backend config settings</p></td>
<td><p>message list</p></td>
<td><dl class="simple">
<dt>backend_configs [ {</dt><dd><p>backend: tensorflow
setting: “allow-soft-placement”
value: “true”</p>
</dd>
</dl>
<p>} ]</p>
</td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>BackendConfig::backend</p></td>
<td><p>Specify backend name</p></td>
<td><p>string</p></td>
<td><p>backend: tensorflow</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>BackendConfig::setting</p></td>
<td><p>Specify backend setting name</p></td>
<td><p>string</p></td>
<td><p>setting: “allow-soft-placement”</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>BackendConfig::value</p></td>
<td><p>Specify backend setting values</p></td>
<td><p>string</p></td>
<td><p>value: “true”</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id18">
<caption><span class="caption-text">Gst-nvinferserver message TritonParams-TritonGrpcParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id18" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><blockquote>
<div><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</div></blockquote>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>url</p></td>
<td><p>Triton server name and port</p></td>
<td><p>string</p></td>
<td><p>url: “localhost:8001”</p></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><p>enable_cuda_buffer_sharing</p></td>
<td><blockquote>
<div><p>“Enable sharing of CUDA buffers with local Triton server for input tensors.</p>
</div></blockquote>
</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>If enabled</p></td>
<td><p>the input CUDA buffers are shared with the Triton server to improve performance.</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>This feature should be enabled only when the Triton server is on the same machine.</p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Applicable for x86 dGPU platform</p></td>
<td><p>not supported on Jetson devices.</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>By default disabled i.e. CUDA buffers are copied to system memory while creating the inference request.”</p></td>
<td><p>Bool</p></td>
<td><p>enable_cuda_buffer_sharing: true</p></td>
<td></td>
<td><blockquote>
<div><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id19">
<caption><span class="caption-text">Gst-nvinferserver message PreProcessParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id19" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs</div>
<div class="line">(Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>network_format</p></td>
<td><p>enum MediaFormat with formats:
MEDIA_FORMAT_NONE
IMAGE_FORMAT_RGB
IMAGE_FORMAT_BGR
IMAGE_FORMAT_GRAY
. use IMAGE_FORMAT_RGB by default.</p></td>
<td><p>MediaFormat</p></td>
<td><p>network_format: IMAGE_FORMAT_RGB</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>tensor_order</p></td>
<td><p>enum TensorOrder with order types:
TENSOR_ORDER_NONE,
TENSOR_ORDER_LINEAR(this includes NCHW, CHW, DCHW, … orders),
TENSOR_ORDER_NHWC.
It can deduce the value from backend layers info if set to TENSOR_ORDER_NONE</p></td>
<td><p>TensorOrder</p></td>
<td><p>tensor_order: TENSOR_ORDER_NONE</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>tensor_name</p></td>
<td><p>Specify the tensor name for the preprocessing buffer.
This is in the case when multiple input tensors in a single network.</p></td>
<td><div class="line-block">
<div class="line">string;</div>
<div class="line">Optional</div>
</div>
</td>
<td><p>tensor_name: “input_0”</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>frame_scaling_hw</p></td>
<td><p>Compute hardware to use for scaling frames / object crops to network resolution</p></td>
<td><p>enum FrameScalingHW
FRAME_SCALING_HW_DEFAULT: Platform default – GPU (dGPU), VIC (Jetson)
FRAME_SCALING_HW_GPU
FRAME_SCALING_HW_VIC (Jetson only)</p></td>
<td><p>frame_scaling_hw:  FRAME_SCALING_HW_GPU</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>frame_scaling_filter</p></td>
<td><p>The filter to use for scaling frames / object crops to network resolution</p></td>
<td><p>int32, refer to enum NvBufSurfTransform_Inter in nvbufsurftransform.h for valid values</p></td>
<td><p>frame_scaling_filter: 1</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>maintain_aspect_ratio</p></td>
<td><p>Indicates whether to maintain aspect ratio while scaling input.</p></td>
<td><p>int32;
0 or 1</p></td>
<td><p>maintain_aspect_ratio: 0</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>symmetric_padding</p></td>
<td><p>Indicates whether to pad image symmetrically while scaling input. DeepStream pads the images asymmetrically by default.</p></td>
<td><p>int32;
0 or 1</p></td>
<td><p>symmetric_padding: 0</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>normalize</p></td>
<td><p>Network input tensor normalization settings for scale-factors, offsets and mean-subtraction</p></td>
<td><p>PreProcessParams::ScaleNormalize</p></td>
<td><p>normalize {
scale_factor: 1.0
channel_offsets: [0, 0, 0]
}
see details in PreProcessParams::ScaleNormalize</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id20">
<caption><span class="caption-text">Gst-nvinferserver message PreProcessParams-ScaleNormalize definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id20" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>scale_factor</p></td>
<td><p>Pixel normalization factor</p></td>
<td><p>float</p></td>
<td><p>scale_factor: 0.0078</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><p>channel_offsets</p></td>
<td><p>Array of mean values of color components to be subtracted from each pixel. Array length must equal the number of color components in the frame. The plugin multiplies mean values by scale_factor.</p></td>
<td><p>float array, Optional</p></td>
<td><p>channel_offsets: [77.5, 21.2, 11.8]</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>mean_file</p></td>
<td><p>Pathname of mean data file (PPM format)</p></td>
<td><p>string;
Optional</p></td>
<td><p>mean_file: “./model_meanfile.ppm”</p></td>
<td><div class="line-block">
<div class="line">All</div>
<div class="line">Both</div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id21">
<caption><span class="caption-text">Gst-nvinferserver message InputTensorFromMeta definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id21" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs</div>
<div class="line">(Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>is_first_dim_batch</p></td>
<td><p>Boolean indicating whether the preprocessed input tensor has first dimention as batch. Set true for batched input, false otherwise.</p></td>
<td><p>Boolean</p></td>
<td><p>is_first_dim_batch: true</p></td>
<td><p>All</p>
<p>Preprocessed tensor input mode</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id22">
<caption><span class="caption-text">Gst-nvinferserver message PostProcessParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id22" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>labelfile_path</p></td>
<td><p>Pathname of a text file containing the labels for the model</p></td>
<td><p>string</p></td>
<td><p>labelfile_path: “=/home/ubuntu/model_labels.txt”</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>oneof process_type</p></td>
<td><p>Indicates one of the postprocessing type
detection;
classification;
segmentation;
other;</p></td>
<td><p>None</p></td>
<td><p>N/A</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>detection</p></td>
<td><p>Specify detection parameters for the network.
It must be oneof process_type</p></td>
<td><p>DetectionParams</p></td>
<td><p>detection {
num_detected_classes: 4
simple_cluster {
threshold: 0.2
}
}
see details in DetectionParams</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>classification</p></td>
<td><p>Specify classification parameters for the network
It is oneof process_type</p></td>
<td><p>ClassificationParams</p></td>
<td><p>classification {
threshold: 0.6
}
see details in ClassificationParams</p></td>
<td><p>Classifier</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>segmentation</p></td>
<td><p>Specify semantic segmentation parameters for the network
It is oneof process_type</p></td>
<td><p>SegmentationParams</p></td>
<td><p>segmentation {
threshold: 0.2
num_segmentation_classes: 2
}</p></td>
<td><p>Segmentation</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>other</p></td>
<td><p>Specify other network parameters.
This is for user-defined networks and usually coexists with output_control.output_tensor_meta: true. Tensor output data would be attached into GstBuffer. Data can be parsed in application. User can increase extra.output_buffer_pool_size if need to hold metadata longer.
It is oneof process_type</p></td>
<td><p>OtherNetworkParams</p></td>
<td><p>other {}
see details in OtherNetworkParams</p></td>
<td><p>Others</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>triton_classification</p></td>
<td><p>Specify Triton classification parameters for the network
It is oneof process_type</p></td>
<td><p>TritonClassifyParams</p></td>
<td><p>Triton_classification {
topk: 1
}
see details in TritonClassifyParams</p></td>
<td><p>Classifier</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id23">
<caption><span class="caption-text">Gst-nvinferserver message DetectionParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id23" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>num_detected_classes</p></td>
<td><p>Define number of classes detected by the network</p></td>
<td><p>int32, &gt; 0</p></td>
<td><p>num_detected_classes:4</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>per_class_params</p></td>
<td><p>Map of specific detection parameters per class. Key-value follows &lt;class_id: per_class_params&gt; order.</p></td>
<td><p>map&lt;int32, PerClassParams&gt;;
Optional</p></td>
<td><p>per_class_params [</p>
<p>{ key: 1,
value { pre_threshold : 0.4}
},</p>
<p>{ key: 2,
value { pre_threshold : 0.5}
}</p>
<p>]
see details for PerClassParams</p>
</td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>custom_parse_bbox_func</p></td>
<td><p>Name of the custom bounding box parsing function. If not specified, Gst-nvinferserver uses the internal function for the resnet model provided by the SDK.
If specified, also need to set custom_lib to load custom library.</p></td>
<td><p>string;</p></td>
<td><p>custom_parse_bbox_func: “NvDsInferParseCustomTfSSD”</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>oneof clustering_policy</p></td>
<td><p>Indicates one of the clustering policies from
nms;
dbscan;
group_rectangle;
simple_cluster;</p></td>
<td><p>None</p></td>
<td><p>N/A</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>nms</p></td>
<td><p>Indicates clustering bounding boxes by Non-Maximum-Suppression method detected objects.
It is oneof clustering_policy</p></td>
<td><p>Nms</p></td>
<td><p>nms {
confidence_threshold: 0.3
iou_threshold: 0.4
}
see details in Nms</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>dbscan</p></td>
<td><p>Indicates clustering bounding boxes by DBSCAN method for detected objects.
It is oneof clustering_policy</p></td>
<td><p>DbScan</p></td>
<td><p>dbscan {
pre_threshold: 0.3
eps: 0.7
min_boxes: 3
}
see details in DbScan</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>group_rectangle</p></td>
<td><p>Indicates clustering bounding boxes by groupRectangles() function for grouping detected objects
It is oneof clustering_policy</p></td>
<td><p>GroupRectangle</p></td>
<td><p>group_rectangle {
confidence_threshold: 0.2
group_threshold: 2
eps: 0.2
}</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>simple_cluster</p></td>
<td><p>Indicates simple clustering method by outlier boxes through threshold</p></td>
<td><p>SimpleCluster</p></td>
<td><p>simple_cluster {
threshold: 0.2
}</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id24">
<caption><span class="caption-text">Gst-nvinferserver message DetectionParams-PerClassParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id24" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>pre_threshold</p></td>
<td><p>Define confidence threshold per class</p></td>
<td><p>float</p></td>
<td><p>pre_threshold:0.3</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id25">
<caption><span class="caption-text">Gst-nvinferserver message DetectionParams-Nms definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id25" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>confidence_threshold</p></td>
<td><p>Detection score lesser than this threshold would be rejected</p></td>
<td><p>float</p></td>
<td><p>confidence_threshold:0.5</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>iou_threshold</p></td>
<td><p>Maximum IOU score between two proposals after which the proposal with the lower confidence will be rejected.</p></td>
<td><p>float</p></td>
<td><p>iou_threshold: 0.3</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>topk</p></td>
<td><p>Specify top k detection results to keep after nms</p></td>
<td><p>int32, &gt;= 0</p></td>
<td><p>topk: 2;
value 0, means keep all.</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id26">
<caption><span class="caption-text">Gst-nvinferserver message DetectionParams-DbScan definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id26" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>pre_threshold</p></td>
<td><p>Detection score lesser than this threshold would be rejected before DBSCAN clustering</p></td>
<td><p>float</p></td>
<td><p>pre_threshold:0.2</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>eps</p></td>
<td><p>DBSCAN epsilon to control merging of overlapping boxes.</p></td>
<td><p>float</p></td>
<td><p>eps: 0.7</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>min_boxes</p></td>
<td><p>Minimum boxes in DBSCAN cluster to be considered an object</p></td>
<td><p>int32, &gt; 0</p></td>
<td><p>min_boxes: 3;</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>min_score</p></td>
<td><p>Minimum score in DBSCAN cluster for it to be considered as an object</p></td>
<td><p>float</p></td>
<td><p>min_score: 0.7</p>
<p>Default value is 0</p>
</td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id27">
<caption><span class="caption-text">Gst-nvinferserver message DetectionParams-GroupRectangle definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id27" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>confidence_threshold</p></td>
<td><p>Detection score lesser than this threshold would be rejected</p></td>
<td><p>float</p></td>
<td><p>confidence_threshold:0.2</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>group_threshold</p></td>
<td><p>Threshold value for rectangle merging for OpenCV grouprectangles() function</p></td>
<td><p>int32; &gt;= 0</p></td>
<td><p>group_threshold: 1</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>eps</p></td>
<td><p>Epsilon to control merging of overlapping boxes</p></td>
<td><p>float</p></td>
<td><p>eps: 0.2</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id28">
<caption><span class="caption-text">Gst-nvinferserver message DetectionParams-SimpleCluster definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id28" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>threshold</p></td>
<td><p>Detection score lesser than this threshold would be rejected</p></td>
<td><p>float</p></td>
<td><p>confidence_threshold:0.6</p></td>
<td><p>Detector</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id29">
<caption><span class="caption-text">Gst-nvinferserver message ClassificationParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id29" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>threshold</p></td>
<td><p>Classification score lesser than this threshold would be rejected</p></td>
<td><p>float</p></td>
<td><p>threshold: 0.5</p></td>
<td><p>Classifier</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>custom_parse_classifier_func</p></td>
<td><p>Name of the custom classifier output parsing function.
If not specified, Gst-nvinfer uses the internal parsing function with NCHW tensor order for softmax layers. User can reshape other output tensor order to NCHW in Triton config.pbtxt to run internal parsing.
If specified, also need to set custom_lib to load custom library.</p></td>
<td><p>string</p></td>
<td><p>parse-classifier-func-name: “parse_bbox_softmax”</p></td>
<td><p>Classifier</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id30">
<caption><span class="caption-text">Gst-nvinferserver message SegmentationParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id30" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>threshold</p></td>
<td><p>Segmentation score lesser than this threshold would be rejected</p></td>
<td><p>float</p></td>
<td><p>threshold: 0.5</p></td>
<td><p>Segmentation</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>num_segmentation_classes</p></td>
<td><p>Number of output classes for the segmentation network</p></td>
<td><p>int32, &gt;0</p></td>
<td><p>num_segmentation_classes: 2</p></td>
<td><p>Segmentation</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>custom_parse_segmentation_func</p></td>
<td><p>Name of the custom segmentation output parsing function. If not specified, Gst-nvinferserver uses the internal function for the UNet model provided by the SDK. If specified, users also need to set custom_lib to load custom library.</p></td>
<td><p>string</p></td>
<td><p>custom_parse_segmentation_func: “NvDsInferParseCustomPeopleSemSegNet”</p></td>
<td><p>Segmentation</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id31">
<caption><span class="caption-text">Gst-nvinferserver message OtherNetworkParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id31" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>type_name</p></td>
<td><p>Specify a user-defined network name</p></td>
<td><p>string; | Optional</p></td>
<td><p>type_name: “face”</p></td>
<td><p>Others</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id32">
<caption><span class="caption-text">Gst-nvinferserver message TritonClassifyParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id32" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>topk</p></td>
<td><p>Specify top k elements need to keep from Triton’s native classification</p></td>
<td><p>uint32; &gt;=0</p></td>
<td><p>topk : 1
Value 0 or empty would keep the top 1 result.</p></td>
<td><p>Classifier</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>threshold</p></td>
<td><p>Classification score lesser than this threshold would be rejected</p></td>
<td><p>float</p></td>
<td><p>threshold: 0.5</p></td>
<td><p>Classifier</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id33">
<caption><span class="caption-text">Gst-nvinferserver message CustomLib definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id33" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>path</p></td>
<td><p>Pathname that points to a custom library for preload</p></td>
<td><p>string</p></td>
<td><p>path: “/home/ubuntu/lib_custom_impl.so”</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id34">
<caption><span class="caption-text">Gst-nvinferserver message ExtraControl definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id34" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 15.0%" />
<col style="width: 20.0%" />
<col style="width: 25.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs</div>
<div class="line">(Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>copy_input_to_host_buffers</p></td>
<td><p>Enable to copy input. If enabled, input tensor would be attached as NvDsInferTensorMeta into GstBuffer with output tensors together tensor data to host buffers.</p></td>
<td><p>bool</p></td>
<td><p>copy_input_to_host_buffers: false</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>output_buffer_pool_size</p></td>
<td><p>Specify the buffer pool size for each output tensor.
When infer_config.postprocess.other is specified or output_control.output_tensor_meta is enabled, the output tensor would be attached as NvDsInferTensorMeta into GstBuffer</p></td>
<td><p>int32;
Range [2, 10]</p></td>
<td><p>output_buffer_pool_size: 4</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>custom_process_funcion</p></td>
<td><p>custom function to create a specific user-defined processor IInferCustomProcessor.
The function symbol is loaded by infer_config.custom_lib</p></td>
<td><p>String</p></td>
<td><p>path: “libnvdsinfer_custom_impl_fasterRCNN.so”</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>LstmParams structures may be changed in future versions</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="pst-scrollable-table-container"><table class="table" id="id35">
<caption><span class="caption-text">Gst-nvinferserver message LstmParams definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id35" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>loops</p></td>
<td><p>Specify LSTM loops between input and output tensors.</p></td>
<td><p>LstmLoop
[repeated]</p></td>
<td><dl class="simple">
<dt>loops [ {</dt><dd><p>input: “init_state”
output: “out_state”</p>
</dd>
</dl>
<p>} ]
See details in LstmParams::LstmLoop</p>
</td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Input and output tensors must have same datatype/dimensions, FP16 is not supported</p></li>
<li><p><cite>LstmParams::LstmLoop</cite> structures might be changed in future versions</p></li>
</ul>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id36">
<caption><span class="caption-text">Gst-nvinferserver message LstmParams-LstmLoop definition details</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id36" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
<th class="head"><div class="line-block">
<div class="line">Network Types /</div>
<div class="line">Applicable to GIEs (Primary/Secondary)</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>input</p></td>
<td><p>Specify input tensor name of the current loop.</p></td>
<td><p>string</p></td>
<td><p>Input: “init_state”</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-odd"><td><p>output</p></td>
<td><p>Specify input tensor name of the current loop. Tensor data will feedback to the input tensor</p></td>
<td><p>string</p></td>
<td><p>onput: “output_state”</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
<tr class="row-even"><td><p>init_const</p></td>
<td><p>Specify the constant values for the input in first frame</p></td>
<td><p>InitConst | value: float</p></td>
<td><p>Init_const { value: 0 }</p></td>
<td><p>All</p>
<p>Both</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="gst-properties">
<h2>Gst Properties<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#gst-properties" title="Link to this heading">#</a></h2>
<p>The values set through Gst properties override the values of properties in the configuration file. The application does this for certain properties that it needs to set programmatically. If user set property though plugin, these values would replace the original value in config files. The following table describes the Gst-nvinferserver plugin’s Gst properties.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id37">
<caption><span class="caption-text">Gst-nvinferserver plugin Gst properties</span><a class="headerlink" href="DS_plugin_gst-nvinferserver.html#id37" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>config-file-path</p></td>
<td><p>Absolute pathname of configuration file for the Gst-nvinferserver element</p></td>
<td><p>String</p></td>
<td><p>config-file-path=config_infer_primary.txt</p></td>
</tr>
<tr class="row-odd"><td><p>process-mode</p></td>
<td><p>Infer Processing Mode
(0):None, (1)FullFrame, (2)ClipObject.
If set, it could replace input_control.process_mode</p></td>
<td><p>Integer, 0, 1 or 2</p></td>
<td><p>process-mode=1</p></td>
</tr>
<tr class="row-even"><td><p>unique-id</p></td>
<td><p>Unique ID identifying metadata generated by this GIE.
If set, it could replace infer_config.unique_id</p></td>
<td><p>Integer,
0 to 4,294,967,295</p></td>
<td><p>unique-id=1</p></td>
</tr>
<tr class="row-odd"><td><p>infer-on-gie-id</p></td>
<td><p>See input_control.operate_on_gie_id in the configuration file table</p></td>
<td><p>Integer,
0 to 4,294,967,295</p></td>
<td><p>infer-on-gie-id=1</p></td>
</tr>
<tr class="row-even"><td><p>operate-on-class-ids</p></td>
<td><p>See input_control.operate_on_class_ids in the configuration file table</p></td>
<td><p>An array of colon- separated integers (class-ids)</p></td>
<td><p>operate-on-class-ids=1:2:4</p></td>
</tr>
<tr class="row-odd"><td><p>batch-size</p></td>
<td><p>Number of frames/objects to be inferred together in a batch.
If set, it could replace infer_config.max_batch_size</p></td>
<td><p>Integer,
1 – 4,294,967,295</p></td>
<td><p>batch-size=4</p></td>
</tr>
<tr class="row-even"><td><p>Interval</p></td>
<td><p>Number of consecutive batches to be skipped for inference
If set, it could replace input_control.interval</p></td>
<td><p>Integer, 0 to 32</p></td>
<td><p>interval=0</p></td>
</tr>
<tr class="row-odd"><td><p>raw-output-generated-callback</p></td>
<td><p>Pointer to the raw output generated callback function</p></td>
<td><p>Pointer</p></td>
<td><p>Cannot be set through gst-launch</p></td>
</tr>
<tr class="row-even"><td><p>raw-output-generated-userdata</p></td>
<td><p>Pointer to user data to be supplied with raw-output-generated-callback</p></td>
<td><p>Pointer</p></td>
<td><p>Cannot be set through gst-launch</p></td>
</tr>
</tbody>
</table>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
<section id="deepstream-triton-samples">
<h2>DeepStream Triton samples<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#deepstream-triton-samples" title="Link to this heading">#</a></h2>
<p>DeepStream Triton samples are located in the folder <code class="docutils literal notranslate"><span class="pre">samples/configs/deepstream-app-triton</span></code>. In terms of Triton model specification, all related models and Triton config files(<code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code>) must be gathered into same root directory which
is <code class="docutils literal notranslate"><span class="pre">samples/triton_model_repo</span></code>. Follow the instructions in <code class="docutils literal notranslate"><span class="pre">samples/configs/deepstream-app-triton/README</span></code> to run the samples.</p>
</section>
<section id="deepstream-triton-grpc-support">
<h2>DeepStream Triton gRPC support<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#deepstream-triton-grpc-support" title="Link to this heading">#</a></h2>
<p>In addition to native Triton server, gst-nvinferserver supports the Triton Inference Server running as independent process. Communication to the server happens through gRPC.
Config files to run the application in gRPC mode are located at <code class="docutils literal notranslate"><span class="pre">samples/config/deepstream-app-triton-grpc</span></code>. Follow the instructions in <code class="docutils literal notranslate"><span class="pre">samples/configs/deepstream-app-triton-grpc/README</span></code> to run the samples.</p>
</section>
<section id="triton-ensemble-models">
<span id="triton-ensemble-label"></span><h2>Triton Ensemble Models<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#triton-ensemble-models" title="Link to this heading">#</a></h2>
<p>The Gst-nvinferserver plugin can support Triton ensemble models for further custom preprocessing, backend and postprocessing through Triton custom-backends.
Triton ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models, such as <code class="docutils literal notranslate"><span class="pre">“data</span> <span class="pre">preprocessing</span> <span class="pre">-&gt;</span> <span class="pre">inference</span> <span class="pre">-&gt;</span> <span class="pre">data</span> <span class="pre">postprocessing”</span></code>. See more details
<a class="github reference external" href="https://github.com/triton-inference-server/server/blob/r24.08/docs/user_guide/architecture.md#ensemble-models">triton-inference-server/server</a>.
To manage memory efficiency and keep clean interface, The Gst-nvinferserver Plugin’s default preprocessing cannot be disabled. Color conversion, datatype conversion, input scaling and object cropping are continue working in <code class="docutils literal notranslate"><span class="pre">nvds_infer_server</span></code> natively. For example, in the case native normalization is not needed, update scale_factor to 1.0:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>infer_config { preprocess {
network_format: IMAGE_FORMAT_RGB
tensor_order: TENSOR_ORDER_LINEAR
normalize { scale_factor: 1.0 } } }
</pre></div>
</div>
</div></blockquote>
<p>The low level <code class="docutils literal notranslate"><span class="pre">nvds_infer_server</span></code> library could deliver specified media-format (RGB/BGR/Gray) in any kind of tensor orders and datatypes as a Cuda GPU buffer input to Triton backend. User’s custom-backend must support GPU memory on this input. Triton custom-backend sample identity can work with Gst-nvinferserver plugin.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Custom backend API must have same Triton codebase version (24.08). Read more details from Triton server release <a class="github reference external" href="https://github.com/triton-inference-server/server/releases/tag/v2.49.0">triton-inference-server/server</a></p>
</div>
<p>To learn details how to implement Triton custom-backend, please refer to
<a class="github reference external" href="https://github.com/triton-inference-server/backend/tree/r24.08#how-can-i-develop-my-own-triton-backend">triton-inference-server/backend</a>
For Triton model’s output, TRTSERVER_MEMORY_GPU and TRTSERVER_MEMORY_CPU buffer allocation are supported in <code class="docutils literal notranslate"><span class="pre">nvds_infer_server</span></code> according to Triton output request. This also works for ensemble model’s final output tensors.
Finally, inference data can be parsed by default for detection, classification, or semantic segmentation. Alternatively, user can implement custom-backend for postprocessing, then deliver the final output to Gst-nvinferserver plugin to do further processing. Besides that, User can also optionally attach raw tensor output data into metadata for downstream or application to parse.</p>
</section>
<section id="custom-process-interface-iinfercustomprocessor-for-extra-input-lstm-loop-output-tensor-postprocess">
<h2>Custom Process interface <cite>IInferCustomProcessor</cite> for Extra Input, LSTM Loop, Output Tensor Postprocess<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#custom-process-interface-iinfercustomprocessor-for-extra-input-lstm-loop-output-tensor-postprocess" title="Link to this heading">#</a></h2>
<p>Gst-nvinferserver plugin supports extra(multiple) input tensors custom preprocessing, input / output tensor custom loop processing (LSTM-based) with multiple streams, output tensor data custom parsing and attaching into NvDsBatchMeta. This custom function is loaded though gst-nvinferserver’s config file:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>infer_config {
  backend {
    triton {
      model_name: &quot;yolov3-10_onnx&quot;
      # option 1: for CAPI inference
      # model_repo { root: &quot;./model_repo&quot; }
      # option 2: for gRPC inference
      # grpc { url: &quot;localhost:8001&quot; }
    }
    # specify output tensor memory type, MEMORY_TYPE_CPU/MEMORY_TYPE_GPU
    output_mem_type: MEMORY_TYPE_CPU
  }
  preprocess { ... } # specify scale and normalization
  # postprocess{ other{} } # skip generic postprocess

  # specify custom processing library
  custom_lib {
     path: &quot;/path/to/libnvdsinferserver_custom_process.so&quot;
  }
  extra {
    # specify custom processing function entrypoint from custom_lib
    custom_process_funcion: &quot;CreateInferServerCustomProcess&quot;
  }
}
</pre></div>
</div>
<p>The interface <code class="docutils literal notranslate"><span class="pre">IInferCustomProcessor</span></code> is defined in <cite>sources/includes/nvdsinferserver/infer_custom_process.h</cite>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">IInferCustomProcessor</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">supportInputMemType</span><span class="p">(</span><span class="n">InferMemType</span><span class="o">&amp;</span><span class="w"> </span><span class="n">type</span><span class="p">);</span><span class="w"> </span><span class="c1">// return supported memory type for `extraInputs`</span>
<span class="w">     </span><span class="k">virtual</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="nf">requireInferLoop</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span><span class="w"> </span><span class="c1">// indicate whether LSTM loop is needed. return &#39;false&#39; if not needed.</span>
<span class="w">     </span><span class="c1">// custom implementation for extra input tensors processing, `primaryInputs` is processed by preprocess{} from config file.</span>
<span class="w">     </span><span class="c1">// param `options` is helpful to carry extra information such as stream_ids, `NvBufSurface`, `NvDsBatchMeta`, `GstBuffer`</span>
<span class="w">     </span><span class="k">virtual</span><span class="w"> </span><span class="n">NvDsInferStatus</span><span class="w"> </span><span class="nf">extraInputProcess</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">vector</span><span class="o">&lt;</span><span class="n">IBatchBuffer</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">primaryInputs</span><span class="p">,</span><span class="w"> </span><span class="n">vector</span><span class="o">&lt;</span><span class="n">IBatchBuffer</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">extraInputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">IOptions</span><span class="o">*</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">     </span><span class="c1">// param `outputs` is a array of all batched output tensors. param `inOptions` is same as extraInputProcess</span>
<span class="w">     </span><span class="k">virtual</span><span class="w"> </span><span class="n">NvDsInferStatus</span><span class="w"> </span><span class="nf">inferenceDone</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">IBatchArray</span><span class="o">*</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">IOptions</span><span class="o">*</span><span class="w"> </span><span class="n">inOptions</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">     </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">notifyError</span><span class="p">(</span><span class="n">NvDsInferStatus</span><span class="w"> </span><span class="n">status</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Users need derive from <cite>IInferCustomProcessor</cite> to implement their own extra preprocessing through <cite>extraInputProcess</cite> and fully postprocessing through <cite>inferenceDone</cite>. the param structred in <cite>IOptions</cite> carry all information from GstBuffer and NvDsBatchMeta. Users can query them through <cite>IOptions</cite> for each frame and batch.
see more examples in <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/sources/TritonOnnxYolo/nvdsinferserver_custom_impl_yolo/nvdsinferserver_custom_process_yolo.cpp</span></code></p>
<p>Take a example for a simple postprocessing to add output tensors.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;inttypes.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;unistd.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cassert&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;unordered_map&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;infer_custom_process.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;nvbufsurface.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;nvdsmeta.h&quot;</span>

<span class="k">typedef</span><span class="w"> </span><span class="k">struct</span><span class="w"> </span><span class="nc">_GstBuffer</span><span class="w"> </span><span class="n">GstBuffer</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">nvdsinferserver</span><span class="p">;</span>

<span class="cp">#defin INFER_ASSERT assert</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NvInferServerCustomProcess</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">IInferCustomProcessor</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="c1">// memtype for ``extraInputs``, set ``kGpuCuda`` for performance</span>
<span class="w">     </span><span class="kt">void</span><span class="w"> </span><span class="nf">supportInputMemType</span><span class="p">(</span><span class="n">InferMemType</span><span class="o">&amp;</span><span class="w"> </span><span class="n">type</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">InferMemType</span><span class="o">::</span><span class="n">kGpuCuda</span><span class="p">;</span><span class="w"> </span><span class="p">}</span>
<span class="w">     </span><span class="c1">// for LSTM loop. return false if not required.</span>
<span class="w">     </span><span class="kt">bool</span><span class="w"> </span><span class="nf">requireInferLoop</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="p">}</span>
<span class="w">     </span><span class="c1">// skip extraInputProcess if there is no extra input tensors</span>
<span class="w">     </span><span class="n">NvDsInferStatus</span><span class="w"> </span><span class="nf">extraInputProcess</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">IBatchBuffer</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">primaryInputs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">IBatchBuffer</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">extraInputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">IOptions</span><span class="o">*</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="k">return</span><span class="w"> </span><span class="n">NVDSINFER_SUCCESS</span><span class="p">;</span>
<span class="w">     </span><span class="p">}</span>
<span class="w">     </span><span class="c1">// output tensor postprocessing function.</span>
<span class="w">     </span><span class="n">NvDsInferStatus</span><span class="w"> </span><span class="nf">inferenceDone</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">IBatchArray</span><span class="o">*</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">IOptions</span><span class="o">*</span><span class="w"> </span><span class="n">inOptions</span><span class="p">)</span><span class="w"> </span><span class="k">override</span>
<span class="w">     </span><span class="p">{</span>
<span class="w">          </span><span class="n">GstBuffer</span><span class="o">*</span><span class="w"> </span><span class="n">gstBuf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">          </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">streamIds</span><span class="p">;</span>
<span class="w">          </span><span class="n">NvDsBatchMeta</span><span class="o">*</span><span class="w"> </span><span class="n">batchMeta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">          </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">NvDsFrameMeta</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">frameMetaList</span><span class="p">;</span>
<span class="w">          </span><span class="n">NvBufSurface</span><span class="o">*</span><span class="w"> </span><span class="n">bufSurf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">          </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">NvBufSurfaceParams</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">surfParamsList</span><span class="p">;</span>
<span class="w">          </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">unique_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">          </span><span class="n">INFER_ASSERT</span><span class="w"> </span><span class="p">(</span><span class="n">inOptions</span><span class="o">-&gt;</span><span class="n">getValueArray</span><span class="p">(</span><span class="n">OPTION_NVDS_SREAM_IDS</span><span class="p">,</span><span class="w"> </span><span class="n">streamIds</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">NVDSINFER_SUCCESS</span><span class="p">);</span>
<span class="w">          </span><span class="n">INFER_ASSERT</span><span class="p">(</span><span class="n">inOptions</span><span class="o">-&gt;</span><span class="n">getObj</span><span class="p">(</span><span class="n">OPTION_NVDS_BUF_SURFACE</span><span class="p">,</span><span class="w"> </span><span class="n">bufSurf</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">NVDSINFER_SUCCESS</span><span class="p">);</span>
<span class="w">          </span><span class="n">INFER_ASSERT</span><span class="p">(</span><span class="n">inOptions</span><span class="o">-&gt;</span><span class="n">getObj</span><span class="p">(</span><span class="n">OPTION_NVDS_BATCH_META</span><span class="p">,</span><span class="w"> </span><span class="n">batchMeta</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">NVDSINFER_SUCCESS</span><span class="p">);</span>
<span class="w">          </span><span class="n">INFER_ASSERT</span><span class="p">(</span><span class="n">inOptions</span><span class="o">-&gt;</span><span class="n">getInt</span><span class="p">(</span><span class="n">OPTION_NVDS_UNIQUE_ID</span><span class="p">,</span><span class="w"> </span><span class="n">unique_id</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">NVDSINFER_SUCCESS</span><span class="p">);</span>
<span class="w">          </span><span class="n">INFER_ASSERT</span><span class="p">(</span><span class="n">inOptions</span><span class="o">-&gt;</span><span class="n">getValueArray</span><span class="p">(</span><span class="n">OPTION_NVDS_BUF_SURFACE_PARAMS_LIST</span><span class="p">,</span><span class="w"> </span><span class="n">surfParamsList</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">NVDSINFER_SUCCESS</span><span class="p">);</span>
<span class="w">          </span><span class="n">INFER_ASSERT</span><span class="p">(</span><span class="n">inOptions</span><span class="o">-&gt;</span><span class="n">getValueArray</span><span class="p">(</span><span class="n">OPTION_NVDS_FRAME_META_LIST</span><span class="p">,</span><span class="w"> </span><span class="n">frameMetaList</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">NVDSINFER_SUCCESS</span><span class="p">);</span>

<span class="w">          </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">nsTimestamp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">UINT64_MAX</span><span class="p">;</span><span class="w"> </span><span class="c1">// nano-seconds</span>
<span class="w">          </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inOptions</span><span class="o">-&gt;</span><span class="n">hasValue</span><span class="p">(</span><span class="n">OPTION_TIMESTAMP</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">               </span><span class="n">INFER_ASSERT</span><span class="p">(</span><span class="n">inOptions</span><span class="o">-&gt;</span><span class="n">getUInt</span><span class="p">(</span><span class="n">OPTION_TIMESTAMP</span><span class="p">,</span><span class="w"> </span><span class="n">nsTimestamp</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">NVDSINFER_SUCCESS</span><span class="p">);</span>
<span class="w">          </span><span class="p">}</span>

<span class="w">          </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">SharedIBatchBuffer</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">;</span>
<span class="w">          </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">outputs</span><span class="o">-&gt;</span><span class="n">getSize</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">               </span><span class="n">SharedIBatchBuffer</span><span class="w"> </span><span class="n">outTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputs</span><span class="o">-&gt;</span><span class="n">getSafeBuf</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">               </span><span class="n">INFER_ASSERT</span><span class="p">(</span><span class="n">outTensor</span><span class="p">);</span>
<span class="w">               </span><span class="k">auto</span><span class="w"> </span><span class="n">desc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outTensor</span><span class="o">-&gt;</span><span class="n">getBufDesc</span><span class="p">();</span>
<span class="w">               </span><span class="n">tensors</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">desc</span><span class="p">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">outTensor</span><span class="p">);</span>
<span class="w">          </span><span class="p">}</span>

<span class="w">          </span><span class="c1">// parsing output tensors</span>
<span class="w">          </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">boxesPtr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">tensors</span><span class="p">[</span><span class="s">&quot;output_bbox&quot;</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">getBufPtr</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">          </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">bboxDesc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="s">&quot;output_bbox&quot;</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">getBufDesc</span><span class="p">();</span>
<span class="w">          </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">scoresPtr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">tensors</span><span class="p">[</span><span class="s">&quot;output_score&quot;</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">getBufPtr</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">          </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">numPtr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">tensors</span><span class="p">[</span><span class="s">&quot;output_bbox_num&quot;</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">getBufPtr</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">          </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">batchSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bboxDesc</span><span class="p">.</span><span class="n">dims</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span><span class="w"> </span><span class="c1">// e.g. tensor shape [Batch, num, 4]</span>

<span class="w">          </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">NvDsInferObjectDetectionInfo</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">batchedObjs</span><span class="p">(</span><span class="n">batchSize</span><span class="p">);</span>
<span class="w">          </span><span class="c1">// parsing data into batchedObjs</span>
<span class="w">          </span><span class="p">...</span>
<span class="w">          </span><span class="c1">// attach to NvDsBatchMeta</span>
<span class="w">          </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">iB</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">iB</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">batchSize</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">iB</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">               </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">objs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batchedObjs</span><span class="p">[</span><span class="n">iB</span><span class="p">];</span>
<span class="w">               </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">obj</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">objs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="n">NvDsObjectMeta</span><span class="o">*</span><span class="w"> </span><span class="n">objMeta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nvds_acquire_obj_meta_from_pool</span><span class="p">(</span><span class="n">batchMeta</span><span class="p">);</span>
<span class="w">                    </span><span class="n">objMeta</span><span class="o">-&gt;</span><span class="n">unique_component_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">unique_id</span><span class="p">;</span>
<span class="w">                    </span><span class="n">objMeta</span><span class="o">-&gt;</span><span class="n">confidence</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="p">.</span><span class="n">detectionConfidence</span><span class="p">;</span>
<span class="w">                    </span><span class="n">objMeta</span><span class="o">-&gt;</span><span class="n">class_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="p">.</span><span class="n">classId</span><span class="p">;</span>
<span class="w">                    </span><span class="n">objMeta</span><span class="o">-&gt;</span><span class="n">rect_params</span><span class="p">.</span><span class="n">left</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="p">.</span><span class="n">left</span><span class="p">;</span>
<span class="w">                    </span><span class="n">objMeta</span><span class="o">-&gt;</span><span class="n">rect_params</span><span class="p">.</span><span class="n">top</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="p">.</span><span class="n">top</span><span class="p">;</span>
<span class="w">                    </span><span class="n">objMeta</span><span class="o">-&gt;</span><span class="n">rect_params</span><span class="p">.</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="p">.</span><span class="n">width</span><span class="p">;</span>
<span class="w">                    </span><span class="n">objMeta</span><span class="o">-&gt;</span><span class="n">rect_params</span><span class="p">.</span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">obj</span><span class="p">.</span><span class="n">height</span><span class="p">;</span>
<span class="w">                    </span><span class="c1">// other settings</span>
<span class="w">                    </span><span class="p">...</span>
<span class="w">                    </span><span class="c1">// add NvDsObjectMeta obj into NvDsFrameMeta frame.</span>
<span class="w">                    </span><span class="n">nvds_add_obj_meta_to_frame</span><span class="p">(</span><span class="n">frameMetaList</span><span class="p">[</span><span class="n">iB</span><span class="p">],</span><span class="w"> </span><span class="n">objMeta</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">);</span>
<span class="w">               </span><span class="p">}</span>
<span class="w">          </span><span class="p">}</span>
<span class="w">     </span><span class="p">}</span>
<span class="p">};</span>

<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="n">IInferCustomProcessor</span><span class="o">*</span><span class="w"> </span><span class="nf">CreateInferServerCustomProcess</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">config</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">configLen</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="k">return</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">NvInferServerCustomProcess</span><span class="p">();</span>
<span class="p">}</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<ul>
<li><p>For extra input tensors preprocess: If the model requires multiple tensor inputs more than the primary image input, Users can derive from this interface <cite>IInferCustomProcessor</cite> and implement <cite>extraInputProcess()</cite> to process extra inputs tensors. This function is for extra input process only. the parameter <cite>IOptions* options</cite> would carry all the information from GstBuffer, NvDsBatchMeta, NvDsFrameMeta, NvDsObjectMeta and so on. User can leverage all of the information from <cite>options</cite> to fill the extra input tensors. All of the input tensor memory is allocated by nvdsinferserver low-level lib.</p></li>
<li><p>For output tensor postprocess(parsing and metadata attaching): If user want to do custom parsing on output tensors into user metadata and attach them into GstBuffer, NvDsBatchMeta, NvDsFrameMeta or NvDsObjectMeta. User can implement ‘inferenceDone(outputs, inOptions)’ to parse all output tensors in <cite>outputs</cite>, and get above GstBuffer, NvDsBatchMeta and other DeepStream information from <cite>inOptions</cite>. Then attach the parsed user metadata into NvDs metadata. This function supports multiple-streams parsing and attaching. See examples in <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/sources/TritonOnnxYolo/nvdsinferserver_custom_impl_yolo/nvdsinferserver_custom_process_yolo.cpp:</span> <span class="pre">NvInferServerCustomProcess::inferenceDone()</span></code> how to parse and attach output metadata.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If user need specific memory type(e.g. CPU) for output tensors in <cite>inferenceDone()</cite>. Update config file.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>infer_config { backend {
output_mem_type: MEMORY_TYPE_CPU
} }
</pre></div>
</div>
</div>
</li>
<li><p>For multi-stream custom loop process: If the model is LSTM based, and next frame’s inputs are generated by previous frame’s output data. User can derive interface <cite>IInferCustomProcessor</cite>, then implement <cite>extraInputProcess()</cite> and <cite>inferenceDone()</cite> for loop process. <cite>extraInputProcess()</cite> could initialize first input tensor states. Then <cite>‘inferenceDone()’</cite> can get the output data and do post processing and store the result into the context. When next <cite>‘extraInputProcess()’</cite> is coming, it can check the stored results and feedback into tensor states. When user override <code class="docutils literal notranslate"><span class="pre">bool</span> <span class="pre">requireInferLoop()</span> <span class="pre">const</span> <span class="pre">{</span> <span class="pre">return</span> <span class="pre">true;</span> <span class="pre">}</span></code>. The nvdsinferver low-level lib shall keep the <cite>extraInputProcess</cite> and <cite>inferenceDone</cite> running in sequence along with its nvds_stream_ids which could be get from <code class="docutils literal notranslate"><span class="pre">options-&gt;getValueArray(OPTION_NVDS_SREAM_IDS,</span> <span class="pre">streamIds)</span></code>. see examples and details in <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/sources/TritonOnnxYolo/nvdsinferserver_custom_impl_yolo/nvdsinferserver_custom_process_yolo.cpp</span></code>. Inside this example, see function <code class="docutils literal notranslate"><span class="pre">NvInferServerCustomProcess::feedbackStreamInput</span></code> how to feedback output into next input loop.</p></li>
</ul>
</section>
<section id="tensor-metadata-output-for-downstream-plugins">
<h2>Tensor Metadata Output for Downstream Plugins<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#tensor-metadata-output-for-downstream-plugins" title="Link to this heading">#</a></h2>
<p>The Gst-nvinferserver plugin can attach raw output tensor data generated by the inference backend as metadata. It is added as an NvDsInferTensorMeta in the <code class="docutils literal notranslate"><span class="pre">frame_user_meta_list</span></code> member of NvDsFrameMeta for primary (full frame) mode, or in the obj_user_meta_list member of NvDsObjectMeta for secondary (object) mode. It uses same metadata structure with Gst-nvinferserver plugin.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Gst-nvinferserver plugin does not attach device buffer pointer <code class="docutils literal notranslate"><span class="pre">NvDsInferTensorMeta::attach</span> <span class="pre">out_buf_ptrs_dev</span></code> at this moment.</p>
</div>
<section id="to-read-or-parse-inference-raw-tensor-data-of-output-layers">
<h3>To read or parse inference raw tensor data of output layers<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#to-read-or-parse-inference-raw-tensor-data-of-output-layers" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>Enable the following fields in the configuration file for the Gst-nvinferserver plugin:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>output_control { output_tensor_meta : true }
</pre></div>
</div>
</div></blockquote>
<p>If native postprocessing need be disabled, update:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>infer_config { postprocess { other {} } }
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>When operating as primary GIE, <code class="docutils literal notranslate"><span class="pre">NvDsInferTensorMeta</span></code> is attached to each frame’s (each NvDsFrameMeta object’s) <code class="docutils literal notranslate"><span class="pre">frame_user_meta_list</span></code>. When operating as secondary GIE, NvDsInferTensorMeta is attached to each NvDsObjectMeta object’s <code class="docutils literal notranslate"><span class="pre">obj_user_meta_list</span></code>.</p>
<p>Metadata attached by Gst-nvinferserver can be accessed in a GStreamer pad probe attached downstream from the Gst-nvinferserver instance.</p>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">NvDsInferTensorMeta</span></code> object’s metadata type is set to NVDSINFER_TENSOR_OUTPUT_META. To get this metadata you must iterate over the NvDsUserMeta user metadata objects in the list referenced by <code class="docutils literal notranslate"><span class="pre">frame_user_meta_list</span></code> or obj_user_meta_list.</p></li>
</ol>
<blockquote>
<div><p>For more information about Gst-infer tensor metadata usage, see the source code in <code class="docutils literal notranslate"><span class="pre">sources/apps/sample_apps/deepstream_infer_tensor_meta-test.cpp</span></code>, provided in the DeepStream SDK samples.</p>
</div></blockquote>
</section>
</section>
<section id="segmentation-metadata">
<h2>Segmentation Metadata<a class="headerlink" href="DS_plugin_gst-nvinferserver.html#segmentation-metadata" title="Link to this heading">#</a></h2>
<p>The Gst-nvinferserver plugin attaches the output of the semantic segmentation model as user metadata in an instance of NvDsInferSegmentationMeta with meta_type set to NVDSINFER_SEGMENTATION_META. The user metadata is added to the <code class="docutils literal notranslate"><span class="pre">frame_user_meta_list</span></code> member of NvDsFrameMeta for primary (full frame) mode, or the obj_user_meta_list member of NvDsObjectMeta for secondary (object) mode. For guidance on how to access user metadata, see User/Custom Metadata Addition Inside NvDsMatchMeta and Tensor Metadata, above.</p>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="DS_plugin_gst-nvinfer.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Gst-nvinfer</p>
      </div>
    </a>
    <a class="right-next"
       href="DS_plugin_gst-nvtracker.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gst-nvtracker</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#inputs-and-outputs">Inputs and Outputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#gst-nvinferserver-configuration-file-specifications">Gst-nvinferserver Configuration File Specifications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#features">Features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#low-level-libnvds-infer-server-so-configuration-file-specifications">Low Level <code class="docutils literal notranslate"><span class="pre">libnvds_infer_server.so</span></code> Configuration File Specifications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#gst-properties">Gst Properties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#deepstream-triton-samples">DeepStream Triton samples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#deepstream-triton-grpc-support">DeepStream Triton gRPC support</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#triton-ensemble-models">Triton Ensemble Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#custom-process-interface-iinfercustomprocessor-for-extra-input-lstm-loop-output-tensor-postprocess">Custom Process interface <cite>IInferCustomProcessor</cite> for Extra Input, LSTM Loop, Output Tensor Postprocess</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#tensor-metadata-output-for-downstream-plugins">Tensor Metadata Output for Downstream Plugins</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#to-read-or-parse-inference-raw-tensor-data-of-output-layers">To read or parse inference raw tensor data of output layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvinferserver.html#segmentation-metadata">Segmentation Metadata</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">



  <p class="copyright">
    
      Copyright © 2024-2025, NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item"><p class="last-updated">
  Last updated on Sep 15, 2025.
  <br/>
</p></div>
      
        <div class="footer-item">
<div class="extra_footer">
  
      <script type="text/javascript">if (typeof _satellite !== "undefined") {_satellite.pageBottom();}</script>
    
  
</div></div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>