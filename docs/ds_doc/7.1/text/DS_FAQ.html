

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" type="text/javascript"
  data-document-language="true" charset="UTF-8" data-domain-script="3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5">
</script>
<script type="text/javascript">
  function OptanonWrapper() {
    var event = new Event('bannerLoaded');
    window.dispatchEvent(event);
  }
</script>
<script src="https://images.nvidia.com/aem-dam/Solutions/ot-js/ot-custom.js" type="text/javascript">
</script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Frequently Asked Questions &#8212; DeepStream documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css%3Fv=a746c00c.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/nvidia-sphinx-theme.css%3Fv=eb367b29.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css%3Fv=7abaf8bc.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css%3Fv=95c83b7e.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js%3Fdigest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js%3Fdigest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js%3Fv=22d9b4cb"></script>
    <script src="../_static/doctools.js%3Fv=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js%3Fv=dc90522c"></script>
    <script src="../_static/design-tabs.js%3Fv=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'text/DS_FAQ';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '../versions1.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '7.1';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="../_static/version-patch.js%3Fv=c24f8c5d"></script>
    <link rel="icon" href="../_static/Nvidia.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="DeepStream On WSL" href="DS_on_WSL2.html" />
    <link rel="prev" title="Troubleshooting" href="DS_troubleshooting.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Sep 15, 2025"/>

    <script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js" ></script>
    


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="DS_FAQ.html#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="DeepStream documentation - Home"/>
    <script>document.write(`<img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark" alt="DeepStream documentation - Home"/>`);</script>
  
  
    <p class="title logo__title">DeepStream documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="DeepStream documentation - Home"/>
    <script>document.write(`<img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark" alt="DeepStream documentation - Home"/>`);</script>
  
  
    <p class="title logo__title">DeepStream documentation</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">


<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Overview.html">Welcome to the DeepStream Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Migration_guide.html">Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Quickstart.html">Quickstart Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_docker_containers.html">Docker Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Samples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_C_Sample_Apps.html">C/C++ Sample Apps Source Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Python_Sample_Apps.html">Python Sample Apps and Bindings Source Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_deepstream.html">DeepStream Reference Application - deepstream-app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_test5.html">DeepStream Reference Application - deepstream-test5 app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_nmos.html">DeepStream Reference Application - deepstream-nmos app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_github.html">DeepStream Reference Application on GitHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_sample_configs_streams.html">Sample Configurations and Streams</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_sample_custom_gstream.html">Implementing a Custom GStreamer Plugin with OpenCV Integration Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TAO toolkit Integration with DeepStream</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_TAO_integration.html">TAO Toolkit Integration with DeepStream</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials and How-to's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Custom_Manual.html">DeepStream-3D Custom Apps and Libs Tutorials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Performance.html">Performance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Accuracy</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Accuracy.html">Accuracy Tuning Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Custom Model</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_using_custom_model.html">Using a Custom Model with DeepStream</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Key Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Sensor_Fusion.html">DeepStream-3D Sensor Fusion Multi-Modal Application and Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Camera_BEVFusion.html">DeepStream-3D Multi-Modal BEVFusion Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Camera_V2XFusion.html">DeepStream-3D Multi-Modal V2XFusion Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Smart_video.html">Smart Video Record</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_IoT.html">IoT</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_on_the_fly_model.html">On the Fly Model Update</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_NTP_Timestamp.html">NTP Timestamp in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_AVSync.html">AV Sync in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_RestServer.html">DeepStream With REST API Sever</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Action.html">DeepStream 3D Action Recognition App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Depth_Camera.html">DeepStream 3D Depth Camera App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Lidar_Inference.html">DeepStream 3D Lidar Inference App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_library_nvdsnmos.html">Networked Media Open Specifications (NMOS) in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_postprocessing_plugin.html">Gst-nvdspostprocess in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Can_Orientation.html">DeepStream Can Orientation App</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Application Migration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Application_migration.html">Application Migration to DeepStream 7.1 from DeepStream 7.0</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Plugin Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="DS_plugin_Intro.html">GStreamer Plugin Overview</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_metadata.html">MetaData in the DeepStream SDK</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdspreprocess.html">Gst-nvdspreprocess (Alpha)</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvinfer.html">Gst-nvinfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvinferserver.html">Gst-nvinferserver</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvtracker.html">Gst-nvtracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreammux.html">Gst-nvstreammux</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreammux2.html">Gst-nvstreammux New</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreamdemux.html">Gst-nvstreamdemux</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmultistreamtiler.html">Gst-nvmultistreamtiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsosd.html">Gst-nvdsosd</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsmetautils.html">Gst-nvdsmetautils</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsvideotemplate.html">Gst-nvdsvideotemplate</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsaudiotemplate.html">Gst-nvdsaudiotemplate</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideoconvert.html">Gst-nvvideoconvert</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdewarper.html">Gst-nvdewarper</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvof.html">Gst-nvof</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvofvisual.html">Gst-nvofvisual</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvsegvisual.html">Gst-nvsegvisual</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideo4linux2.html">Gst-nvvideo4linux2</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvjpegdec.html">Gst-nvjpegdec</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvimagedec.html">Gst-nvimagedec</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvjpegenc.html">Gst-nvjpegenc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvimageenc.html">Gst-nvimageenc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmsgconv.html">Gst-nvmsgconv</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmsgbroker.html">Gst-nvmsgbroker</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsanalytics.html">Gst-nvdsanalytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsudpsrc.html">Gst-nvdsudpsrc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsudpsink.html">Gst-nvdsudpsink</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdspostprocess.html">Gst-nvdspostprocess (Alpha)</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dfilter.html">Gst-nvds3dfilter</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dbridge.html">Gst-nvds3dbridge</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dmixer.html">Gst-nvds3dmixer</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsucx.html">Gst-NvDsUcx</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsxfer.html">Gst-nvdsxfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideotestsrc.html">Gst-nvvideotestsrc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmultiurisrcbin.html">Gst-nvmultiurisrcbin</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvurisrcbin.html">Gst-nvurisrcbin</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Troubleshooting and FAQ</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="DS_FAQ.html#">Frequently Asked Questions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream On WSL2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_on_WSL2.html">DeepStream On WSL</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_WSL2_FAQ.html">FAQ for Deepstream On WSL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream API Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_API_Guide.html">DeepStream API Guides</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Service Maker</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_intro.html">What is Deepstream Service Maker</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_cpp.html">Service Maker for C/C++ Developers</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="DS_service_maker_python.html">Service Maker for Python Developers(alpha)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_quick_start.html">Quick Start Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_into_to_flow_api.html">Introduction to Flow APIs</a></li>

<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_into_to_pipeline_api.html">Introduction to Pipeline APIs</a></li>

<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_advanced_features.html">Advanced Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_traditional_app_migration.html">Migrating Traditional Deepstream Apps to Service Maker Apps in Python</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_plugin.html">What is a Deepstream Service Maker Plugin</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deepstream Libraries</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Libraries.html">DeepStream Libraries (Developer Preview)</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph Composer</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_intro.html">Overview</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Platforms.html">Supported platforms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Getting_Started.html">Application Development Workflow</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_GraphComposer_Create_Graph.html">Creating an AI Application</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_Sample_Graphs.html">Reference graphs</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Dev_Workflow.html">Extension Development Workflow</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_Developing_Extension.html">Developing Extensions for DeepStream</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_DS_Components.html">DeepStream Components</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Internals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Internals.html">GXF Internals</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph eXecution Engine</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Graph_Runtime.html">Graph Execution Engine</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph Composer Containers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Containers.html">Graph Composer and GXF Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Component Interfaces</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Component_Interfaces.html">GXF Component Interfaces</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Application API's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_App_C++_APIs.html">GXF App C++ APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_App_Python_APIs.html">GXF App Python APIs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Runtime API's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_C++_APIs.html">GXF Core C++ APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_C_APIs.html">GXF Core C APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_Python_APIs.html">GXF Core Python APIs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Extension Manual</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="Extensionmanual_toc.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/CudaExtension.html">CudaExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/StreamSync.html">GXF Stream Sync</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/StandardExtension.html">StandardExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/Python_Codelet.html">Python Codelets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/NetworkExtension.html">NetworkExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/NvTritonExt.html">NvTritonExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/SerializationExtension.html">SerializationExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/MultimediaExtension.html">MultimediaExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/VideoEncoderExtension.html">VideoEncoderExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/VideoDecoderExtension.html">VideoDecoderExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/Behavior_Tree.html">Behavior Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/UcxExtension.html">UCX Extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/HttpExtension.html">HttpExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/GrpcExtension.html">GrpcExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/TensorrtExtension.html">TensorRTExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDs3dProcessingExt.html">NvDs3dProcessingExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsActionRecognitionExt.html">NvDsActionRecognitionExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsAnalyticsExt.html">NvDsAnalyticsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsBaseExt.html">NvDsBaseExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsCloudMsgExt.html">NvDsCloudMsgExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsConverterExt.html">NvDsConverterExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsDewarperExt.html">NvDsDewarperExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInferenceExt.html">NvDsInferenceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInferenceUtilsExt.html">NvDsInferenceUtilsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInterfaceExt.html">NvDsInterfaceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsMuxDemuxExt.html">NvDsMuxDemuxExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsOpticalFlowExt.html">NvDsOpticalFlowExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsOutputSinkExt.html">NvDsOutputSinkExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSampleExt.html">NvDsSampleExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSampleModelsExt.html">NvDsSampleModelsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSourceExt.html">NvDsSourceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTemplateExt.html">NvDsTemplateExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTrackerExt.html">NvDsTrackerExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTranscodeExt.html">NvDsTranscodeExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTritonExt.html">NvDsTritonExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsUcxExt.html">NvDsUcxExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsUdpExt.html">NvDsUdpExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsVisualizationExt.html">NvDsVisualizationExt</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Registry.html">Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Registry_CLI.html">Registry Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Composer.html">Composer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Container_Builder.html">Container Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_gxf_CLI.html">GXF Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pipetuner-guide.html">Pipetuner Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">FAQ Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_FAQ.html">FAQ</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Legal Information</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Legal.html">DeepStream End User License Agreement</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Feedback</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DeepStream_Main_Feedback_Form.html">Feedback form</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">


<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
  </div>
  
  <div id="rtd-footer-container"></div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Frequently...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="frequently-asked-questions">
<h1>Frequently Asked Questions<a class="headerlink" href="DS_FAQ.html#frequently-asked-questions" title="Link to this heading">#</a></h1>
<section id="deepstream-general-topics">
<h2>DeepStream General topics<a class="headerlink" href="DS_FAQ.html#deepstream-general-topics" title="Link to this heading">#</a></h2>
<section id="how-do-i-uninstall-deepstream">
<h3>How do I uninstall DeepStream?<a class="headerlink" href="DS_FAQ.html#how-do-i-uninstall-deepstream" title="Link to this heading">#</a></h3>
<ul>
<li><p>For dGPU:</p>
<blockquote>
<div><p>To remove all previous DeepStream 3.0 or prior installations, enter the command:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">rm</span><span class="w"> </span><span class="o">-</span><span class="n">rf</span><span class="w"> </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">deepstream</span><span class="w"> </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span><span class="o">/</span><span class="n">gstreamer</span><span class="mf">-1.0</span><span class="o">/</span><span class="n">libgstnv</span><span class="o">*</span><span class="w"> </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">deepstream</span><span class="o">*</span><span class="w"> </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span><span class="o">/</span><span class="n">gstreamer</span><span class="mf">-1.0</span><span class="o">/</span><span class="n">libnvdsgst</span><span class="o">*</span>
<span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span><span class="o">/</span><span class="n">gstreamer</span><span class="mf">-1.0</span><span class="o">/</span><span class="n">deepstream</span><span class="o">*</span>
<span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">deepstream</span><span class="o">*</span>
<span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">rm</span><span class="w"> </span><span class="o">-</span><span class="n">rf</span><span class="w"> </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span><span class="o">/</span><span class="n">libv41</span><span class="o">/</span><span class="n">plugins</span><span class="o">/</span><span class="n">libcuvidv4l2_plugin</span><span class="p">.</span><span class="n">so</span>
</pre></div>
</div>
<p>To remove DeepStream 4.0 or later installations:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Open the uninstall.sh file in <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/</span></code></p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">PREV_DS_VER</span></code> as <code class="docutils literal notranslate"><span class="pre">4.0</span></code></p></li>
<li><p>Run the script as sudo: <code class="docutils literal notranslate"><span class="pre">./uninstall.sh</span></code></p></li>
</ol>
</div></blockquote>
</div></blockquote>
</li>
<li><p>For Jetson: Flash the target device with the latest release of JetPack.</p></li>
</ul>
</section>
<section id="what-types-of-input-streams-does-deepstream-7-1-support">
<h3>What types of input streams does DeepStream 7.1 support?<a class="headerlink" href="DS_FAQ.html#what-types-of-input-streams-does-deepstream-7-1-support" title="Link to this heading">#</a></h3>
<p>It supports H.264, H.265, JPEG, and MJPEG streams.</p>
</section>
<section id="what-s-the-throughput-of-h-264-and-h-265-decode-on-dgpu-tesla">
<h3>What’s the throughput of H.264 and H.265 decode on dGPU (Tesla)?<a class="headerlink" href="DS_FAQ.html#what-s-the-throughput-of-h-264-and-h-265-decode-on-dgpu-tesla" title="Link to this heading">#</a></h3>
<p>See <a class="reference external" href="https://developer.nvidia.com/nvidia-video-codec-sdk">https://developer.nvidia.com/nvidia-video-codec-sdk</a> for information.</p>
</section>
<section id="how-can-i-run-the-deepstream-sample-application-in-debug-mode">
<h3>How can I run the DeepStream sample application in debug mode?<a class="headerlink" href="DS_FAQ.html#how-can-i-run-the-deepstream-sample-application-in-debug-mode" title="Link to this heading">#</a></h3>
<p>Enter this command:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">deepstream-app</span> <span class="pre">-c</span> <span class="pre">&lt;config&gt;</span> <span class="pre">--gst-debug=&lt;debug#&gt;</span></code></p>
</div></blockquote>
<p>Where:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;config&gt;</span></code> is the pathname of the configuration file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;debug#&gt;</span></code> is a number specifying the amount of detail in the debugging output</p></li>
</ul>
</div></blockquote>
<p>For information about debugging tools, see:
<a class="reference external" href="https://gstreamer.freedesktop.org/documentation/tutorials/basic/debugging-tools.html">https://gstreamer.freedesktop.org/documentation/tutorials/basic/debugging-tools.html</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Running Deepstream app over SSH (via putty) with X11 forwarding does not work.</p>
</div>
</section>
<section id="where-can-i-find-the-deepstream-sample-applications">
<h3>Where can I find the DeepStream sample applications?<a class="headerlink" href="DS_FAQ.html#where-can-i-find-the-deepstream-sample-applications" title="Link to this heading">#</a></h3>
<p>The DeepStream sample applications are located at:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">&lt;DeepStream</span> <span class="pre">installation</span> <span class="pre">dir&gt;/sources/apps/sample_apps/</span></code></p>
</div></blockquote>
<p>The configuration files for the sample applications are located at:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">&lt;DeepStream</span> <span class="pre">installation</span> <span class="pre">dir&gt;/samples/configs/deepstream-app</span></code></p>
</div></blockquote>
<p>For more information, see the <em>NVIDIA DeepStream Development Guide</em>.</p>
</section>
<section id="how-can-i-verify-that-cuda-was-installed-correctly">
<h3>How can I verify that CUDA was installed correctly?<a class="headerlink" href="DS_FAQ.html#how-can-i-verify-that-cuda-was-installed-correctly" title="Link to this heading">#</a></h3>
<p>Check the CUDA version:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">nvcc</span> <span class="pre">--version</span></code></p>
</div></blockquote>
</section>
<section id="how-can-i-interpret-frames-per-second-fps-display-information-on-console">
<h3>How can I interpret frames per second (FPS) display information on console?<a class="headerlink" href="DS_FAQ.html#how-can-i-interpret-frames-per-second-fps-display-information-on-console" title="Link to this heading">#</a></h3>
<p>The FPS number shown on the console when deepstream-app runs is an average of the most recent five seconds. The number in brackets is average FPS over the entire run. The numbers are displayed per stream. The performance measurement interval is set by the <code class="docutils literal notranslate"><span class="pre">perf-measurement-interval-sec</span></code> setting in the configuration file.</p>
</section>
<section id="my-deepstream-performance-is-lower-than-expected-how-can-i-determine-the-reason">
<h3>My DeepStream performance is lower than expected. How can I determine the reason?<a class="headerlink" href="DS_FAQ.html#my-deepstream-performance-is-lower-than-expected-how-can-i-determine-the-reason" title="Link to this heading">#</a></h3>
<p>See the <a class="reference internal" href="DS_troubleshooting.html"><span class="doc">Troubleshooting</span></a> chapter in the <em>NVIDIA DeepStream Development Guide</em>.</p>
</section>
<section id="how-can-i-specify-rtsp-streaming-of-deepstream-output">
<h3>How can I specify RTSP streaming of DeepStream output?<a class="headerlink" href="DS_FAQ.html#how-can-i-specify-rtsp-streaming-of-deepstream-output" title="Link to this heading">#</a></h3>
<p>You can enable remote display by adding an RTSP sink in the application configuration file. The sample configuration file <code class="docutils literal notranslate"><span class="pre">source30_1080p_dec_infer-resnet_tiled_display_int8.txt</span></code> has an example of this in the <code class="docutils literal notranslate"><span class="pre">[sink2]</span></code> section. You must set the <code class="docutils literal notranslate"><span class="pre">enable</span></code> flag to <code class="docutils literal notranslate"><span class="pre">1</span></code>.
Once you enable remote display, the application prints the RTSP URL, which you can open in any media player like VLC.</p>
<p>Note: When using VLC to view DeepStream RTSP sink output remotely, please force usage of TCP to avoid unnecessary packet drops.</p>
<p>Example command to force TCP:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">vlc</span> <span class="pre">--rtsp-tcp</span> <span class="pre">rtsp://path/to/stream</span></code></p>
</div></blockquote>
</section>
<section id="what-is-the-official-deepstream-docker-image-and-where-do-i-get-it">
<h3>What is the official DeepStream Docker image and where do I get it?<a class="headerlink" href="DS_FAQ.html#what-is-the-official-deepstream-docker-image-and-where-do-i-get-it" title="Link to this heading">#</a></h3>
<p>You can download the official DeepStream Docker image from DeepStream docker image.
For dGPU, see: <a class="reference external" href="https://ngc.nvidia.com/containers/nvidia:deepstream">https://ngc.nvidia.com/containers/nvidia:deepstream</a>. For Jetson, see: <a class="reference external" href="https://ngc.nvidia.com/containers/nvidia:deepstream-l4t">https://ngc.nvidia.com/containers/nvidia:deepstream-l4t</a></p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>nvdrmvideosink plugin is not supported in docker. It is an unsupported use case, nvdrmvideosink only work well when there is no compositor
running in background, and that is not possible inside a docker.</p>
</div>
</div></blockquote>
</section>
<section id="what-is-the-recipe-for-creating-my-own-docker-image">
<h3>What is the recipe for creating my own Docker image?<a class="headerlink" href="DS_FAQ.html#what-is-the-recipe-for-creating-my-own-docker-image" title="Link to this heading">#</a></h3>
<p>One can use the DeepStream container as the base image and add custom layers on top of it using standard technique in Docker.</p>
<p>Alternatively, follow below  steps if you want to create a DeepStream triton docker on the top of a particular triton base image.</p>
<p>1. For example to build a custom Triton base container image, follow the instructions mentioned on <a class="reference external" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/customization_guide/compose.html#use-the-compose-py-script">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/customization_guide/compose.html#use-the-compose-py-script</a>
Once the “server repository” is cloned use below command to create the base docker.</p>
<p><code class="docutils literal notranslate"><span class="pre">python3</span> <span class="pre">compose.py</span> <span class="pre">--backend</span> <span class="pre">tensorrt</span> <span class="pre">--backend</span> <span class="pre">onnxruntime</span> <span class="pre">--repoagent</span>&#160; <span class="pre">checksum</span></code></p>
<p>The command will create a tritonserver  container locally named “tritonserver:latest”.</p>
<p>2. Go through README mentioned on <a class="github reference external" href="https://github.com/NVIDIA-AI-IOT/deepstream_dockers#4-triton-migration-guide">NVIDIA-AI-IOT/deepstream_dockers</a>
In the required Dockerfile, edit the FROM command to use the base docker name created in step 1 e.g. <code class="docutils literal notranslate"><span class="pre">FROM</span> <span class="pre">tritonserver:latest</span></code></p>
<p>Once the Dockerfile is updated, follow README to create a DeepStream based docker on the top of pulled triton based docker.</p>
</section>
<section id="how-can-i-display-graphical-output-remotely-over-vnc-how-can-i-determine-whether-x11-is-running">
<h3>How can I display graphical output remotely over VNC? How can I determine whether X11 is running?<a class="headerlink" href="DS_FAQ.html#how-can-i-display-graphical-output-remotely-over-vnc-how-can-i-determine-whether-x11-is-running" title="Link to this heading">#</a></h3>
<p>If the host machine is running X, starting VNC is trivial. Otherwise you must start X, then start VNC.
To determine whether X is running, check the DISPLAY environment variable.
If X is not running you must start it first, then run DeepStream with GUI, or set type to 1 or 3 under sink groups to select fakesink or save to a file. If you are using an NVIDIA® Tesla® GPU Accelerators (compute-only cards without a display), you can set type to 4 for DeepStream output RTSP streaming. See the NVIDIA DeepStream SDK Development Guide for sink settings or create a virtual display to visualize the output using VNC. Refer to <a class="reference internal" href="DS_Quickstart.html#visualize-output"><span class="std std-ref">How to visualize the output if the display is not attached to the system</span></a> in <a class="reference internal" href="DS_Quickstart.html"><span class="doc">Quickstart Guide</span></a> for more details</p>
</section>
<section id="why-does-the-deepstream-nvof-test-application-show-the-error-message-device-does-not-support-optical-flow-functionality">
<h3>Why does the deepstream-nvof-test application show the error message “Device Does NOT support Optical Flow Functionality” ?<a class="headerlink" href="DS_FAQ.html#why-does-the-deepstream-nvof-test-application-show-the-error-message-device-does-not-support-optical-flow-functionality" title="Link to this heading">#</a></h3>
<p>Optical flow functionality is supported only on NVIDIA® Jetson AGX Orin, NVIDIA® Jetson Orin NX™ and on GPUs with Turing architecture (NVIDIA® T4, NVIDIA® GeForce® RTX 2080 etc.).</p>
</section>
<section id="why-is-the-gst-nvstreammux-plugin-required-in-deepstream-4-0">
<h3>Why is the Gst-nvstreammux plugin required in DeepStream 4.0+?<a class="headerlink" href="DS_FAQ.html#why-is-the-gst-nvstreammux-plugin-required-in-deepstream-4-0" title="Link to this heading">#</a></h3>
<p>Multiple source components like decoder, camera, etc. are connected to the Gst-nvstreammux plugin to form a batch.
This plugin is responsible for creating batch metadata, which is stored in the structure <code class="docutils literal notranslate"><span class="pre">NvDsBatchMeta</span></code>. This is the primary form of metadata in DeepStream 4.0.1.
All plugins downstream from Gst-nvstreammux work on <code class="docutils literal notranslate"><span class="pre">NvDsBatchMeta</span></code> to access metadata and fill in the metadata they generate.</p>
</section>
<section id="how-do-i-profile-deepstream-pipeline">
<h3>How do I profile DeepStream pipeline?<a class="headerlink" href="DS_FAQ.html#how-do-i-profile-deepstream-pipeline" title="Link to this heading">#</a></h3>
<p>You can use NVIDIA<sup>®</sup> Nsight<sup>™</sup> Systems, a system-wide performance analysis tool. See <a class="reference external" href="https://developer.nvidia.com/nsight-systems">https://developer.nvidia.com/nsight-systems</a> for more details.</p>
</section>
<section id="how-can-i-check-gpu-and-memory-utilization-on-a-dgpu-system">
<h3>How can I check GPU and memory utilization on a dGPU system?<a class="headerlink" href="DS_FAQ.html#how-can-i-check-gpu-and-memory-utilization-on-a-dgpu-system" title="Link to this heading">#</a></h3>
<p>Enter <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> or <code class="docutils literal notranslate"><span class="pre">nvidia-settings</span></code> on the console.</p>
</section>
<section id="what-is-the-approximate-memory-utilization-for-1080p-streams-on-dgpu">
<h3>What is the approximate memory utilization for 1080p streams on dGPU?<a class="headerlink" href="DS_FAQ.html#what-is-the-approximate-memory-utilization-for-1080p-streams-on-dgpu" title="Link to this heading">#</a></h3>
<p>Use the table below as a guide to memory utilization in this case.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Width and height in Gst-nvstreammux are set to the input stream resolution specified in the configuration file.
The pipeline is: <code class="docutils literal notranslate"><span class="pre">decoder</span> <span class="pre">|rarr|</span> <span class="pre">nvstreammux</span> <span class="pre">|rarr|</span> <span class="pre">nvinfer</span> <span class="pre">|rarr|</span> <span class="pre">fakesink</span></code>.</p>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id1">
<caption><span class="caption-text">Memory utilization for 1080p streams</span><a class="headerlink" href="DS_FAQ.html#id1" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Batch size
(Number of streams)</p></th>
<th class="head"><p>Decode memory</p></th>
<th class="head"><p>Gst-nvinfer
memory</p></th>
<th class="head"><p>Gst-nvstreammux
memory</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>32 MB</p></td>
<td><p>333 MB</p></td>
<td><p>0 MB</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>64 MB</p></td>
<td><p>341 MB</p></td>
<td><p>0 MB</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>128 MB</p></td>
<td><p>359 MB</p></td>
<td><p>0 MB</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>256 MB</p></td>
<td><p>391 MB</p></td>
<td><p>0 MB</p></td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><p>512 MB</p></td>
<td><p>457 MB</p></td>
<td><p>0 MB</p></td>
</tr>
</tbody>
</table>
</div>
<p>If input stream resolution and Gst-nvstreammux resolution (set in the configuration file) are the same, no additional GPU memory is allocated in Gst-nvstreammux.
If input stream resolution is not same as Gst-nvstreammux resolution, Gst-nvstreammux allocates memory of size:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">buffers*(1.5*width*height)*mismatches</span></code></p>
</div></blockquote>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">buffers</span></code> is the number of Gst-nvstreammux output buffers (set to 4).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">width</span></code> and <code class="docutils literal notranslate"><span class="pre">height</span></code> are the mux output width and height.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mismatches</span></code> is the number of sources with resolution mismatch.</p></li>
</ul>
<p>This table shows some examples:</p>
<div class="pst-scrollable-table-container"><table class="table" id="id2">
<caption><span class="caption-text">Memory allocation for 1080p streams</span><a class="headerlink" href="DS_FAQ.html#id2" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 35.0%" />
<col style="width: 30.0%" />
<col style="width: 35.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Example</p></th>
<th class="head"><p>Gst-nvstreammux <code class="docutils literal notranslate"><span class="pre">width*height</span></code> settings</p></th>
<th class="head"><p>Gst-nvstreammux GPU memory size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>16 sources at <code class="docutils literal notranslate"><span class="pre">1920*1080</span></code> resolution</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1280*720</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">4*(1.5*1280*720)*16</span> <span class="pre">=</span> <span class="pre">84</span></code> MB</p></td>
</tr>
<tr class="row-odd"><td><p>15 sources at <code class="docutils literal notranslate"><span class="pre">1280*720</span></code> resolution and one source at <code class="docutils literal notranslate"><span class="pre">1920*1080</span></code> resolution</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1280*720</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">4*(1.5*1280*720)*1</span> <span class="pre">=</span> <span class="pre">5.2</span></code> MB</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="deepstream-apps-fails-while-encoding-on-orin-nano">
<h3>DeepStream apps fails while encoding on Orin Nano<a class="headerlink" href="DS_FAQ.html#deepstream-apps-fails-while-encoding-on-orin-nano" title="Link to this heading">#</a></h3>
<p>Orin does not have HW encoder. You should modify the application code to use SW encoder.</p>
</section>
<section id="when-deepstream-app-is-run-in-loop-on-jetson-agx-orin-using-while-true-do-deepstream-app-c-config-file-done-after-a-few-iterations-i-see-low-fps-for-certain-iterations-why-is-that">
<h3>When deepstream-app is run in loop on Jetson AGX Orin using “while true; do deepstream-app -c &lt;config_file&gt;; done;”, after a few iterations I see low FPS for certain iterations. Why is that?<a class="headerlink" href="DS_FAQ.html#when-deepstream-app-is-run-in-loop-on-jetson-agx-orin-using-while-true-do-deepstream-app-c-config-file-done-after-a-few-iterations-i-see-low-fps-for-certain-iterations-why-is-that" title="Link to this heading">#</a></h3>
<p>This may happen when you are running thirty 1080p streams at 30 frames/second. The issue is caused by initial load. I/O operations bog down the CPU, and with <code class="docutils literal notranslate"><span class="pre">qos=1</span></code> as a default property of the <code class="docutils literal notranslate"><span class="pre">[sink0]</span></code> group, <code class="docutils literal notranslate"><span class="pre">decodebin</span></code> starts dropping frames. To avoid this, set <code class="docutils literal notranslate"><span class="pre">qos=0</span></code> in the <code class="docutils literal notranslate"><span class="pre">[sink0]</span></code> group in the configuration file.</p>
</section>
<section id="why-do-i-get-the-error-incorrect-camera-parameters-provided-please-provide-supported-resolution-and-frame-rate-when-i-compile-deepstream-sample-application-with-source1-usb-dec-infer-resnet-int8-txt-config-in-default-setting-on-jetson">
<h3>Why do I get the error <code class="docutils literal notranslate"><span class="pre">incorrect</span> <span class="pre">camera</span> <span class="pre">parameters</span> <span class="pre">provided,</span> <span class="pre">please</span> <span class="pre">provide</span> <span class="pre">supported</span> <span class="pre">resolution</span> <span class="pre">and</span> <span class="pre">frame</span> <span class="pre">rate</span></code> when I compile DeepStream sample application with source1_usb_dec_infer_resnet_int8.txt config in default setting on Jetson?<a class="headerlink" href="DS_FAQ.html#why-do-i-get-the-error-incorrect-camera-parameters-provided-please-provide-supported-resolution-and-frame-rate-when-i-compile-deepstream-sample-application-with-source1-usb-dec-infer-resnet-int8-txt-config-in-default-setting-on-jetson" title="Link to this heading">#</a></h3>
<p>This is because <cite>nvdrmvideosink</cite> does not work along with <cite>Xorg</cite>. Refer to <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/README</span></code> (Notes section, Point#3) to resolve the error.</p>
</section>
<section id="why-do-on-jetson-engine-file-generation-fails-occasionally-inside-container">
<h3>Why do on Jetson, engine file generation fails occasionally inside container?<a class="headerlink" href="DS_FAQ.html#why-do-on-jetson-engine-file-generation-fails-occasionally-inside-container" title="Link to this heading">#</a></h3>
<p>This is due to a bug in TensorRT-10.3 version. The issue is fixed in TensorRT-10.5 version. So, to overcome this issue, you should either install TensorRT-10.5 inside the docker or generate the engine file outside docker, on baremetal, and copy it inside the docker.</p>
</section>
<section id="why-the-application-hangs-sometimes-with-rtsp-streams-on-reaching-eos">
<h3>Why the application hangs sometimes with RTSP streams on reaching EOS?<a class="headerlink" href="DS_FAQ.html#why-the-application-hangs-sometimes-with-rtsp-streams-on-reaching-eos" title="Link to this heading">#</a></h3>
<p>This is because of an issue in rtpjitterbuffer component. To fix this issue,a script “update_rtpmanager.sh” at <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/</span></code> has been provided with required details to update gstrtpmanager library.
The script should be executed once user installs packages mentioned under “Install Dependencies” section of Quickstart Guide.</p>
</section>
<section id="why-do-i-get-error-glib-gthread-posix-c-unexpected-error-from-c-library-during-pthread-setspecific-invalid-argument-aborting">
<h3>Why do I get error <code class="docutils literal notranslate"><span class="pre">GLib</span> <span class="pre">(gthread-posix.c):</span> <span class="pre">Unexpected</span> <span class="pre">error</span> <span class="pre">from</span> <span class="pre">C</span> <span class="pre">library</span> <span class="pre">during</span> <span class="pre">'pthread_setspecific':</span> <span class="pre">Invalid</span> <span class="pre">argument.</span>&#160; <span class="pre">Aborting.</span></code><a class="headerlink" href="DS_FAQ.html#why-do-i-get-error-glib-gthread-posix-c-unexpected-error-from-c-library-during-pthread-setspecific-invalid-argument-aborting" title="Link to this heading">#</a></h3>
<p>The issue is caused because of a bug in glib 2.0-2.72 version which comes with ubuntu22.04 by default.
The issue is addressed in glib2.76 and its installation is required to fix the issue (<a class="github reference external" href="https://github.com/GNOME/glib/tree/2.76.6">GNOME/glib</a>).</p>
</section>
<section id="why-do-i-get-the-error-makefile-13-cuda-ver-is-not-set-stop-when-i-compile-deepstream-sample-applications">
<h3>Why do I get the error <code class="docutils literal notranslate"><span class="pre">Makefile:13:</span> <span class="pre">***</span> <span class="pre">&quot;CUDA_VER</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">set&quot;.</span>&#160; <span class="pre">Stop</span></code> when I compile DeepStream sample applications?<a class="headerlink" href="DS_FAQ.html#why-do-i-get-the-error-makefile-13-cuda-ver-is-not-set-stop-when-i-compile-deepstream-sample-applications" title="Link to this heading">#</a></h3>
<p>Export this environment variable:</p>
<ul class="simple">
<li><p>For both Jetson &amp; x86 : <code class="docutils literal notranslate"><span class="pre">CUDA_VER=12.6</span></code></p></li>
</ul>
<p>Then compile again using <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">-E</span> <span class="pre">make</span></code>.</p>
</section>
<section id="how-can-i-construct-the-deepstream-gstreamer-pipeline">
<h3>How can I construct the DeepStream GStreamer pipeline?<a class="headerlink" href="DS_FAQ.html#how-can-i-construct-the-deepstream-gstreamer-pipeline" title="Link to this heading">#</a></h3>
<p>Here are few examples of how to construct the pipeline. To run these example pipelines as-is, run the applications from the samples directory:</p>
<ul>
<li><p>V4l2 decoder → nvinfer → nvtracker → nvinfer (secondary) → nvmultistreamtiler → nvdsosd → nveglglessink</p>
<blockquote>
<div><ul>
<li><p>For multistream (4x1080p) operation on dGPU:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ gst-launch-1.0 filesrc location= streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder \
! m.sink_0 nvstreammux name=m batch-size=4 width=1920 height=1080 ! nvinfer config-file-path= configs/deepstream-app/config_infer_primary.txt \
batch-size=4 unique-id=1 ! nvtracker ll-lib-file=/opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so \
! nvinfer config-file-path= configs/deepstream-app/config_infer_secondary_vehicletypes.txt batch-size=16 unique-id=2 infer-on-gie-id=1 infer-on-class-ids=0 \
! nvmultistreamtiler rows=2 columns=2 width=1280 height=720 ! nvvideoconvert ! nvdsosd ! nveglglessink filesrc location= streams/sample_1080p_h264.mp4 \
! qtdemux ! h264parse ! nvv4l2decoder ! m.sink_1 filesrc location= streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder \
! m.sink_2 filesrc location= streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder ! m.sink_3
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>For multistream (4x1080p) operation on Jetson:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ gst-launch-1.0 filesrc location= streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder \
! m.sink_0 nvstreammux name=m batch-size=4 width=1920 height=1080 ! nvinfer config-file-path= configs/deepstream-app/config_infer_primary.txt \
batch-size=4 unique-id=1 ! nvtracker ll-lib-file=/opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so \
! nvinfer config-file-path= configs/deepstream-app/config_infer_secondary_vehicletypes.txt batch-size=16 unique-id=2 infer-on-gie-id=1 infer-on-class-ids=0 \
! nvmultistreamtiler rows=2 columns=2 width=1280 height=720 ! nvvideoconvert ! nvdsosd ! nv3dsink \
filesrc location= streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder \
! m.sink_1 filesrc location= streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder \
! m.sink_2 filesrc location= streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder ! m.sink_3
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>For single stream (1080p) operation on dGPU:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">gst</span><span class="o">-</span><span class="n">launch</span><span class="mf">-1.0</span><span class="w"> </span><span class="n">filesrc</span><span class="w"> </span><span class="n">location</span><span class="o">=</span><span class="w"> </span><span class="n">streams</span><span class="o">/</span><span class="n">sample_1080p_h264</span><span class="p">.</span><span class="n">mp4</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">qtdemux</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">h264parse</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvv4l2decoder</span><span class="w"> </span>\
<span class="o">!</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">sink_0</span><span class="w"> </span><span class="n">nvstreammux</span><span class="w"> </span><span class="n">name</span><span class="o">=</span><span class="n">m</span><span class="w"> </span><span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="mi">1920</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="mi">1080</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvinfer</span><span class="w"> </span><span class="n">config</span><span class="o">-</span><span class="n">file</span><span class="o">-</span><span class="n">path</span><span class="o">=</span><span class="w"> </span><span class="n">configs</span><span class="o">/</span><span class="n">deepstream</span><span class="o">-</span><span class="n">app</span><span class="o">/</span><span class="n">config_infer_primary</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">unique</span><span class="o">-</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvtracker</span><span class="w"> </span><span class="n">ll</span><span class="o">-</span><span class="n">lib</span><span class="o">-</span><span class="n">file</span><span class="o">=/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">libnvds_nvmultiobjecttracker</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="o">!</span><span class="w"> </span><span class="n">nvinfer</span><span class="w"> </span><span class="n">config</span><span class="o">-</span><span class="n">file</span><span class="o">-</span><span class="n">path</span><span class="o">=</span><span class="w"> </span><span class="n">configs</span><span class="o">/</span><span class="n">deepstream</span><span class="o">-</span><span class="n">app</span><span class="o">/</span><span class="n">config_infer_secondary_vehicletypes</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="w"> </span><span class="n">unique</span><span class="o">-</span><span class="n">id</span><span class="o">=</span><span class="mi">2</span><span class="w"> </span><span class="n">infer</span><span class="o">-</span><span class="n">on</span><span class="o">-</span><span class="n">gie</span><span class="o">-</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">infer</span><span class="o">-</span><span class="n">on</span><span class="o">-</span><span class="k">class</span><span class="o">-</span><span class="n">ids</span><span class="o">=</span><span class="mi">0</span><span class="w"> </span>\
<span class="o">!</span><span class="w"> </span><span class="n">nvmultistreamtiler</span><span class="w"> </span><span class="n">rows</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">columns</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="mi">1280</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="mi">720</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvvideoconvert</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvdsosd</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nveglglessink</span>
</pre></div>
</div>
</li>
<li><p>For single stream (1080p) operation on Jetson:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">gst</span><span class="o">-</span><span class="n">launch</span><span class="mf">-1.0</span><span class="w"> </span><span class="n">filesrc</span><span class="w"> </span><span class="n">location</span><span class="o">=</span><span class="w"> </span><span class="n">streams</span><span class="o">/</span><span class="n">sample_1080p_h264</span><span class="p">.</span><span class="n">mp4</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">qtdemux</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">h264parse</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvv4l2decoder</span><span class="w"> </span>\
<span class="o">!</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">sink_0</span><span class="w"> </span><span class="n">nvstreammux</span><span class="w"> </span><span class="n">name</span><span class="o">=</span><span class="n">m</span><span class="w"> </span><span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="mi">1920</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="mi">1080</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvinfer</span><span class="w"> </span><span class="n">config</span><span class="o">-</span><span class="n">file</span><span class="o">-</span><span class="n">path</span><span class="o">=</span><span class="w"> </span><span class="n">configs</span><span class="o">/</span><span class="n">deepstream</span><span class="o">-</span><span class="n">app</span><span class="o">/</span><span class="n">config_infer_primary</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span>\
<span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">unique</span><span class="o">-</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvtracker</span><span class="w"> </span><span class="n">ll</span><span class="o">-</span><span class="n">lib</span><span class="o">-</span><span class="n">file</span><span class="o">=/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">deepstream</span>
<span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">libnvds_nvmultiobjecttracker</span><span class="p">.</span><span class="n">so</span><span class="w"> </span>\
<span class="o">!</span><span class="w"> </span><span class="n">nvinfer</span><span class="w"> </span><span class="n">config</span><span class="o">-</span><span class="n">file</span><span class="o">-</span><span class="n">path</span><span class="o">=</span><span class="w"> </span><span class="n">configs</span><span class="o">/</span><span class="n">deepstream</span><span class="o">-</span><span class="n">app</span><span class="o">/</span><span class="n">config_infer_secondary_vehicletypes</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="w"> </span><span class="n">unique</span><span class="o">-</span><span class="n">id</span><span class="o">=</span><span class="mi">2</span><span class="w"> </span><span class="n">infer</span><span class="o">-</span><span class="n">on</span><span class="o">-</span><span class="n">gie</span><span class="o">-</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">infer</span><span class="o">-</span><span class="n">on</span><span class="o">-</span><span class="k">class</span><span class="o">-</span><span class="n">ids</span><span class="o">=</span><span class="mi">0</span><span class="w"> </span>\
<span class="o">!</span><span class="w"> </span><span class="n">nvmultistreamtiler</span><span class="w"> </span><span class="n">rows</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">columns</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="mi">1280</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="mi">720</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvvideoconvert</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvdsosd</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nv3dsink</span>
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</li>
<li><p>JPEG decode</p>
<blockquote>
<div><ul>
<li><p>Using nvv4l2decoder on Jetson:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">gst-launch-1.0</span> <span class="pre">filesrc</span> <span class="pre">location=</span> <span class="pre">./streams/sample_720p.jpg</span> <span class="pre">!</span> <span class="pre">jpegparse</span> <span class="pre">!</span> <span class="pre">nvv4l2decoder</span> <span class="pre">!</span> <span class="pre">nv3dsink</span></code></p>
</div></blockquote>
</li>
<li><p>Using nvv4l2decoder on dGPU:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">gst-launch-1.0</span> <span class="pre">filesrc</span> <span class="pre">location=</span> <span class="pre">./streams/sample_720p.jpg</span> <span class="pre">!</span> <span class="pre">jpegparse</span> <span class="pre">!</span> <span class="pre">nvv4l2decoder</span> <span class="pre">!</span> <span class="pre">nveglglessink</span></code></p>
</div></blockquote>
</li>
<li><p>Using nvjpegdec on Jetson:</p>
<p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">gst-launch-1.0</span> <span class="pre">filesrc</span> <span class="pre">location=</span> <span class="pre">./streams/sample_720p.jpg</span> <span class="pre">!</span> <span class="pre">nvjpegdec</span> <span class="pre">!</span> <span class="pre">nv3dsink</span></code></p>
</li>
<li><p>Using nvjpegdec on dGPU:</p>
<p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">gst-launch-1.0</span> <span class="pre">filesrc</span> <span class="pre">location=</span> <span class="pre">./streams/sample_720p.jpg</span> <span class="pre">!</span> <span class="pre">nvjpegdec</span> <span class="pre">!</span>&#160;&#160; <span class="pre">nveglglessink</span></code></p>
</li>
</ul>
</div></blockquote>
</li>
<li><p>Dewarper</p>
<ul class="simple">
<li><p>On dGPU:</p></li>
</ul>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ gst-launch-1.0 uridecodebin uri= file://`pwd`/../../../../samples/streams/sample_cam6.mp4 ! nvvideoconvert \
! nvdewarper source-id=6 num-output-buffers=4 config-file=config_dewarper.txt ! m.sink_0 nvstreammux name=m width=1280 height=720 batch-size=4 \
batched-push-timeout=100000 num-surfaces-per-frame=4 ! nvmultistreamtiler rows=1 columns=1 width=720 height=576 ! nvvideoconvert ! nveglglessink
</pre></div>
</div>
</div></blockquote>
<ul class="simple">
<li><p>On Jetson:</p></li>
</ul>
<blockquote>
<div><blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ gst-launch-1.0 uridecodebin uri= file://`pwd`/../../../../samples/streams/sample_cam6.mp4 ! nvvideoconvert ! nvdewarper source-id=6 num-output-buffers=4 \
config-file=config_dewarper.txt ! m.sink_0 nvstreammux name=m width=1280 height=720 batch-size=4 batched-push-timeout=100000 \
num-surfaces-per-frame=4 ! nvmultistreamtiler rows=1 columns=1 width=720 height=576 ! nvvideoconvert ! nv3dsink
</pre></div>
</div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This Gst pipeline must be run from the dewarper test application directory, <code class="docutils literal notranslate"><span class="pre">sources/apps/sample_apps/deepstream-dewarper-test</span></code>.
This pipeline runs only for four surfaces. To run for one, two, or three surfaces, use the dewarper test application.</p>
</div>
</div></blockquote>
</li>
<li><p>Dsexample</p>
<ul class="simple">
<li><p>On dGPU:</p></li>
</ul>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ gst-launch-1.0 filesrc location = ./streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder \
! m.sink_0 nvstreammux name=m width=1280 height=720 batch-size=1  ! nvinfer config-file-path= ./configs/deepstream-app/config_infer_primary.txt \
! dsexample full-frame=1 ! nvvideoconvert ! nvdsosd ! nveglglessink sync=0
</pre></div>
</div>
</div></blockquote>
<ul class="simple">
<li><p>On Jetson:</p></li>
</ul>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ gst-launch-1.0 filesrc location = ./streams/sample_1080p_h264.mp4 ! qtdemux ! h264parse ! nvv4l2decoder \
! m.sink_0 nvstreammux name=m width=1280 height=720 batch-size=1  ! nvinfer config-file-path= ./configs/deepstream-app/config_infer_primary.txt \
! dsexample full-frame=1 ! nvvideoconvert ! nvdsosd ! nv3dsink sync=0
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="how-to-set-camera-calibration-parameters-in-dewarper-plugin-config-file">
<h3>How to set camera calibration parameters in Dewarper plugin config file?<a class="headerlink" href="DS_FAQ.html#how-to-set-camera-calibration-parameters-in-dewarper-plugin-config-file" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Focal length:</p></li>
</ol>
<blockquote>
<div><p>Source focal length can be set using the property <code class="docutils literal notranslate"><span class="pre">focal-length</span></code> in dewarper configuration file. It has the units of “pixels per radian”.
It can be specified as a single focal length or two different focal lengths for X and Y directions.</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Center of projection/optical center/principal point:</p></li>
</ol>
<blockquote>
<div><p>The principal point can be set using the properties <code class="docutils literal notranslate"><span class="pre">src-x0</span></code> and <code class="docutils literal notranslate"><span class="pre">src-y0</span></code>.</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>Distortion  coefficients:</p></li>
</ol>
<blockquote>
<div><p>Up to 5 (k0 to k4) distortion coefficients can be configured in Dewarper plugin using the property <code class="docutils literal notranslate"><span class="pre">distortion</span></code>.
The plugin supports two types of distortion coefficients based on the type of input/source camera:</p>
<blockquote>
<div><ol class="arabic">
<li><p>Perspective camera input : To correct the distortion caused by perspective camera, 5 distortion coefficients are supported (k0 to k4) which consist of:</p>
<blockquote>
<div><p>3  radial distortion coefficients : k0 to k2</p>
<p>2 tangential distortion coefficients : k3 &amp; k4</p>
</div></blockquote>
</li>
<li><p>Fisheye camera input: To correct fisheye distortion, 4 distortion coefficients (k0 to  k3) need to be specified.</p></li>
<li><p>The distortion coefficients are unused for equirectangular input sources.</p></li>
</ol>
</div></blockquote>
</div></blockquote>
</section>
<section id="how-to-get-camera-calibration-parameters-for-usage-in-dewarper-plugin">
<h3>How to get camera calibration parameters for usage in Dewarper plugin?<a class="headerlink" href="DS_FAQ.html#how-to-get-camera-calibration-parameters-for-usage-in-dewarper-plugin" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>Using calibrations algorithms like the one in OpenCV. For more details, please refer the following OpenCV links:</p>
<p>For perspective input : <a class="reference external" href="https://docs.opencv.org/2.4.13.7/doc/tutorials/calib3d/camera_calibration/camera_calibration.html">https://docs.opencv.org/2.4.13.7/doc/tutorials/calib3d/camera_calibration/camera_calibration.html</a></p>
<p>For fisheye input : <a class="reference external" href="https://docs.opencv.org/3.4/db/d58/group__calib3d__fisheye.html">https://docs.opencv.org/3.4/db/d58/group__calib3d__fisheye.html</a></p>
<blockquote>
<div><p>The mapping from dewarper configuration parameters to OpenCV parameters is:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>“focal-length” =&gt; fx,fy
“src-x0” =&gt; cx
“src-y0” =&gt; cy
</pre></div>
</div>
<p>Distortion coefficients:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>For perspective input : (k0,k1,k2,k3,k4) =&gt; (k1,k2,k3,p1,p2)
For fisheye input : (k0,k1,k2,k3) =&gt; (k1,k2,k3,k4)
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
</li>
<li><p>EXIF data:</p></li>
</ol>
<blockquote>
<div><p>The focal length (in pixels or pixels/radian) can also be acquired from the image EXIF data, by multiplying the focal length in millimeters by the pixel density in pixels per millimeter.</p>
<p>The relevant EXIF tags are as follows:</p>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>FocalLength (37386)</p></li>
<li><p>FocalPlaneXResolution (41486)</p></li>
<li><p>FocalPlaneYResolution (41487)</p></li>
<li><p>FocalPlaneResolutionUnit (41488)</p></li>
</ol>
</div></blockquote>
</div></blockquote>
</section>
<section id="how-to-minimize-fps-jitter-with-ds-application-while-using-rtsp-camera-streams">
<h3>How to minimize FPS jitter with DS application while using RTSP Camera Streams?<a class="headerlink" href="DS_FAQ.html#how-to-minimize-fps-jitter-with-ds-application-while-using-rtsp-camera-streams" title="Link to this heading">#</a></h3>
<p>Below are the few DeepStream application tuning parameters that can help to lower the FPS Jitter.</p>
<ol class="arabic">
<li><p>Set the kernel receive max window size <code class="docutils literal notranslate"><span class="pre">rmem_max</span></code> to 52428800 or higher value:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ sudo sysctl -w net.core.rmem_max=52428800
$ sudo sysctl -p
</pre></div>
</div>
</div></blockquote>
<p>Also try to set DeepStream config file parameter under source section as <code class="docutils literal notranslate"><span class="pre">udp-buffer-size=2000000</span></code>.</p>
</li>
<li><p>Set Latency=1000 under the DeepStream Config File Source section.</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">rtsp-reconnect-interval-sec=30</span></code> or <code class="docutils literal notranslate"><span class="pre">60</span></code>, so that it can wait for sufficient time required for camera to reboot and start before. DS application attempts camera reconnection after waiting for this duration.</p></li>
<li><p>If deepstream-test5 application is used, then try running application with  –no-force-tcp parameter, which will use to RTP over UDP streaming instead of RTP over TCP streaming.</p></li>
<li><p>Set perf-measurement-interval-sec=5 so that better average FPS reported over 5 seconds duration where few cameras transmits the packets in bursts.</p></li>
</ol>
<blockquote>
<div><p>Sample DeepStream configuration file snippet with above tuning parameters as below:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[application]
enable-perf-measurement=1
mux-pool-size=8
perf-measurement-interval-sec=5

[source0]
enable=1
#Type - 1=CameraV4L2 2=URI 3=MultiURI
type=4
uri=rtsp://&lt;Camera URL&gt;
num-sources=1
gpu-id=0
nvbuf-memory-type=0
latency=1000
rtsp-reconnect-interval-sec=30
udp-buffer-size=2000000
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
</section>
<section id="why-am-i-getting-importerror-no-module-named-google-protobuf-internal-when-running-convert-to-uff-py-on-jetson-agx-orin">
<h3>Why am I getting “ImportError: No module named google.protobuf.internal when running convert_to_uff.py on Jetson AGX Orin”?<a class="headerlink" href="DS_FAQ.html#why-am-i-getting-importerror-no-module-named-google-protobuf-internal-when-running-convert-to-uff-py-on-jetson-agx-orin" title="Link to this heading">#</a></h3>
<p>If you set up Tensorflow using <a class="reference external" href="https://elinux.org/Jetson_Zoo#TensorFlow">https://elinux.org/Jetson_Zoo#TensorFlow</a>, please use Python 3 for running <code class="docutils literal notranslate"><span class="pre">convert_to_uff.py</span></code>:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">python3</span> <span class="pre">/usr/lib/python3.6/dist-packages/uff/bin/convert_to_uff.py</span></code></p>
</div></blockquote>
</section>
<section id="does-deepstream-support-10-bit-video-streams">
<h3>Does DeepStream Support 10 Bit Video streams?<a class="headerlink" href="DS_FAQ.html#does-deepstream-support-10-bit-video-streams" title="Link to this heading">#</a></h3>
<p>Decoder supports 10-bit as well as 12bit 4:2:0 semiplanar decoding (P010_10LE/I420_12LE), although most of the components work on 8-bit input.
10 and 12bit 4:4:4 planar decoding (Y444_10LE/Y444_12LE) is also supported, but limited to x86/dGPU platforms.
It is suggested to use <code class="docutils literal notranslate"><span class="pre">nvvideoconvert</span></code> to transform stream from 10-bit to 8-bit, and then add the relevant components, if they do not support 10/12 bit streams.
Sometimes, RTSP output from DeepStream is not observed remotely
Try running following pipeline to see if there is issue in network. With this you’ll be able to see output.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">gst-launch-1.0</span> <span class="pre">uridecodebin</span> <span class="pre">uri=rtsp://&lt;rtsp</span> <span class="pre">link&gt;</span> <span class="pre">!</span> <span class="pre">nveglglessink</span> <span class="pre">sync=0</span></code> on remote machine.</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>For 12bit NV12 format, the output caps shows I420_12LE. But the data is handled as semiplanar 12 bit data. This is a workaround since gstreamer v1.16 does not have the particular caps for 12 bit NV12. Downstream component should process it considering as 12bit NV12 only.</p></li>
</ul>
</div>
</section>
<section id="what-is-the-difference-between-batch-size-of-nvstreammux-and-nvinfer-what-are-the-recommended-values-for-nvstreammux-batch-size">
<h3>What is the difference between batch-size of nvstreammux and nvinfer? What are the recommended values for <code class="docutils literal notranslate"><span class="pre">nvstreammux</span></code> batch-size?<a class="headerlink" href="DS_FAQ.html#what-is-the-difference-between-batch-size-of-nvstreammux-and-nvinfer-what-are-the-recommended-values-for-nvstreammux-batch-size" title="Link to this heading">#</a></h3>
<p>nvstreammux’s batch-size is the number of buffers(frames) it will batch together in one muxed buffer. Nvinfer’s batch-size is the number of frame(primary-mode)/objects(secondary-mode) it will infer together.
We recommend that the nvstreammux’s batch-size be set to either number of sources linked to it or the primary nvinfer’s batch-size.</p>
</section>
<section id="how-do-i-configure-the-pipeline-to-get-ntp-timestamps">
<h3>How do I configure the pipeline to get NTP timestamps?<a class="headerlink" href="DS_FAQ.html#how-do-i-configure-the-pipeline-to-get-ntp-timestamps" title="Link to this heading">#</a></h3>
<p>To get NTP timestamps, set <code class="docutils literal notranslate"><span class="pre">attach-sys-ts</span></code> property to <code class="docutils literal notranslate"><span class="pre">FALSE</span></code> on nvstreammux component.</p>
</section>
<section id="why-is-the-ntp-timestamp-value-0">
<h3>Why is the NTP timestamp value 0?<a class="headerlink" href="DS_FAQ.html#why-is-the-ntp-timestamp-value-0" title="Link to this heading">#</a></h3>
<p>NTP timestamp 0 suggests that you are not receiving NTP timestamp from RTCP sender report. You can verify this using a tool like Wireshark.</p>
</section>
<section id="why-do-i-see-confidence-value-as-0-1">
<h3>Why do I see confidence value as -0.1.?<a class="headerlink" href="DS_FAQ.html#why-do-i-see-confidence-value-as-0-1" title="Link to this heading">#</a></h3>
<p>If “Group Rectangles” mode of clustering is chosen then confidence value is set to <code class="docutils literal notranslate"><span class="pre">-0.1</span></code> because the algorithm does not preserve confidence value.
Also, for the objects being tracked by the tracker but not detected by the inference component, confidence value is set to <code class="docutils literal notranslate"><span class="pre">-0.1</span></code>.</p>
</section>
<section id="how-to-use-the-oss-version-of-the-tensorrt-plugins-in-deepstream">
<h3>How to use the OSS version of the TensorRT plugins in DeepStream?<a class="headerlink" href="DS_FAQ.html#how-to-use-the-oss-version-of-the-tensorrt-plugins-in-deepstream" title="Link to this heading">#</a></h3>
<p>If TensorRT OSS plugins library is not already available, please follow instructions from <a class="github reference external" href="https://github.com/NVIDIA/TensorRT">NVIDIA/TensorRT</a> to build the library. To use the library in DeepStream, <cite>export LD_PRELOAD=/path/to/oss/libnvinfer_plugin.so</cite> before running any DeepStream app.</p>
</section>
<section id="why-do-i-see-the-below-error-while-processing-h265-rtsp-stream">
<h3>Why do I see the below Error while processing H265 RTSP stream?<a class="headerlink" href="DS_FAQ.html#why-do-i-see-the-below-error-while-processing-h265-rtsp-stream" title="Link to this heading">#</a></h3>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="nl">Error</span><span class="p">:</span><span class="w"> </span><span class="n">gstrtph265depay</span><span class="p">.</span><span class="n">c</span><span class="o">:</span><span class="mi">1196</span><span class="o">:</span><span class="n">gst_rtp_h265_finish_fragmentation_unit</span><span class="o">:</span><span class="w"> </span><span class="n">assertion</span><span class="w"> </span><span class="n">failed</span><span class="o">:</span><span class="p">(</span><span class="n">outsize</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>This issue is observed from <code class="docutils literal notranslate"><span class="pre">h265depay</span></code> gstreamer plugin component when size of <code class="docutils literal notranslate"><span class="pre">rtp</span> <span class="pre">payload</span></code> is less than <code class="docutils literal notranslate"><span class="pre">4</span></code>. The component throws assertion.
This invalid packet size could be because of packet corruption. To overcome this issue, you should ignore assertion and handle such errors.
Required modification in the code is present at <a class="reference external" href="https://forums.developer.nvidia.com/t/deepstream-sdk-faq/80236">https://forums.developer.nvidia.com/t/deepstream-sdk-faq/80236</a>. You’ll need to compile the code and place the lib at the appropriate location.</p>
</section>
<section id="why-do-i-observe-a-lot-of-buffers-are-being-dropped-when-running-live-camera-streams-even-for-few-or-single-stream-also-output-looks-jittery">
<h3>Why do I observe: A lot of buffers are being dropped. When running live camera streams even for few or single stream, also output looks jittery?<a class="headerlink" href="DS_FAQ.html#why-do-i-observe-a-lot-of-buffers-are-being-dropped-when-running-live-camera-streams-even-for-few-or-single-stream-also-output-looks-jittery" title="Link to this heading">#</a></h3>
<p>For live streams, nvstreammux element <code class="docutils literal notranslate"><span class="pre">live-source</span></code> property should be set as <code class="docutils literal notranslate"><span class="pre">1</span></code>. Also, <code class="docutils literal notranslate"><span class="pre">sink/renderer</span></code> element’s <code class="docutils literal notranslate"><span class="pre">sync</span></code> and <code class="docutils literal notranslate"><span class="pre">qos</span></code> property should be set as <code class="docutils literal notranslate"><span class="pre">0</span></code> or <code class="docutils literal notranslate"><span class="pre">FALSE</span></code>.</p>
</section>
<section id="why-does-the-rtsp-source-used-in-gst-launch-pipeline-through-uridecodebin-show-blank-screen-followed-by-the-error-warning-from-element-gstpipeline-pipeline0-gstnvstreammux-m-no-sources-found-at-the-input-of-muxer-waiting-for-sources">
<h3>Why does the RTSP source used in gst-launch pipeline through uridecodebin show blank screen followed by the error -  <code class="docutils literal notranslate"><span class="pre">WARNING:</span> <span class="pre">from</span> <span class="pre">element</span> <span class="pre">/GstPipeline:pipeline0/GstNvStreamMux:m:</span> <span class="pre">No</span> <span class="pre">Sources</span> <span class="pre">found</span> <span class="pre">at</span> <span class="pre">the</span> <span class="pre">input</span> <span class="pre">of</span> <span class="pre">muxer.</span> <span class="pre">Waiting</span> <span class="pre">for</span> <span class="pre">sources</span></code>?<a class="headerlink" href="DS_FAQ.html#why-does-the-rtsp-source-used-in-gst-launch-pipeline-through-uridecodebin-show-blank-screen-followed-by-the-error-warning-from-element-gstpipeline-pipeline0-gstnvstreammux-m-no-sources-found-at-the-input-of-muxer-waiting-for-sources" title="Link to this heading">#</a></h3>
<p>At times the requested muxer pad gets deleted before linking happens, as streams might contain both video and audio. If queue element is added between nvstreammux and the <code class="docutils literal notranslate"><span class="pre">uridecodebin</span></code> then the above pipeline will work. As <code class="docutils literal notranslate"><span class="pre">uridecodebin</span></code> will link to queue pad and not nvstreammux pad. This problem is not observed programmatically as the linking takes place new pad callback of decoder on video stream.</p>
</section>
<section id="what-if-i-do-not-get-expected-30-fps-from-camera-using-v4l2src-plugin-in-pipeline-but-instead-get-15-fps-or-less-than-30-fps">
<h3>What if I do not get expected 30 FPS from camera using v4l2src plugin in pipeline but instead get 15 FPS or less than 30 FPS?<a class="headerlink" href="DS_FAQ.html#what-if-i-do-not-get-expected-30-fps-from-camera-using-v4l2src-plugin-in-pipeline-but-instead-get-15-fps-or-less-than-30-fps" title="Link to this heading">#</a></h3>
<p>This could be possible due to exposure or lighting conditions around camera however this can be fixed by changing camera settings through below reference commands to change the exposure settings.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">v4l2</span><span class="o">-</span><span class="n">ctl</span><span class="w"> </span><span class="o">-</span><span class="n">d</span><span class="w"> </span><span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">video0</span><span class="w"> </span><span class="o">--</span><span class="n">list</span><span class="o">-</span><span class="n">ctrls</span>
<span class="n">v4l2</span><span class="o">-</span><span class="n">ctl</span><span class="w"> </span><span class="o">--</span><span class="n">set</span><span class="o">-</span><span class="n">ctrl</span><span class="o">=</span><span class="n">exposure_auto</span><span class="o">=</span><span class="mi">1</span>
<span class="n">v4l2</span><span class="o">-</span><span class="n">ctl</span><span class="w"> </span><span class="o">--</span><span class="n">set</span><span class="o">-</span><span class="n">ctrl</span><span class="o">=</span><span class="n">exposure_absolute</span><span class="o">=</span><span class="mi">300</span>
</pre></div>
</div>
</section>
<section id="on-jetson-platform-i-get-same-output-when-multiple-jpeg-images-are-fed-to-nvv4l2decoder-using-multifilesrc-plugin-why-is-that">
<h3>On Jetson platform, I get same output when multiple Jpeg images are fed to nvv4l2decoder using multifilesrc plugin. Why is that?<a class="headerlink" href="DS_FAQ.html#on-jetson-platform-i-get-same-output-when-multiple-jpeg-images-are-fed-to-nvv4l2decoder-using-multifilesrc-plugin-why-is-that" title="Link to this heading">#</a></h3>
<p>For example</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">multifilesrc</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">frame</span><span class="o">%</span><span class="n">d</span><span class="p">.</span><span class="n">jpeg</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">jpegparse</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvv4l2decoder</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nv3dsink</span>
</pre></div>
</div>
<p>On Jetson platforms nvv4l2decoder needs to set property <code class="docutils literal notranslate"><span class="pre">mjpeg=1</span></code> in order to work with <code class="docutils literal notranslate"><span class="pre">multifilesrc</span></code>.</p>
</section>
<section id="on-jetson-platform-i-observe-lower-fps-output-when-screen-goes-idle">
<h3>On Jetson platform, I observe lower FPS output when screen goes idle.<a class="headerlink" href="DS_FAQ.html#on-jetson-platform-i-observe-lower-fps-output-when-screen-goes-idle" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>If you are running Ubuntu UI, then run below commands:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ export DISPLAY=:0
$ gsettings set org.gnome.desktop.session idle-delay 0
$ gsettings set org.gnome.desktop.lockdown disable-lock-screen &#39;true&#39;
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
<p>This will prevent screen lock and display going off. You should not see low fps when this is done</p>
<ol class="arabic" start="2">
<li><p>If you are running this on a bare X server and  have an ubuntu UI up then kill it with:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">service</span><span class="w"> </span><span class="n">gdm</span><span class="w"> </span><span class="n">stop</span>
<span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">pkill</span><span class="w"> </span><span class="mi">-15</span><span class="w"> </span><span class="n">Xorg</span>
</pre></div>
</div>
</li>
</ol>
<p>Then start a bare X server with either of the 2 commands below:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">xinit</span><span class="w"> </span><span class="o">&amp;</span>
<span class="n">$</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">-</span><span class="n">noreset</span><span class="w"> </span><span class="o">&amp;</span>
</pre></div>
</div>
<p>Wait for few seconds and then run:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="k">export</span><span class="w"> </span><span class="n">DISPLAY</span><span class="o">=:</span><span class="mi">0</span>
<span class="n">$</span><span class="w"> </span><span class="n">xset</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="n">off</span><span class="p">;</span><span class="w"> </span><span class="n">xset</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="n">noblank</span><span class="p">;</span><span class="w"> </span><span class="n">xset</span><span class="w"> </span><span class="o">-</span><span class="n">dpms</span>
</pre></div>
</div>
</section>
<section id="how-do-i-obtain-individual-sources-after-batched-inferencing-processing-what-are-the-sample-pipelines-for-nvstreamdemux">
<h3>How do I obtain individual sources after batched inferencing/processing? What are the sample pipelines for nvstreamdemux?<a class="headerlink" href="DS_FAQ.html#how-do-i-obtain-individual-sources-after-batched-inferencing-processing-what-are-the-sample-pipelines-for-nvstreamdemux" title="Link to this heading">#</a></h3>
<p>Some sample nvstreamdemux pipelines:</p>
<blockquote>
<div><blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 filesrc location = sample_1080p_h264.mp4 ! decodebin ! m.sink_0 \
filesrc location = sample_1080p_h264.mp4 ! decodebin ! m.sink_1 \
filesrc location = sample_1080p_h264.mp4 ! decodebin ! m.sink_2 \
filesrc location = sample_1080p_h264.mp4 ! decodebin ! m.sink_3 \
nvstreammux name=m width=1920 height=1080 batch-size=4 batched-push-timeout=40000 ! \
queue ! nvinfer config-file-path=&lt;config&gt; batch-size=4 ! \
queue ! nvtracker ll-lib-file=&lt;lib-file&gt; ! \
nvstreamdemux name=d \
d.src_0 ! queue ! nvvideoconvert ! nvdsosd ! nveglglessink \
d.src_1 ! queue ! nvvideoconvert ! nvdsosd ! nveglglessink \
d.src_2 ! queue ! nvvideoconvert ! nvdsosd ! nveglglessink \
d.src_3 ! queue ! nvvideoconvert ! nvdsosd ! nveglglessink
</pre></div>
</div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Queue element should be inserted after every <code class="docutils literal notranslate"><span class="pre">nvstreamdemux</span> <span class="pre">src</span></code> pad.</p>
</div>
</div></blockquote>
<p>It is not required to demux all sources / create all nvstreamdemux src pad. Also, the downstream pipeline for every source may be different. Sample pipeline:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">gst</span><span class="o">-</span><span class="n">launch</span><span class="mf">-1.0</span><span class="w"> </span><span class="n">filesrc</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sample_1080p_h264</span><span class="p">.</span><span class="n">mp4</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">decodebin</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">sink_0</span><span class="w"> </span>\
<span class="n">filesrc</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sample_1080p_h264</span><span class="p">.</span><span class="n">mp4</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">decodebin</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">sink_1</span><span class="w"> </span>\
<span class="n">filesrc</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sample_1080p_h264</span><span class="p">.</span><span class="n">mp4</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">decodebin</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">sink_2</span><span class="w"> </span>\
<span class="n">filesrc</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sample_1080p_h264</span><span class="p">.</span><span class="n">mp4</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">decodebin</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">m</span><span class="p">.</span><span class="n">sink_3</span><span class="w"> </span>\
<span class="n">nvstreammux</span><span class="w"> </span><span class="n">name</span><span class="o">=</span><span class="n">m</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="mi">1920</span><span class="w"> </span><span class="n">height</span><span class="o">=</span><span class="mi">1080</span><span class="w"> </span><span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="w"> </span><span class="n">batched</span><span class="o">-</span><span class="n">push</span><span class="o">-</span><span class="n">timeout</span><span class="o">=</span><span class="mi">40000</span><span class="w"> </span><span class="o">!</span><span class="w"> </span>\
<span class="n">queue</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvinfer</span><span class="w"> </span><span class="n">config</span><span class="o">-</span><span class="n">file</span><span class="o">-</span><span class="n">path</span><span class="o">=&lt;</span><span class="n">config</span><span class="o">&gt;</span><span class="w"> </span><span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="w"> </span><span class="o">!</span><span class="w"> </span>\
<span class="n">queue</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvtracker</span><span class="w"> </span><span class="n">ll</span><span class="o">-</span><span class="n">lib</span><span class="o">-</span><span class="n">file</span><span class="o">=&lt;</span><span class="n">lib</span><span class="o">-</span><span class="n">file</span><span class="o">&gt;</span><span class="w"> </span><span class="o">!</span><span class="w"> </span>\
<span class="n">vstreamdemux</span><span class="w"> </span><span class="n">name</span><span class="o">=</span><span class="n">d</span><span class="w"> </span>\
<span class="n">d</span><span class="p">.</span><span class="n">src_1</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">queue</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvvideoconvert</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvdsosd</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvvideoconvert</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvv4l2h264enc</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">h264parse</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">qtmux</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">filesink</span><span class="w"> </span><span class="n">location</span><span class="o">=</span><span class="n">out</span><span class="p">.</span><span class="n">mp4</span><span class="w"> </span>\
<span class="n">d</span><span class="p">.</span><span class="n">src_2</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">queue</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvvideoconvert</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nvdsosd</span><span class="w"> </span><span class="o">!</span><span class="w"> </span><span class="n">nveglglessink</span>
</pre></div>
</div>
</section>
<section id="why-do-i-encounter-such-error-while-running-deepstream-pipeline-memory-type-configured-and-i-p-buffer-mismatch-ip-surf-0-muxer-3">
<h3>Why do I encounter such error while running Deepstream pipeline memory type configured and i/p buffer mismatch ip_surf 0 muxer 3?<a class="headerlink" href="DS_FAQ.html#why-do-i-encounter-such-error-while-running-deepstream-pipeline-memory-type-configured-and-i-p-buffer-mismatch-ip-surf-0-muxer-3" title="Link to this heading">#</a></h3>
<p>This error is observed on dGPU, when <code class="docutils literal notranslate"><span class="pre">NvStreamMux</span></code> is configured for memory type 3, i.e., <code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_CUDA_UNIFIED</span></code> and the input surface to the <code class="docutils literal notranslate"><span class="pre">nvstreammux</span></code> has the memory type 0 i.e., <code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_CUDA_DEFAULT</span></code> (Cuda device for dGPU). The resolution of input surface is same as <code class="docutils literal notranslate"><span class="pre">nvstreammux</span></code> configured resolution, in such scenario the <code class="docutils literal notranslate"><span class="pre">nvstreammux</span> <span class="pre">``tries</span> <span class="pre">to</span> <span class="pre">send</span> <span class="pre">the</span> <span class="pre">original</span> <span class="pre">buffer</span> <span class="pre">on</span> <span class="pre">its</span> <span class="pre">sinkpad</span> <span class="pre">to</span> <span class="pre">downstream</span> <span class="pre">muxed</span> <span class="pre">with</span> <span class="pre">buffers</span> <span class="pre">from</span> <span class="pre">other</span> <span class="pre">sources,</span> <span class="pre">but</span> <span class="pre">due</span> <span class="pre">to</span> <span class="pre">different</span> <span class="pre">configured</span> <span class="pre">memory</span> <span class="pre">type</span> <span class="pre">of</span> <span class="pre">``nvstreammux</span></code> it can’t do the same. To get around this ensure that all the sources connected to <code class="docutils literal notranslate"><span class="pre">nvstreammux</span></code> are generating same type of memory and configure the <code class="docutils literal notranslate"><span class="pre">nvstreammux</span></code> memory to the same type. Alternatively, if there is scaling in <code class="docutils literal notranslate"><span class="pre">nvstreammux</span></code> this error won’t be encountered.</p>
</section>
<section id="how-does-secondary-gie-crop-and-resize-objects">
<h3>How does secondary GIE crop and resize objects?<a class="headerlink" href="DS_FAQ.html#how-does-secondary-gie-crop-and-resize-objects" title="Link to this heading">#</a></h3>
<p>SGIE will crop the object from <code class="docutils literal notranslate"><span class="pre">NvStreamMux</span></code> buffer using the object’s bbox detected by the Primary GIE. The crop is then scaled/converted to the network resolution/color format. For example, if the <code class="docutils literal notranslate"><span class="pre">NvStreamMux</span></code> resolution is 1920x1080, SGIE will crop using object bbox co-ordinates (e.g. x=1000, y=20, w=400, y=500) from the 1920x1080 image and then scale it to the SGIE network resolution (say 224x224). In practice, the object crop + scaling + color conversion happens in one go.</p>
</section>
<section id="how-to-save-frames-from-gstbuffer">
<h3>How to save frames from GstBuffer?<a class="headerlink" href="DS_FAQ.html#how-to-save-frames-from-gstbuffer" title="Link to this heading">#</a></h3>
<p>To save frames from gst buffer you need to Map gst buffer using <code class="docutils literal notranslate"><span class="pre">gst_buffer_map</span> <span class="pre">()</span></code> API.</p>
<p>Here is the pseudo code:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">GstMapInfo</span><span class="w"> </span><span class="n">in_map_info</span><span class="p">;</span>
<span class="n">NvBufSurface</span><span class="w"> </span><span class="o">*</span><span class="n">surface</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span>

<span class="n">memset</span><span class="w"> </span><span class="p">(</span><span class="o">&amp;</span><span class="n">in_map_info</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="w"> </span><span class="p">(</span><span class="n">in_map_info</span><span class="p">));</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">gst_buffer_map</span><span class="w"> </span><span class="p">(</span><span class="n">inbuf</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">in_map_info</span><span class="p">,</span><span class="w"> </span><span class="n">GST_MAP_READ</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="n">g_print</span><span class="w"> </span><span class="p">(</span><span class="s">&quot;Error: Failed to map gst buffer</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">surface</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">NvBufSurface</span><span class="w"> </span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="n">in_map_info</span><span class="p">.</span><span class="n">data</span><span class="p">;</span>
</pre></div>
</div>
<p>Now that you have access to <code class="docutils literal notranslate"><span class="pre">NvBufSurface</span></code> structure, you can access actual frame memory and save it. At the end you need to unmap gst buffer using <code class="docutils literal notranslate"><span class="pre">gst_buffer_unmap</span> <span class="pre">(inbuf,</span> <span class="pre">&amp;in_map_info)</span></code>
For more details, see <code class="docutils literal notranslate"><span class="pre">gst_dsexample_transform_ip()</span></code> in <code class="docutils literal notranslate"><span class="pre">gst-dsexample</span> <span class="pre">plugin</span></code> source code.</p>
</section>
<section id="what-are-different-memory-types-supported-on-jetson-and-dgpu">
<h3>What are different Memory types supported on Jetson and dGPU?<a class="headerlink" href="DS_FAQ.html#what-are-different-memory-types-supported-on-jetson-and-dgpu" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table" id="id3">
<caption><span class="caption-text">Memory types supported on Jetson and dGPU</span><a class="headerlink" href="DS_FAQ.html#id3" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Memory Type</p></th>
<th class="head"><p>Jetson</p></th>
<th class="head"><p>dGPU or X86_64</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_DEFAULT</span></code></p></td>
<td><p>Memory of type Surface Array which is 2D pitched allocated by default: Used by all hardware accelerator on the platform. Accessible by CPU using <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceMap</span></code> and <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceSyncForCpu</span></code> or <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceSyncForDevice</span></code> based on read write usage. GPU Access using <code class="docutils literal notranslate"><span class="pre">EGLImageCreate</span></code> and Map APIs</p></td>
<td><p>Memory of type Cuda Device is allocated by default, accessible only by GPU. User might need to have custom Cuda kernels to access or modify memory. Or <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceCopy</span></code> to copy content into CPU accessible memory</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_CUDA_PINNED</span></code></p></td>
<td><p>Page Locked Memory allocated using <code class="docutils literal notranslate"><span class="pre">cudaMallocHost</span> <span class="pre">()</span></code>, accessible by CPU and GPU</p></td>
<td><p>Page Locked Memory allocated using <code class="docutils literal notranslate"><span class="pre">cudaMallocHost</span> <span class="pre">()</span></code> accessible by CPU and GPU.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_CUDA_DEVICE</span></code></p></td>
<td><p>Memory of type Cuda Device is allocated, accessible only by GPU. User might need to have custom Cuda kernels to access or modify memory. <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceCopy</span></code> is not supported for Cuda memory on Jetson</p></td>
<td><p>Memory of type Cuda Device is allocated, accessible only by GPU. User might need to have custom Cuda kernels to access or modify memory. Or <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceCopy</span></code> to copy content into CPU accessible memory</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_CUDA_UNIFIED</span></code></p></td>
<td><p>Unsupported</p></td>
<td><p>Unified Virtual Memory allocated using <code class="docutils literal notranslate"><span class="pre">cudaMallocManaged</span> <span class="pre">()</span></code> accessible by CPU and multiple GPU</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_SURFACE_ARRAY</span></code></p></td>
<td><p>Memory of type Surface Array which is 2D pitched allocated by default; used by all hardware accelerator on the platform; accessible by CPU using <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceMap</span> <span class="pre">()</span></code> and  <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceSyncForCpu</span> <span class="pre">()</span></code> or <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceSyncForDevice</span> <span class="pre">()</span></code> based on usage read write usage.
GPU Access using <code class="docutils literal notranslate"><span class="pre">EGLImageCreate</span></code> and Map APIs</p></td>
<td><p>Unsupported</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_HANDLE</span></code></p></td>
<td><p>Used internally for Jetson</p></td>
<td><p>Unsupported</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_SYSTEM</span></code></p></td>
<td><p>Allocated using <code class="docutils literal notranslate"><span class="pre">malloc</span></code></p></td>
<td><p>Allocated using <code class="docutils literal notranslate"><span class="pre">malloc</span> <span class="pre">()</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="what-are-different-memory-transformations-supported-on-jetson-and-dgpu">
<h3>What are different Memory transformations supported on Jetson and dGPU?<a class="headerlink" href="DS_FAQ.html#what-are-different-memory-transformations-supported-on-jetson-and-dgpu" title="Link to this heading">#</a></h3>
<p><strong>dGPU:</strong> User can use <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceCopy()</span></code> to copy from one memory type to another. If transformation is required, nvvideoconvert plugin support <code class="docutils literal notranslate"><span class="pre">nvbuf-memory-type</span></code> property to allow different type of memory. <code class="docutils literal notranslate"><span class="pre">NvBufSurfTransform()</span></code> can also be used to do the transformation between various CUDA types of memories. CUDA to <code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_SYSTEM</span></code> transformation is not supported by <code class="docutils literal notranslate"><span class="pre">NvBufSurfTransform</span></code> directly, user can use <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceCopy()</span></code> to copy into CUDA memory and perform transformation on that memory.</p>
<p><strong>Jetson:</strong>  User can use <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceCopy()</span></code> to copy from one memory type to another, although CUDA memory copies are not supported directly. User can perform <code class="docutils literal notranslate"><span class="pre">NvBufSurfTransform()</span></code> for transformation from <code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_SURFACE_ARRAY/NVBUF_MEM_DEFAULT</span></code> to Cuda Memory, but user need to use GPU as compute device for doing the transformation, as VIC doesn’t support transformation to CUDA memory or <code class="docutils literal notranslate"><span class="pre">NVBUF_MEM_SYTEM</span></code>. See <code class="docutils literal notranslate"><span class="pre">NvBufSurfTransform</span></code> APIs for more information.</p>
</section>
<section id="why-does-my-image-look-distorted-if-i-wrap-my-cudamalloc-ed-memory-into-nvbufsurface-and-provide-to-nvbufsurftransform">
<h3>Why does my image look distorted if I wrap my cudaMalloc’ed memory into NvBufSurface and provide to NvBufSurfTransform?<a class="headerlink" href="DS_FAQ.html#why-does-my-image-look-distorted-if-i-wrap-my-cudamalloc-ed-memory-into-nvbufsurface-and-provide-to-nvbufsurftransform" title="Link to this heading">#</a></h3>
<p>If you are not using <code class="docutils literal notranslate"><span class="pre">NvBufSurfaceCreate</span></code> for allocation, ensure the pitch of the allocated memory is multiple of 32. Also ensure that the starting address of each plane of the input is 128-byte aligned.</p>
</section>
<section id="why-do-transformations-below-16x16-dimensions-fail-on-jetson">
<h3>Why do transformations below 16x16 dimensions fail on Jetson?<a class="headerlink" href="DS_FAQ.html#why-do-transformations-below-16x16-dimensions-fail-on-jetson" title="Link to this heading">#</a></h3>
<p>Jetson (VIC) has a hardware limitation which only allows transformations with minimum 16x16 dimensions therefore NvBufSurfTransform has the same limitations on this platform.</p>
</section>
<section id="how-to-find-out-the-maximum-number-of-streams-supported-on-given-platform">
<h3>How to find out the maximum number of streams supported on given platform?<a class="headerlink" href="DS_FAQ.html#how-to-find-out-the-maximum-number-of-streams-supported-on-given-platform" title="Link to this heading">#</a></h3>
<p>DeepStream can support as many streams as possible as long as application under run is not limited by memory/decode/compute capability.</p>
</section>
<section id="how-to-find-the-performance-bottleneck-in-deepstream">
<h3>How to find the performance bottleneck in DeepStream?<a class="headerlink" href="DS_FAQ.html#how-to-find-the-performance-bottleneck-in-deepstream" title="Link to this heading">#</a></h3>
<p><strong>On dGPU (X86)</strong>:
Run following command to check GPU, decoder, encoder, memory utilization:</p>
<p><code class="docutils literal notranslate"><span class="pre">$nvidia-smi</span> <span class="pre">dmon</span></code></p>
<p>You need to check the temperature and power usage as well.
Performance is limited by the temperature and power numbers beyond the allowed limits for a given platform.
To check if CPU is the bottleneck, run <code class="docutils literal notranslate"><span class="pre">htop</span></code> command on the console to see if any of the CPU core utilizations is ~100%</p>
<p><strong>On Jetson</strong>:
Run following command to check CPU, GPU, and memory utilization:</p>
<p><code class="docutils literal notranslate"><span class="pre">sudo./tegrastats</span></code></p>
<p>For more details, see the link: <a class="reference external" href="https://docs.nvidia.com/jetson/l4t/index.html#page/Tegra%2520Linux%2520Driver%2520Package%2520Development%2520Guide%2FAppendixTegraStats.html%23wwpID0E0SB0HA">https://docs.nvidia.com/jetson/l4t/index.html#page/Tegra%2520Linux%2520Driver%2520Package%2520Development%2520Guide%2FAppendixTegraStats.html%23wwpID0E0SB0HA</a>.</p>
</section>
<section id="how-to-fix-cannot-allocate-memory-in-static-tls-block-error">
<h3>How to fix “cannot allocate memory in static TLS block” error?<a class="headerlink" href="DS_FAQ.html#how-to-fix-cannot-allocate-memory-in-static-tls-block-error" title="Link to this heading">#</a></h3>
<p>On Jetson, sometimes the following error might occur:</p>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(gst-plugin-scanner:21845): GStreamer-WARNING **: 04:34:02.887: Failed to load plugin &#39;/usr/lib/aarch64-linux-gnu/gstreamer-1.0/libgstlibav.so&#39;: /usr/lib/aarch64-linux-gnu/libgomp.so.1: cannot allocate memory in static TLS block
</pre></div>
</div>
</div></blockquote>
<p>This error can be fixed by running the following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1
</pre></div>
</div>
</section>
<section id="how-to-measure-pipeline-latency-if-pipeline-contains-open-source-components">
<h3>How to measure pipeline latency  if pipeline contains open source components<a class="headerlink" href="DS_FAQ.html#how-to-measure-pipeline-latency-if-pipeline-contains-open-source-components" title="Link to this heading">#</a></h3>
<p>If the open source component cannot be modified to measure latency using APIs mentioned in <a class="reference external" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/sdk-api/group__ee__nvlatency__group.html">https://docs.nvidia.com/metropolis/deepstream/dev-guide/sdk-api/group__ee__nvlatency__group.html</a> , then following approach can be used.</p>
<p>You can insert a probe on sink pad of the decoder, measure the time at which input buffer arrives. Insert another probe on sink pad of the sink component and measure the time at which output buffer arrives corresponding to the input buffer . Time difference between these two will give you the latency of the buffer.</p>
</section>
<section id="migration-to-newer-gstreamer-version">
<h3>Migration to  newer gstreamer version<a class="headerlink" href="DS_FAQ.html#migration-to-newer-gstreamer-version" title="Link to this heading">#</a></h3>
<p>In order to migrate to newer gstreamer version (e.g. gstreamer-1.22.6) follow below steps:</p>
<ol class="arabic">
<li><p>Prerequisites: Install below packages:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">apt</span><span class="o">-</span><span class="n">get</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">python3</span><span class="o">-</span><span class="n">pip</span>
<span class="n">pip3</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">meson</span>
<span class="n">pip3</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">ninja</span>
<span class="n">apt</span><span class="o">-</span><span class="n">get</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">libmount</span><span class="o">-</span><span class="n">dev</span>
<span class="n">apt</span><span class="o">-</span><span class="n">get</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">flex</span>
<span class="n">apt</span><span class="o">-</span><span class="n">get</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">flex</span><span class="w"> </span><span class="n">bison</span>
<span class="n">apt</span><span class="o">-</span><span class="n">get</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">libglib2</span><span class="mf">.0</span><span class="o">-</span><span class="n">dev</span>
</pre></div>
</div>
</li>
<li><p>Compilation and installation steps</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#Remove older version</span>
<span class="n">apt</span><span class="o">-</span><span class="n">get</span><span class="w"> </span><span class="n">remove</span><span class="w"> </span><span class="o">*</span><span class="n">gstreamer</span><span class="o">*</span>
<span class="n">mkdir</span><span class="w"> </span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">gst</span><span class="o">-&lt;</span><span class="n">gst</span><span class="o">-</span><span class="n">version</span><span class="o">&gt;</span>
<span class="cp">#e.g. mkdir /tmp/gst-1.22.6</span>

<span class="n">cd</span><span class="w"> </span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">gst</span><span class="o">-&lt;</span><span class="n">gst</span><span class="o">-</span><span class="n">version</span><span class="o">&gt;</span>
<span class="cp">#e.g. cd /tmp/gst-1.22.6</span>
<span class="cp">#Clone the repository</span>
<span class="n">git</span><span class="w"> </span><span class="n">clone</span><span class="w"> </span><span class="n">https</span><span class="o">:</span><span class="c1">//gitlab.freedesktop.org/gstreamer/gstreamer.git</span>
<span class="n">cd</span><span class="w"> </span><span class="n">gstreamer</span>

<span class="cp">#Switch to gst-version</span>
<span class="n">git</span><span class="w"> </span><span class="n">checkout</span><span class="w"> </span><span class="o">&lt;</span><span class="n">gst</span><span class="o">-</span><span class="n">version</span><span class="o">-</span><span class="n">branch</span><span class="o">&gt;</span>
<span class="cp">#e.g. git checkout 1.22.6</span>

<span class="n">meson</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="o">--</span><span class="n">prefix</span><span class="o">=/</span><span class="n">usr</span>
<span class="n">ninja</span><span class="w"> </span><span class="o">-</span><span class="n">C</span><span class="w"> </span><span class="n">build</span><span class="o">/</span>
<span class="n">cd</span><span class="w"> </span><span class="n">build</span><span class="o">/</span>
<span class="n">ninja</span><span class="w"> </span><span class="n">install</span>
<span class="n">cd</span>

<span class="n">rm</span><span class="w"> </span><span class="o">-</span><span class="n">rf</span><span class="w"> </span><span class="o">&lt;</span><span class="n">directory</span><span class="w"> </span><span class="n">created</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="mi">2</span><span class="o">&gt;</span>
<span class="cp">#e.g. rm -rf /tmp/gst-1.22.6</span>
</pre></div>
</div>
</li>
<li><p>Check and confirm the newly installed gstreamer version:</p></li>
</ol>
<blockquote>
<div><div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">gst</span><span class="o">-</span><span class="n">inspect</span><span class="mf">-1.0</span><span class="w"> </span><span class="o">--</span><span class="n">version</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Run commands with sudo when prompted for permissions.</p>
</div>
</div></blockquote>
</section>
<section id="why-do-i-get-the-warning-warning-from-src-elem-no-decoder-available-for-type-audio-mpeg-mpegversion-int-4-when-i-run-deepstream-app-sample-configuration-on-ds-docker-containers">
<h3>Why do I get the WARNING <code class="docutils literal notranslate"><span class="pre">WARNING</span> <span class="pre">from</span> <span class="pre">src_elem:</span> <span class="pre">No</span> <span class="pre">decoder</span> <span class="pre">available</span> <span class="pre">for</span> <span class="pre">type</span> <span class="pre">'audio/mpeg,</span> <span class="pre">mpegversion=(int)4’</span></code> when I run deepstream-app sample configuration on DS docker containers?<a class="headerlink" href="DS_FAQ.html#why-do-i-get-the-warning-warning-from-src-elem-no-decoder-available-for-type-audio-mpeg-mpegversion-int-4-when-i-run-deepstream-app-sample-configuration-on-ds-docker-containers" title="Link to this heading">#</a></h3>
<p>With DS 6.2+, DeepStream docker containers do not package libraries necessary for certain multimedia operations like audio data parsing, CPU decode, and CPU encode.
This change could affect processing certain video streams/files like mp4 that include audio track.</p>
<p>Please run the below script inside the docker images to install additional packages that might be necessary to use all of the DeepStreamSDK features:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">user_additional_install</span><span class="p">.</span><span class="n">sh</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more FAQs and troubleshooting information, see <a class="reference external" href="https://forums.developer.nvidia.com/t/deepstream-sdk-faq/">https://forums.developer.nvidia.com/t/deepstream-sdk-faq/</a>.</p>
</div>
</section>
<section id="why-do-i-get-the-warning-failed-to-load-plugin-usr-lib-aarch64-linux-gnu-gstreamer-1-0-libgstlibav-so-libavfilter-so-7-cannot-open-shared-object-file-no-such-file-or-directory-when-using-deepstream-app-version-all">
<h3>Why do I get the WARNING: <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">load</span> <span class="pre">plugin</span> <span class="pre">'/usr/lib/aarch64-linux-gnu/gstreamer-1.0/libgstlibav.so':</span> <span class="pre">libavfilter.so.7:</span></code> <code class="docutils literal notranslate"><span class="pre">cannot</span> <span class="pre">open</span> <span class="pre">shared</span> <span class="pre">object</span> <span class="pre">file:</span> <span class="pre">No</span> <span class="pre">such</span> <span class="pre">file</span> <span class="pre">or</span> <span class="pre">directory</span></code> when using deepstream-app –version-all?<a class="headerlink" href="DS_FAQ.html#why-do-i-get-the-warning-failed-to-load-plugin-usr-lib-aarch64-linux-gnu-gstreamer-1-0-libgstlibav-so-libavfilter-so-7-cannot-open-shared-object-file-no-such-file-or-directory-when-using-deepstream-app-version-all" title="Link to this heading">#</a></h3>
<p>With DS 6.2+, DeepStream containers do not package gstreamer1.0-libav, library necessary for certain multimedia operations.</p>
<p>Please run the below script inside the docker images to install additional packages that might be necessary to use all of the DeepStreamSDK features:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">user_additional_install</span><span class="p">.</span><span class="n">sh</span>
</pre></div>
</div>
</section>
<section id="why-do-i-get-the-warning-message-libegl-warning-dri3-screen-seems-not-dri3-capable-and-libegl-warning-dri2-failed-to-authenticate-when-running-deepstream-pipeline-with-nveglglesink">
<h3>Why do I get the warning message: <code class="docutils literal notranslate"><span class="pre">libEGL</span> <span class="pre">warning:</span> <span class="pre">DRI3:</span> <span class="pre">Screen</span> <span class="pre">seems</span> <span class="pre">not</span> <span class="pre">DRI3</span> <span class="pre">capable</span></code> and <code class="docutils literal notranslate"><span class="pre">libEGL</span> <span class="pre">warning:</span> <span class="pre">DRI2:</span> <span class="pre">failed</span> <span class="pre">to</span> <span class="pre">authenticate</span></code> when running deepstream pipeline with nveglglesink?<a class="headerlink" href="DS_FAQ.html#why-do-i-get-the-warning-message-libegl-warning-dri3-screen-seems-not-dri3-capable-and-libegl-warning-dri2-failed-to-authenticate-when-running-deepstream-pipeline-with-nveglglesink" title="Link to this heading">#</a></h3>
<p>This warning is seen on T4 dGPU when the pipeline is run with nveglglessink. This warning is harmless and can be ignored.</p>
</section>
<section id="how-to-run-higher-number-of-streams-200-on-hopper-ampere-and-ada">
<h3>How to run higher number of streams (200+) on Hopper,Ampere and ADA?<a class="headerlink" href="DS_FAQ.html#how-to-run-higher-number-of-streams-200-on-hopper-ampere-and-ada" title="Link to this heading">#</a></h3>
<p>To run higher number of streams, follow below steps:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$sudo</span><span class="w"> </span><span class="n">service</span><span class="w"> </span><span class="n">display</span><span class="o">-</span><span class="n">manager</span><span class="w"> </span><span class="n">stop</span>
<span class="cp">#Make sure no process is running on GPU i.e. Xorg or trition server etc</span>
<span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">pkill</span><span class="w"> </span><span class="mi">-9</span><span class="w"> </span><span class="n">Xorg</span>
<span class="cp">#Remove kernel modules</span>
<span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">rmmod</span><span class="w"> </span><span class="n">nvidia_drm</span><span class="w"> </span><span class="n">nvidia_modeset</span><span class="w"> </span><span class="n">nvidia</span>
<span class="cp">#Load Modules with Regkeys</span>
<span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">modprobe</span><span class="w"> </span><span class="n">nvidia</span><span class="w"> </span><span class="n">NVreg_RegistryDwords</span><span class="o">=</span><span class="s">&quot;RMDebugOverridePerRunlistChannelRam = 1;RMIncreaseRsvdMemorySizeMB = 1024;RMDisableChIdIsolation = 0x1;RmGspFirmwareHeapSizeMB = 256&quot;</span>
<span class="n">$</span><span class="w"> </span><span class="n">sudo</span><span class="w"> </span><span class="n">service</span><span class="w"> </span><span class="n">display</span><span class="o">-</span><span class="n">manager</span><span class="w"> </span><span class="n">start</span>
</pre></div>
</div>
<p>Note that these steps are applicable only in testing environment and not for production. This is a known issue with the driver.</p>
</section>
<section id="why-do-i-get-the-message-error-this-container-was-built-for-nvidia-driver-release-560-28-or-later-but-version-540-4-0-was-detected-and-compatibility-mode-is-unavailable">
<h3>Why do I get the message: <code class="docutils literal notranslate"><span class="pre">ERROR:</span> <span class="pre">This</span> <span class="pre">container</span> <span class="pre">was</span> <span class="pre">built</span> <span class="pre">for</span> <span class="pre">NVIDIA</span> <span class="pre">Driver</span> <span class="pre">Release</span> <span class="pre">560.28</span> <span class="pre">or</span> <span class="pre">later,</span> <span class="pre">but</span> <span class="pre">version</span> <span class="pre">540.4.0</span> <span class="pre">was</span> <span class="pre">detected</span> <span class="pre">and</span> <span class="pre">compatibility</span> <span class="pre">mode</span> <span class="pre">is</span> <span class="pre">UNAVAILABLE</span></code>?<a class="headerlink" href="DS_FAQ.html#why-do-i-get-the-message-error-this-container-was-built-for-nvidia-driver-release-560-28-or-later-but-version-540-4-0-was-detected-and-compatibility-mode-is-unavailable" title="Link to this heading">#</a></h3>
<p>While running multiaarch triton docker on jetson, above prints are observed. These prints are harmless and should be ignored.</p>
</section>
<section id="on-x86-why-do-i-get-below-error-during-compilation-and-running-of-application-libs-plugins-cuda-failure-the-provided-ptx-was-compiled-with-an-unsupported-toolchain-nvdsinferconvert-c3top3float-cuda-err-the-provided-ptx-was-compiled-with-an-unsupported-toolchain">
<h3>On x86, why do I get below ERROR during compilation and running of application/libs/plugins : <code class="docutils literal notranslate"><span class="pre">Cuda</span> <span class="pre">failure:</span> <span class="pre">the</span> <span class="pre">provided</span> <span class="pre">PTX</span> <span class="pre">was</span> <span class="pre">compiled</span> <span class="pre">with</span> <span class="pre">an</span> <span class="pre">unsupported</span> <span class="pre">toolchain</span></code> <code class="docutils literal notranslate"><span class="pre">NvDsInferConvert_C3ToP3Float:</span> <span class="pre">cuda</span> <span class="pre">err</span> <span class="pre">=</span> <span class="pre">the</span> <span class="pre">provided</span> <span class="pre">PTX</span> <span class="pre">was</span> <span class="pre">compiled</span> <span class="pre">with</span> <span class="pre">an</span> <span class="pre">unsupported</span> <span class="pre">toolchain.</span></code>?<a class="headerlink" href="DS_FAQ.html#on-x86-why-do-i-get-below-error-during-compilation-and-running-of-application-libs-plugins-cuda-failure-the-provided-ptx-was-compiled-with-an-unsupported-toolchain-nvdsinferconvert-c3top3float-cuda-err-the-provided-ptx-was-compiled-with-an-unsupported-toolchain" title="Link to this heading">#</a></h3>
<p>Below steps are needed to be executed for source files which include “.cu” files during compilation and before running the application</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">sudo</span><span class="w"> </span><span class="n">apt</span><span class="o">-</span><span class="n">get</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="o">-</span><span class="n">y</span><span class="w"> </span><span class="n">cuda</span><span class="o">-</span><span class="n">compat</span><span class="mi">-12</span><span class="o">-</span><span class="n">x</span>
<span class="k">export</span><span class="w"> </span><span class="n">LD_LIBRARY_PATH</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="n">compat</span><span class="o">:</span><span class="n">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Replace x with the CUDA version mentioned in the <a class="reference internal" href="DS_Quickstart.html"><span class="doc">Quickstart Guide</span></a> Guide.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is applicable for data center GPUs only.</p>
</div>
</section>
<section id="why-am-i-getting-error-could-not-get-egl-display-connection-while-running-deepstream-sample-application">
<h3>Why am I getting error <code class="docutils literal notranslate"><span class="pre">Could</span> <span class="pre">not</span> <span class="pre">get</span> <span class="pre">EGL</span> <span class="pre">display</span> <span class="pre">connection</span></code> while running deepstream sample application?<a class="headerlink" href="DS_FAQ.html#why-am-i-getting-error-could-not-get-egl-display-connection-while-running-deepstream-sample-application" title="Link to this heading">#</a></h3>
<p>Before running the application, if display device is connected, use command:
$export DISPLAY=:0</p>
<p>If display device is not connected, use command:
$unset DISPLAY</p>
</section>
<section id="why-am-i-getting-error-cugraphicsglregisterbuffer-failed-with-error-219-gst-eglglessink-cuda-init-texture-1-while-running-deepstream-sample-application-with-eglsink-on-sbsa-gh100">
<h3>Why am I getting error <code class="docutils literal notranslate"><span class="pre">cuGraphicsGLRegisterBuffer</span> <span class="pre">failed</span> <span class="pre">with</span> <span class="pre">error(219)</span> <span class="pre">gst_eglglessink_cuda_init</span> <span class="pre">texture</span> <span class="pre">=</span> <span class="pre">1</span></code> while running deepstream sample application with EGLSink on SBSA/GH100?<a class="headerlink" href="DS_FAQ.html#why-am-i-getting-error-cugraphicsglregisterbuffer-failed-with-error-219-gst-eglglessink-cuda-init-texture-1-while-running-deepstream-sample-application-with-eglsink-on-sbsa-gh100" title="Link to this heading">#</a></h3>
<p>SBSA/dGPU on ARM does not support nveglglessink. You should instead be using nv3dsink in your application.</p>
</section>
<section id="why-are-all-deepstream-reference-apps-failing-on-dgpu-on-arm-with-ds-7-1-and-throwing-a-segfault">
<h3>Why are all deepstream reference apps failing on dGPU on ARM with DS 7.1 and throwing a segfault?<a class="headerlink" href="DS_FAQ.html#why-are-all-deepstream-reference-apps-failing-on-dgpu-on-arm-with-ds-7-1-and-throwing-a-segfault" title="Link to this heading">#</a></h3>
<p>This is a known limitation with DS 7.1 arm sbsa docker. Video sink will not work by default on dGPU on ARM systems. Please see section <a class="reference internal" href="DS_Installation.html#ds-installation-sbsa-known-limitation"><span class="std std-ref">Known Limitation with Video Subsystem and Workaround</span></a> for details and workaround on how to get nv3dsink working.</p>
</section>
<section id="why-am-i-getting-error-gpuassert-invalid-argument-when-setting-outputreidtensor-1-in-gst-nvtracker-low-level-config-file">
<h3>Why am I getting error “GPUassert: invalid argument” when setting <code class="docutils literal notranslate"><span class="pre">outputReidTensor:</span> <span class="pre">1</span></code> in Gst-nvtracker low level config file?<a class="headerlink" href="DS_FAQ.html#why-am-i-getting-error-gpuassert-invalid-argument-when-setting-outputreidtensor-1-in-gst-nvtracker-low-level-config-file" title="Link to this heading">#</a></h3>
<p>The actual number of objects in all the streams exceeds the memory allocated for tracker re-identification embedding output. When outputting re-identification tensor is enabled, users are recommended to set <code class="docutils literal notranslate"><span class="pre">maxTargetsPerStream</span></code> to be larger than the total number of objects across all streams in the pipeline.</p>
</section>
<section id="profiling-using-nvtx">
<h3>Profiling using NVTX<a class="headerlink" href="DS_FAQ.html#profiling-using-nvtx" title="Link to this heading">#</a></h3>
<p>NVIDIA<sup>®</sup> Tools Extension SDK (NVTX) provides APIs for annotating events, code ranges, and resources in an application. The core GStreamer plugins in DeepStreamSDK have integrated NVTX and the events and ranges can be captured and visualized using NVIDIA Nsight tool, Tegra System Profiler, and Visual Profiler to capture and visualize these events and ranges. For more information on NVTX, visit <a class="reference external" href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm">https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm</a>. Following are the instructions on how to capture and visualize these events using deepstream-app with NVIDIA Nsight tool:</p>
<ol class="arabic">
<li><p>Download and install Nsight tool from <a class="reference external" href="https://developer.nvidia.com/nsight-systems">https://developer.nvidia.com/nsight-systems</a>. Installation can be done using any file (.deb, .run or .rpm).</p></li>
<li><p>Run the following command on terminal:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">nsys</span><span class="o">-</span><span class="n">ui</span>
</pre></div>
</div>
</li>
<li><p>Click on File -&gt; New Project</p></li>
<li><p>Select target system as “Target for profiling”.</p></li>
<li><p>Now the deepstream-app process needs to be launched by Nsight tool for profiling. In order to do that, add the following command in “Command line with arguments”:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">deepstream</span><span class="o">-</span><span class="n">app</span><span class="w"> </span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">deepstream</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">configs</span><span class="o">/</span><span class="n">deepstream</span><span class="o">-</span><span class="n">app</span><span class="o">/</span><span class="n">source30_1080p_dec_infer</span><span class="o">-</span><span class="n">resnet_tiled_display_int8</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</li>
<li><p>In the tick box options, enable “Collect NVTX trace”. User can select other custom options as per requirement on the tool.</p></li>
<li><p>To begin profiling, click on “Start”.</p></li>
<li><p>Manually stop collection of traces once application run is completed.</p></li>
<li><p>Once collection is stopped, in analytics view, the framewise analysis of each plugin will be shown, which gives an idea of time taken by each plugin.</p></li>
</ol>
<blockquote>
<div><img alt="DeepStream Reference Application NVTX Analytics" class="align-center" src="../_images/DS_NVTX_analytics.png" />
</div></blockquote>
<p>For more information about performance settings, refer “Performance” section of troubleshooting page.</p>
</section>
</section>
<section id="smart-record">
<h2>Smart Record<a class="headerlink" href="DS_FAQ.html#smart-record" title="Link to this heading">#</a></h2>
<section id="does-smart-record-module-work-with-local-video-streams">
<h3>Does smart record module work with local video streams?<a class="headerlink" href="DS_FAQ.html#does-smart-record-module-work-with-local-video-streams" title="Link to this heading">#</a></h3>
<p>Yes. Smart record module expects encoded frames which can be from either local video or RTSP stream. But deepstream-test5-app only supports RTSP sources for smart record.</p>
</section>
<section id="are-multiple-parallel-records-on-same-source-supported">
<h3>Are multiple parallel records on same source supported?<a class="headerlink" href="DS_FAQ.html#are-multiple-parallel-records-on-same-source-supported" title="Link to this heading">#</a></h3>
<p>No. Only single record at a time on the same source is supported. You need to stop the ongoing record to start the new recording again.</p>
</section>
<section id="what-if-i-forgot-to-stop-the-recording">
<h3>What if I forgot to stop the recording?<a class="headerlink" href="DS_FAQ.html#what-if-i-forgot-to-stop-the-recording" title="Link to this heading">#</a></h3>
<p>There is default duration setting and if record is not stopped by stop event it would be stopped automatically as per default duration value.</p>
</section>
<section id="i-started-the-record-with-a-set-duration-can-i-stop-it-before-that-duration-ends">
<h3>I started the record with a set duration. Can I stop it before that duration ends?<a class="headerlink" href="DS_FAQ.html#i-started-the-record-with-a-set-duration-can-i-stop-it-before-that-duration-ends" title="Link to this heading">#</a></h3>
<p>Yes, running recording instance can be stopped any time.</p>
</section>
<section id="what-if-i-dont-set-default-duration-for-smart-record">
<h3>What if I don’t set default duration for smart record?<a class="headerlink" href="DS_FAQ.html#what-if-i-dont-set-default-duration-for-smart-record" title="Link to this heading">#</a></h3>
<p>Default value of record duration is 10 seconds.</p>
</section>
<section id="what-if-i-dont-set-video-cache-size-for-smart-record">
<h3>What if I don’t set video cache size for smart record?<a class="headerlink" href="DS_FAQ.html#what-if-i-dont-set-video-cache-size-for-smart-record" title="Link to this heading">#</a></h3>
<p>Default value of video cache size is 30 seconds.</p>
</section>
<section id="what-is-maximum-duration-of-data-i-can-cache-as-history-for-smart-record">
<h3>What is maximum duration of data I can cache as history for smart record?<a class="headerlink" href="DS_FAQ.html#what-is-maximum-duration-of-data-i-can-cache-as-history-for-smart-record" title="Link to this heading">#</a></h3>
<p>As such, there is no limit on cache size. It is limited by available system memory.</p>
</section>
<section id="can-i-record-the-video-with-bounding-boxes-and-other-information-overlaid">
<h3>Can I record the video with bounding boxes and other information overlaid?<a class="headerlink" href="DS_FAQ.html#can-i-record-the-video-with-bounding-boxes-and-other-information-overlaid" title="Link to this heading">#</a></h3>
<p>To have better performance and optimization, smart record avoids transcoding and caches only encoded frames. To that extent recording the video with overlaid bounding boxes is not possible. But you can implement that use case in two ways:</p>
<ol class="arabic simple">
<li><p>Run the inference pipeline on recorded video and save the output in a file using sink (<code class="docutils literal notranslate"><span class="pre">type</span> <span class="pre">=</span> <span class="pre">filesink</span></code>)</p></li>
<li><p>Add encoding components in the pipeline after OSD and then add smart record module.</p></li>
</ol>
</section>
</section>
<section id="triton">
<h2>Triton<a class="headerlink" href="DS_FAQ.html#triton" title="Link to this heading">#</a></h2>
<section id="can-jetson-platform-support-the-same-features-as-dgpu-for-triton-plugin">
<h3>Can Jetson platform support the same features as dGPU for Triton plugin?<a class="headerlink" href="DS_FAQ.html#can-jetson-platform-support-the-same-features-as-dgpu-for-triton-plugin" title="Link to this heading">#</a></h3>
<p>Not exactly. dGPU can support most models such as TensorRT, Tensorflow (and TF-TRT), ONNX(and with TRT optimization), Pytorch.
Jetson can support TensorRT, Tensorflow (and TF-TRT). Support for other models are coming in future releases.
For more details, see Deepstream Plugin Guide section.</p>
</section>
<section id="how-to-enable-tensorrt-optimization-for-tensorflow-and-onnx-models">
<h3>How to enable TensorRT optimization for Tensorflow and ONNX models?<a class="headerlink" href="DS_FAQ.html#how-to-enable-tensorrt-optimization-for-tensorflow-and-onnx-models" title="Link to this heading">#</a></h3>
<p>To learn details TensorRT optimization setting in Triton models, see:
<a class="github reference external" href="https://github.com/triton-inference-server/server/blob/r23.10/README.md">triton-inference-server/server</a>
TF-TRT is supported on both dGPU and Jetson platforms.
1. Open the model’s Triton <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code> file
2. Make sure GPU instance enabled
3. Append <cite>tensorrt</cite> accelerator.(e.g. <code class="docutils literal notranslate"><span class="pre">triton_model_repo/ssd_mobilenet_v1_coco_2018_01_28/config.pbtxt</span></code>)</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">optimization</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">execution_accelerators</span><span class="w"> </span><span class="p">{</span>
<span class="nl">gpu_execution_accelerator</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="p">{</span>
<span class="nl">name</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="s">&quot;tensorrt&quot;</span>
<span class="n">parameters</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">key</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;precision_mode&quot;</span><span class="w"> </span><span class="n">value</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;FP16&quot;</span><span class="w"> </span><span class="p">}</span>
<span class="p">}]</span><span class="w"> </span><span class="p">}}</span>
</pre></div>
</div>
<p>For more on TF-TRT parameters, see <code class="docutils literal notranslate"><span class="pre">TF-TRT</span></code> API in Tensorflow 1.x. <code class="docutils literal notranslate"><span class="pre">is_dynamic_op</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> natively in Triton.
Additionally, you can generate offline TF-TRT models by their own script running with Tensorflow environment.
Read TF-TRT User guide to generate offline(static) models. Once the original online model is replaced by offline model, remove <code class="docutils literal notranslate"><span class="pre">optimization</span></code> block in case TF-TRT online runs again to overwrite offline TF-TRT caches.
ONNX is supported on dGPU only. TensorRT optimization can be enabled by</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="n">optimization</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">execution_accelerators</span><span class="w"> </span><span class="p">{</span>
<span class="nl">gpu_execution_accelerator</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="s">&quot;tensorrt&quot;</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">]</span>
<span class="p">}}</span>
</pre></div>
</div>
<p>The TensorRT engine caches will be generated at run time. It is getting started during initialization or first frame come. This might take from several seconds even to minutes depends on how heavy the model is and how good platform it is.</p>
</section>
<section id="how-to-tune-gpu-memory-for-tensorflow-models">
<h3>How to tune GPU memory for Tensorflow models?<a class="headerlink" href="DS_FAQ.html#how-to-tune-gpu-memory-for-tensorflow-models" title="Link to this heading">#</a></h3>
<p>When running TensorFlow models using Triton Inference Server, the GPU device memory may fall short. The allowed GPU device memory allocation for TensorFlow models can be tuned using the <code class="docutils literal notranslate"><span class="pre">tf_gpu_memory_fraction</span></code> parameter in the nvdsinferserver’s config files (<code class="docutils literal notranslate"><span class="pre">config_infer_*</span></code>). For more details, see <code class="docutils literal notranslate"><span class="pre">samples/configs/deepstream-app-triton/README</span></code>.
This parameter is same as Tensorflow config’s <code class="docutils literal notranslate"><span class="pre">per_process_gpu_memory_fraction</span></code>. For more details, see:</p>
<ul class="simple">
<li><p><em>Tensorflow 1.x gpu-guide</em></p></li>
<li><p><em>TF-TRT user guide</em></p></li>
</ul>
</section>
<section id="why-am-i-getting-following-warning-when-running-deepstream-app-for-first-time">
<h3>Why am I getting following warning when running deepstream app for first time?<a class="headerlink" href="DS_FAQ.html#why-am-i-getting-following-warning-when-running-deepstream-app-for-first-time" title="Link to this heading">#</a></h3>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="s">&quot;GStreamer-WARNING: Failed to load plugin &#39;...libnvdsgst_inferserver.so&#39;: libtrtserver.so: cannot open shared object file: No such file or directory&quot;</span>
</pre></div>
</div>
<p>This is a harmless warning indicating that the DeepStream’s nvinferserver plugin cannot be used since “Triton Inference Server” is not installed.
If DeepStream-Triton is required, try to pull DeepStream’s Triton docker image on dGPU following instructions
at <a class="reference external" href="https://ngc.nvidia.com/catalog/containers/nvidia:deepstream">https://ngc.nvidia.com/catalog/containers/nvidia:deepstream</a> and run the test apps inside container. Triton Inference Server is automatically installed on Jetson along with DeepStream package. No extra instructions are needed.</p>
</section>
<section id="why-am-i-getting-warning-failed-to-query-video-capabilities-invalid-argument">
<h3>Why am I getting warning “Failed to query video capabilities: Invalid argument”?<a class="headerlink" href="DS_FAQ.html#why-am-i-getting-warning-failed-to-query-video-capabilities-invalid-argument" title="Link to this heading">#</a></h3>
<p>This is a harmless warning. It is due to a bug in nvv4l2decoder and can be safely ignored.</p>
</section>
<section id="how-to-handle-operations-not-supported-by-triton-inference-server">
<h3>How to handle operations not supported by Triton Inference Server?<a class="headerlink" href="DS_FAQ.html#how-to-handle-operations-not-supported-by-triton-inference-server" title="Link to this heading">#</a></h3>
<p>For details on handling unsupported operations, see:
<a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/triton-inference-server-guide/docs/user_guide/custom_operations.html">https://docs.nvidia.com/deeplearning/sdk/triton-inference-server-guide/docs/user_guide/custom_operations.html</a>.</p>
<p>The custom library mentioned in the document can be loaded in the DeepStream application by one of the following methods:</p>
<ul>
<li><p>Running the application as</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">LD_PRELOAD</span><span class="o">=</span><span class="p">.</span><span class="o">/</span><span class="n">libcustomOp</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="n">deepstream</span><span class="o">-</span><span class="n">app</span><span class="w"> </span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="n">app</span><span class="o">-</span><span class="n">config</span><span class="o">&gt;</span>
</pre></div>
</div>
</li>
<li><p>Add the custom-lib path in “nvinferserver” config file as</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">infer_config</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="p">...</span>
<span class="n">custom_lib</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">path</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;./libcustomOp.so&quot;</span><span class="w"> </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="can-gst-nvinferserver-support-models-across-processes-or-containers">
<h3>Can Gst-nvinferserver support models across processes or containers?<a class="headerlink" href="DS_FAQ.html#can-gst-nvinferserver-support-models-across-processes-or-containers" title="Link to this heading">#</a></h3>
<p>The plugin can be configured to use gRPC API to access Triton Inference server using the Triton Client library. In this mode, the Triton server is run as a separate process, on same or different  machine / container.</p>
<p>When the plugin is configured for Triton Server C-APIs, it doesn’t support client/server architecture. But the single process could run a standalone Triton model repo no matter how many models running together.</p>
</section>
<section id="can-users-set-different-model-repos-when-running-multiple-triton-models-in-single-process">
<h3>Can users set different model repos when running multiple Triton models in single process?<a class="headerlink" href="DS_FAQ.html#can-users-set-different-model-repos-when-running-multiple-triton-models-in-single-process" title="Link to this heading">#</a></h3>
<p>No. All config files for a same deepstream-app process must have same model_repo. Otherwise <code class="docutils literal notranslate"><span class="pre">GSt-nvinferserver</span></code> may report error or use random config on <code class="docutils literal notranslate"><span class="pre">model_repo</span></code>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>infer_config { triton { model_repo {
    root: “path/to/model_repo”
    strict_model_config: true
    tf_gpu_memory_fraction: 0.35
    ...
} } }
</pre></div>
</div>
</section>
<section id="why-is-the-script-prepare-classification-test-video-sh-failing-on-triton-docker-s">
<h3>Why is the script <code class="docutils literal notranslate"><span class="pre">prepare_classification_test_video.sh</span></code> failing on Triton docker(s)?<a class="headerlink" href="DS_FAQ.html#why-is-the-script-prepare-classification-test-video-sh-failing-on-triton-docker-s" title="Link to this heading">#</a></h3>
<p>The script <code class="docutils literal notranslate"><span class="pre">prepare_classification_test_video.sh</span></code> present at <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/samples</span></code> requires <code class="docutils literal notranslate"><span class="pre">ffmpeg</span></code> to be installed. Some of the low level codec libraries need to be re-installed along with ffmpeg. Use the following command to install/re-install  ffmpeg: <code class="docutils literal notranslate"><span class="pre">apt-get</span> <span class="pre">install</span> <span class="pre">--reinstall</span> <span class="pre">libflac8</span> <span class="pre">libmp3lame0</span> <span class="pre">libxvidcore4</span> <span class="pre">ffmpeg</span></code></p>
</section>
<section id="what-is-the-difference-between-deepstream-classification-and-triton-classification">
<h3>What is the difference between DeepStream classification and Triton classification?<a class="headerlink" href="DS_FAQ.html#what-is-the-difference-between-deepstream-classification-and-triton-classification" title="Link to this heading">#</a></h3>
<p>Gst-nvinferserver plugin support 2 classification methods:</p>
<ol class="arabic simple">
<li><p>Use DeepStream plugin to parse classification output and select labels. Configure the plugin’s postprocess block with <code class="docutils literal notranslate"><span class="pre">labelfile_path</span></code>, and classification options.</p></li>
</ol>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>infer_config { postprocess {
   labelfile_path: “path/to/classification_labels.txt”
   classification { threshold: 0.5 }
} }
</pre></div>
</div>
<p>Example: <code class="docutils literal notranslate"><span class="pre">samples/</span> <span class="pre">configs/deepstream-app-triton/config_infer_primary_classifier_inception_graphdef_postprocessInDS.txt</span></code></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Use Triton native classification method. The label file configured in Triton model’s <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code> (e.g. <code class="docutils literal notranslate"><span class="pre">samples/triton_model_repo/inception_graphdef/config.pbtxt</span></code>)</p></li>
</ol>
<blockquote>
<div><div class="highlight-text notranslate"><div class="highlight"><pre><span></span>output [ {
name: &quot;InceptionV3/Predictions/Softmax&quot;
data_type: TYPE_FP32
dims: [ 1001 ]
label_filename: &quot;inception_labels.txt&quot;
} ]
</pre></div>
</div>
</div></blockquote>
<p>To enable it, need update Gst-nvinferserver’s config file with:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">infer_config</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">postprocess</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">triton_classification</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">topk</span><span class="o">:</span><span class="mi">1</span><span class="w"> </span><span class="n">threshold</span><span class="o">:</span><span class="w"> </span><span class="mf">0.5</span><span class="w"> </span><span class="p">}</span>
<span class="p">}</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<p>Example: <code class="docutils literal notranslate"><span class="pre">samples/configs/deepstream-app-triton/config_infer_primary_classifier_inception_graphdef_postprocessIntriton.txt</span></code></p>
</section>
<section id="why-is-max-batch-size-0-used-in-some-triton-model-config-files-samples-triton-model-repo-config-pbtxt">
<h3>Why is <code class="docutils literal notranslate"><span class="pre">max_batch_size:</span> <span class="pre">0</span></code> used in some Triton model config files (<code class="docutils literal notranslate"><span class="pre">samples/triton_model_repo/*/config.pbtxt</span></code>)?<a class="headerlink" href="DS_FAQ.html#why-is-max-batch-size-0-used-in-some-triton-model-config-files-samples-triton-model-repo-config-pbtxt" title="Link to this heading">#</a></h3>
<p>This is the parameter settings for Triton runtime. Some models do not support batching according to Triton docs (<a class="reference external" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#maximum-batch-size">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#maximum-batch-size</a>).
Gst-nvinferserver plugin supports these models in the non-batching mode, and tensor input/output shapes usually exist with full dimensions (1st dim is a batch-size).
For example in <code class="docutils literal notranslate"><span class="pre">triton_model_repo/densenet_onnx/config.pbtxt</span></code> with:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="nl">max_batch_size</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="n">input</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="p">...</span>
<span class="w"> </span><span class="nl">dims</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="w"> </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The 1st dim <code class="docutils literal notranslate"><span class="pre">1</span></code> is a batch-size in full-dims.</p>
<p>In addition to some specific cases, if a model can support dynamic-shape with:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="nl">max_batch_size</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="n">input</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="p">...</span>
<span class="w"> </span><span class="nl">dims</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="w"> </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The 1st dim <code class="docutils literal notranslate"><span class="pre">-1</span></code> means a dynamic batching size. In this case, to control the maximum batching size for pre-allocated buffer pool, user need
to configure Gst-nvinferserver plugin <code class="docutils literal notranslate"><span class="pre">configs/deepstream-app-triton/config_infer_**.txt</span></code> with a valid maximum batch-size:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">infer_config</span><span class="w"> </span><span class="p">{</span>
<span class="nl">unique_id</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="nl">gpu_ids</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nl">max_batch_size</span><span class="p">:</span><span class="w"> </span><span class="mi">30</span>
<span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The example above limits the final input batch-size to &lt;= 30.</p>
<p>Gst-nvinferserver can also support Triton models reshape in config files. Read more details here:
<a class="reference external" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#reshape">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#reshape</a></p>
</section>
<section id="how-to-support-triton-ensemble-model">
<h3>How to support Triton ensemble model?<a class="headerlink" href="DS_FAQ.html#how-to-support-triton-ensemble-model" title="Link to this heading">#</a></h3>
<p>See details in <a class="reference internal" href="DS_plugin_gst-nvinferserver.html#triton-ensemble-label"><span class="std std-ref">Triton Ensemble Models</span></a> in :doc: <cite>DS_plugin_gst_nvinferserver</cite> section.</p>
</section>
<section id="does-gst-nvinferserver-support-triton-multiple-instance-groups">
<h3>Does Gst-nvinferserver support Triton multiple instance groups?<a class="headerlink" href="DS_FAQ.html#does-gst-nvinferserver-support-triton-multiple-instance-groups" title="Link to this heading">#</a></h3>
<p>Yes, you can configure Triton model <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code> with multiple instances on single GPU or CPU to make them run in parallel. If multiple instances are configured on different settings (e.g. run an instance on GPU, and another instance on GPU), warmup of the instances is recommended. This is to avoid timeout or other live streaming errors in case the specific instance takes too long to initialize on the first frame.
To enable multiple instances, see:
<a class="reference external" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#instance-groups">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#instance-groups</a>
To enable warmup, see:
<a class="reference external" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#model-warmup">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#model-warmup</a></p>
</section>
<section id="can-gst-nvinferserver-support-inference-on-multiple-gpus">
<h3>Can Gst-nvinferserver support inference on multiple GPUs?<a class="headerlink" href="DS_FAQ.html#can-gst-nvinferserver-support-inference-on-multiple-gpus" title="Link to this heading">#</a></h3>
<p>Not yet. When running on multiple-gpu platform, you’ll need a specific single <cite>gpu-id</cite> for GPU instances. If no <cite>gpu-id</cite> is specified, all GPU instances would be running together by default. This could cause unexpected behaviors. Update <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code> and specify single <cite>gpu-id</cite> explicitly.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">instance_group</span><span class="w"> </span><span class="p">{</span>
<span class="nl">count</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span>
<span class="nl">gpus</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>
<span class="nl">kind</span><span class="p">:</span><span class="w"> </span><span class="n">KIND_GPU</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Or specify single GPU in docker cmdline:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="o">-</span><span class="n">it</span><span class="w"> </span><span class="o">--</span><span class="n">rm</span><span class="w"> </span><span class="o">--</span><span class="n">gpus</span><span class="w"> </span><span class="sc">&#39;&quot;&#39;</span><span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="sc">&#39;&quot;&#39;</span><span class="w"> </span><span class="p">...</span>
</pre></div>
</div>
</section>
<section id="what-is-batch-size-differences-for-a-single-model-in-different-config-files-gie-group-in-source-config-inferserver-and-triton-models-config-pbtxt">
<h3>What is batch-size differences for a single model in different config files (<code class="docutils literal notranslate"><span class="pre">gie</span> <span class="pre">group</span> <span class="pre">in</span> <span class="pre">source</span></code>, <code class="docutils literal notranslate"><span class="pre">config_inferserver..</span></code>, and Triton model’s <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code>)?<a class="headerlink" href="DS_FAQ.html#what-is-batch-size-differences-for-a-single-model-in-different-config-files-gie-group-in-source-config-inferserver-and-triton-models-config-pbtxt" title="Link to this heading">#</a></h3>
<p>Take TensorRT Primary_Detector for example:</p>
<ol class="arabic">
<li><p>Gst-nvinferserver Plugin’s config file <code class="docutils literal notranslate"><span class="pre">configs/deepstream-app-triton/config_infer_plan_engine_primary.txt</span></code>, defines</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">infer_config</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">max_batch_size</span><span class="o">:</span><span class="w"> </span><span class="mi">30</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>This indicates the Gst plugin would pre-allocate 30 input buffers for preprocessing and pass at most 30 batched preprocessed
buffers into Triton Runtime each time. This value must be &gt; 0</p>
</div></blockquote>
<ol class="arabic" start="2">
<li><p>Deepstream-app’s config file <code class="docutils literal notranslate"><span class="pre">configs/deepstream-app-triton/source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt</span></code>, defines</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">primary</span><span class="o">-</span><span class="n">gie</span><span class="p">]</span>
<span class="n">batch</span><span class="o">-</span><span class="n">size</span><span class="o">=</span><span class="mi">4</span>
<span class="n">config</span><span class="o">-</span><span class="n">file</span><span class="o">=</span><span class="n">config_infer_plan_engine_primary</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>This config-file has top-priority and would overwrite <code class="docutils literal notranslate"><span class="pre">configs/deepstream-app-triton/config_infer_plan_engine_primary.txt</span></code> with <code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code> to <code class="docutils literal notranslate"><span class="pre">4</span></code> at run time.</p>
</div></blockquote>
<ol class="arabic" start="3">
<li><p>Triton Runtime has its own config file format following <a class="reference external" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#model-configuration">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#model-configuration</a>.
Inside <code class="docutils literal notranslate"><span class="pre">triton_model_repo/Primary_Detector/config.pbtxt</span></code>, it defines</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Primary_Detector&quot;</span>
<span class="nl">platform</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;tensorrt_plan&quot;</span>
<span class="nl">max_batch_size</span><span class="p">:</span><span class="w"> </span><span class="mi">30</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>This indicates Triton Runtime can batch at most 30 input buffers for model inference. In this example,
plan engine model <code class="docutils literal notranslate"><span class="pre">resnet18_trafficcamnet_pruned.onnx_b30_gpu0_int8.engine</span></code> in Triton backend can support maximum <code class="docutils literal notranslate"><span class="pre">batch-size:</span> <span class="pre">30</span></code>.
If Triton model is not non-batching configured with <code class="docutils literal notranslate"><span class="pre">max_batch_size:</span> <span class="pre">&gt;0</span></code>,  you’ll need to make sure batch-size in config-file
of Gst-nvinferserver and deepstream-app must less than or equal to this Triton model’s <code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code> in <code class="docutils literal notranslate"><span class="pre">triton_model_repo/${model}/config.pbtxt</span></code>.
If Triton model is non-batching configured with <code class="docutils literal notranslate"><span class="pre">max_batch_size:</span> <span class="pre">0</span></code>, see the non-batching support questions above.</p>
</div></blockquote>
<p>#TODO Mention EGLSink error, suggest to use nv3dsink rather than EGL</p>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="DS_troubleshooting.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Troubleshooting</p>
      </div>
    </a>
    <a class="right-next"
       href="DS_on_WSL2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">DeepStream On WSL</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#deepstream-general-topics">DeepStream General topics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-do-i-uninstall-deepstream">How do I uninstall DeepStream?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-types-of-input-streams-does-deepstream-7-1-support">What types of input streams does DeepStream 7.1 support?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-s-the-throughput-of-h-264-and-h-265-decode-on-dgpu-tesla">What’s the throughput of H.264 and H.265 decode on dGPU (Tesla)?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-can-i-run-the-deepstream-sample-application-in-debug-mode">How can I run the DeepStream sample application in debug mode?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#where-can-i-find-the-deepstream-sample-applications">Where can I find the DeepStream sample applications?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-can-i-verify-that-cuda-was-installed-correctly">How can I verify that CUDA was installed correctly?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-can-i-interpret-frames-per-second-fps-display-information-on-console">How can I interpret frames per second (FPS) display information on console?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#my-deepstream-performance-is-lower-than-expected-how-can-i-determine-the-reason">My DeepStream performance is lower than expected. How can I determine the reason?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-can-i-specify-rtsp-streaming-of-deepstream-output">How can I specify RTSP streaming of DeepStream output?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-is-the-official-deepstream-docker-image-and-where-do-i-get-it">What is the official DeepStream Docker image and where do I get it?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-is-the-recipe-for-creating-my-own-docker-image">What is the recipe for creating my own Docker image?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-can-i-display-graphical-output-remotely-over-vnc-how-can-i-determine-whether-x11-is-running">How can I display graphical output remotely over VNC? How can I determine whether X11 is running?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-does-the-deepstream-nvof-test-application-show-the-error-message-device-does-not-support-optical-flow-functionality">Why does the deepstream-nvof-test application show the error message “Device Does NOT support Optical Flow Functionality” ?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-is-the-gst-nvstreammux-plugin-required-in-deepstream-4-0">Why is the Gst-nvstreammux plugin required in DeepStream 4.0+?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-do-i-profile-deepstream-pipeline">How do I profile DeepStream pipeline?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-can-i-check-gpu-and-memory-utilization-on-a-dgpu-system">How can I check GPU and memory utilization on a dGPU system?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-is-the-approximate-memory-utilization-for-1080p-streams-on-dgpu">What is the approximate memory utilization for 1080p streams on dGPU?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#deepstream-apps-fails-while-encoding-on-orin-nano">DeepStream apps fails while encoding on Orin Nano</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#when-deepstream-app-is-run-in-loop-on-jetson-agx-orin-using-while-true-do-deepstream-app-c-config-file-done-after-a-few-iterations-i-see-low-fps-for-certain-iterations-why-is-that">When deepstream-app is run in loop on Jetson AGX Orin using “while true; do deepstream-app -c &lt;config_file&gt;; done;”, after a few iterations I see low FPS for certain iterations. Why is that?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-get-the-error-incorrect-camera-parameters-provided-please-provide-supported-resolution-and-frame-rate-when-i-compile-deepstream-sample-application-with-source1-usb-dec-infer-resnet-int8-txt-config-in-default-setting-on-jetson">Why do I get the error <code class="docutils literal notranslate"><span class="pre">incorrect</span> <span class="pre">camera</span> <span class="pre">parameters</span> <span class="pre">provided,</span> <span class="pre">please</span> <span class="pre">provide</span> <span class="pre">supported</span> <span class="pre">resolution</span> <span class="pre">and</span> <span class="pre">frame</span> <span class="pre">rate</span></code> when I compile DeepStream sample application with source1_usb_dec_infer_resnet_int8.txt config in default setting on Jetson?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-on-jetson-engine-file-generation-fails-occasionally-inside-container">Why do on Jetson, engine file generation fails occasionally inside container?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-the-application-hangs-sometimes-with-rtsp-streams-on-reaching-eos">Why the application hangs sometimes with RTSP streams on reaching EOS?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-get-error-glib-gthread-posix-c-unexpected-error-from-c-library-during-pthread-setspecific-invalid-argument-aborting">Why do I get error <code class="docutils literal notranslate"><span class="pre">GLib</span> <span class="pre">(gthread-posix.c):</span> <span class="pre">Unexpected</span> <span class="pre">error</span> <span class="pre">from</span> <span class="pre">C</span> <span class="pre">library</span> <span class="pre">during</span> <span class="pre">'pthread_setspecific':</span> <span class="pre">Invalid</span> <span class="pre">argument.</span>  <span class="pre">Aborting.</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-get-the-error-makefile-13-cuda-ver-is-not-set-stop-when-i-compile-deepstream-sample-applications">Why do I get the error <code class="docutils literal notranslate"><span class="pre">Makefile:13:</span> <span class="pre">***</span> <span class="pre">"CUDA_VER</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">set".</span>  <span class="pre">Stop</span></code> when I compile DeepStream sample applications?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-can-i-construct-the-deepstream-gstreamer-pipeline">How can I construct the DeepStream GStreamer pipeline?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-set-camera-calibration-parameters-in-dewarper-plugin-config-file">How to set camera calibration parameters in Dewarper plugin config file?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-get-camera-calibration-parameters-for-usage-in-dewarper-plugin">How to get camera calibration parameters for usage in Dewarper plugin?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-minimize-fps-jitter-with-ds-application-while-using-rtsp-camera-streams">How to minimize FPS jitter with DS application while using RTSP Camera Streams?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-am-i-getting-importerror-no-module-named-google-protobuf-internal-when-running-convert-to-uff-py-on-jetson-agx-orin">Why am I getting “ImportError: No module named google.protobuf.internal when running convert_to_uff.py on Jetson AGX Orin”?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#does-deepstream-support-10-bit-video-streams">Does DeepStream Support 10 Bit Video streams?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-is-the-difference-between-batch-size-of-nvstreammux-and-nvinfer-what-are-the-recommended-values-for-nvstreammux-batch-size">What is the difference between batch-size of nvstreammux and nvinfer? What are the recommended values for <code class="docutils literal notranslate"><span class="pre">nvstreammux</span></code> batch-size?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-do-i-configure-the-pipeline-to-get-ntp-timestamps">How do I configure the pipeline to get NTP timestamps?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-is-the-ntp-timestamp-value-0">Why is the NTP timestamp value 0?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-see-confidence-value-as-0-1">Why do I see confidence value as -0.1.?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-use-the-oss-version-of-the-tensorrt-plugins-in-deepstream">How to use the OSS version of the TensorRT plugins in DeepStream?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-see-the-below-error-while-processing-h265-rtsp-stream">Why do I see the below Error while processing H265 RTSP stream?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-observe-a-lot-of-buffers-are-being-dropped-when-running-live-camera-streams-even-for-few-or-single-stream-also-output-looks-jittery">Why do I observe: A lot of buffers are being dropped. When running live camera streams even for few or single stream, also output looks jittery?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-does-the-rtsp-source-used-in-gst-launch-pipeline-through-uridecodebin-show-blank-screen-followed-by-the-error-warning-from-element-gstpipeline-pipeline0-gstnvstreammux-m-no-sources-found-at-the-input-of-muxer-waiting-for-sources">Why does the RTSP source used in gst-launch pipeline through uridecodebin show blank screen followed by the error -  <code class="docutils literal notranslate"><span class="pre">WARNING:</span> <span class="pre">from</span> <span class="pre">element</span> <span class="pre">/GstPipeline:pipeline0/GstNvStreamMux:m:</span> <span class="pre">No</span> <span class="pre">Sources</span> <span class="pre">found</span> <span class="pre">at</span> <span class="pre">the</span> <span class="pre">input</span> <span class="pre">of</span> <span class="pre">muxer.</span> <span class="pre">Waiting</span> <span class="pre">for</span> <span class="pre">sources</span></code>?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-if-i-do-not-get-expected-30-fps-from-camera-using-v4l2src-plugin-in-pipeline-but-instead-get-15-fps-or-less-than-30-fps">What if I do not get expected 30 FPS from camera using v4l2src plugin in pipeline but instead get 15 FPS or less than 30 FPS?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#on-jetson-platform-i-get-same-output-when-multiple-jpeg-images-are-fed-to-nvv4l2decoder-using-multifilesrc-plugin-why-is-that">On Jetson platform, I get same output when multiple Jpeg images are fed to nvv4l2decoder using multifilesrc plugin. Why is that?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#on-jetson-platform-i-observe-lower-fps-output-when-screen-goes-idle">On Jetson platform, I observe lower FPS output when screen goes idle.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-do-i-obtain-individual-sources-after-batched-inferencing-processing-what-are-the-sample-pipelines-for-nvstreamdemux">How do I obtain individual sources after batched inferencing/processing? What are the sample pipelines for nvstreamdemux?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-encounter-such-error-while-running-deepstream-pipeline-memory-type-configured-and-i-p-buffer-mismatch-ip-surf-0-muxer-3">Why do I encounter such error while running Deepstream pipeline memory type configured and i/p buffer mismatch ip_surf 0 muxer 3?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-does-secondary-gie-crop-and-resize-objects">How does secondary GIE crop and resize objects?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-save-frames-from-gstbuffer">How to save frames from GstBuffer?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-are-different-memory-types-supported-on-jetson-and-dgpu">What are different Memory types supported on Jetson and dGPU?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-are-different-memory-transformations-supported-on-jetson-and-dgpu">What are different Memory transformations supported on Jetson and dGPU?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-does-my-image-look-distorted-if-i-wrap-my-cudamalloc-ed-memory-into-nvbufsurface-and-provide-to-nvbufsurftransform">Why does my image look distorted if I wrap my cudaMalloc’ed memory into NvBufSurface and provide to NvBufSurfTransform?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-transformations-below-16x16-dimensions-fail-on-jetson">Why do transformations below 16x16 dimensions fail on Jetson?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-find-out-the-maximum-number-of-streams-supported-on-given-platform">How to find out the maximum number of streams supported on given platform?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-find-the-performance-bottleneck-in-deepstream">How to find the performance bottleneck in DeepStream?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-fix-cannot-allocate-memory-in-static-tls-block-error">How to fix “cannot allocate memory in static TLS block” error?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-measure-pipeline-latency-if-pipeline-contains-open-source-components">How to measure pipeline latency  if pipeline contains open source components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#migration-to-newer-gstreamer-version">Migration to  newer gstreamer version</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-get-the-warning-warning-from-src-elem-no-decoder-available-for-type-audio-mpeg-mpegversion-int-4-when-i-run-deepstream-app-sample-configuration-on-ds-docker-containers">Why do I get the WARNING <code class="docutils literal notranslate"><span class="pre">WARNING</span> <span class="pre">from</span> <span class="pre">src_elem:</span> <span class="pre">No</span> <span class="pre">decoder</span> <span class="pre">available</span> <span class="pre">for</span> <span class="pre">type</span> <span class="pre">'audio/mpeg,</span> <span class="pre">mpegversion=(int)4’</span></code> when I run deepstream-app sample configuration on DS docker containers?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-get-the-warning-failed-to-load-plugin-usr-lib-aarch64-linux-gnu-gstreamer-1-0-libgstlibav-so-libavfilter-so-7-cannot-open-shared-object-file-no-such-file-or-directory-when-using-deepstream-app-version-all">Why do I get the WARNING: <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">load</span> <span class="pre">plugin</span> <span class="pre">'/usr/lib/aarch64-linux-gnu/gstreamer-1.0/libgstlibav.so':</span> <span class="pre">libavfilter.so.7:</span></code> <code class="docutils literal notranslate"><span class="pre">cannot</span> <span class="pre">open</span> <span class="pre">shared</span> <span class="pre">object</span> <span class="pre">file:</span> <span class="pre">No</span> <span class="pre">such</span> <span class="pre">file</span> <span class="pre">or</span> <span class="pre">directory</span></code> when using deepstream-app –version-all?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-get-the-warning-message-libegl-warning-dri3-screen-seems-not-dri3-capable-and-libegl-warning-dri2-failed-to-authenticate-when-running-deepstream-pipeline-with-nveglglesink">Why do I get the warning message: <code class="docutils literal notranslate"><span class="pre">libEGL</span> <span class="pre">warning:</span> <span class="pre">DRI3:</span> <span class="pre">Screen</span> <span class="pre">seems</span> <span class="pre">not</span> <span class="pre">DRI3</span> <span class="pre">capable</span></code> and <code class="docutils literal notranslate"><span class="pre">libEGL</span> <span class="pre">warning:</span> <span class="pre">DRI2:</span> <span class="pre">failed</span> <span class="pre">to</span> <span class="pre">authenticate</span></code> when running deepstream pipeline with nveglglesink?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-run-higher-number-of-streams-200-on-hopper-ampere-and-ada">How to run higher number of streams (200+) on Hopper,Ampere and ADA?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-do-i-get-the-message-error-this-container-was-built-for-nvidia-driver-release-560-28-or-later-but-version-540-4-0-was-detected-and-compatibility-mode-is-unavailable">Why do I get the message: <code class="docutils literal notranslate"><span class="pre">ERROR:</span> <span class="pre">This</span> <span class="pre">container</span> <span class="pre">was</span> <span class="pre">built</span> <span class="pre">for</span> <span class="pre">NVIDIA</span> <span class="pre">Driver</span> <span class="pre">Release</span> <span class="pre">560.28</span> <span class="pre">or</span> <span class="pre">later,</span> <span class="pre">but</span> <span class="pre">version</span> <span class="pre">540.4.0</span> <span class="pre">was</span> <span class="pre">detected</span> <span class="pre">and</span> <span class="pre">compatibility</span> <span class="pre">mode</span> <span class="pre">is</span> <span class="pre">UNAVAILABLE</span></code>?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#on-x86-why-do-i-get-below-error-during-compilation-and-running-of-application-libs-plugins-cuda-failure-the-provided-ptx-was-compiled-with-an-unsupported-toolchain-nvdsinferconvert-c3top3float-cuda-err-the-provided-ptx-was-compiled-with-an-unsupported-toolchain">On x86, why do I get below ERROR during compilation and running of application/libs/plugins : <code class="docutils literal notranslate"><span class="pre">Cuda</span> <span class="pre">failure:</span> <span class="pre">the</span> <span class="pre">provided</span> <span class="pre">PTX</span> <span class="pre">was</span> <span class="pre">compiled</span> <span class="pre">with</span> <span class="pre">an</span> <span class="pre">unsupported</span> <span class="pre">toolchain</span></code> <code class="docutils literal notranslate"><span class="pre">NvDsInferConvert_C3ToP3Float:</span> <span class="pre">cuda</span> <span class="pre">err</span> <span class="pre">=</span> <span class="pre">the</span> <span class="pre">provided</span> <span class="pre">PTX</span> <span class="pre">was</span> <span class="pre">compiled</span> <span class="pre">with</span> <span class="pre">an</span> <span class="pre">unsupported</span> <span class="pre">toolchain.</span></code>?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-am-i-getting-error-could-not-get-egl-display-connection-while-running-deepstream-sample-application">Why am I getting error <code class="docutils literal notranslate"><span class="pre">Could</span> <span class="pre">not</span> <span class="pre">get</span> <span class="pre">EGL</span> <span class="pre">display</span> <span class="pre">connection</span></code> while running deepstream sample application?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-am-i-getting-error-cugraphicsglregisterbuffer-failed-with-error-219-gst-eglglessink-cuda-init-texture-1-while-running-deepstream-sample-application-with-eglsink-on-sbsa-gh100">Why am I getting error <code class="docutils literal notranslate"><span class="pre">cuGraphicsGLRegisterBuffer</span> <span class="pre">failed</span> <span class="pre">with</span> <span class="pre">error(219)</span> <span class="pre">gst_eglglessink_cuda_init</span> <span class="pre">texture</span> <span class="pre">=</span> <span class="pre">1</span></code> while running deepstream sample application with EGLSink on SBSA/GH100?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-are-all-deepstream-reference-apps-failing-on-dgpu-on-arm-with-ds-7-1-and-throwing-a-segfault">Why are all deepstream reference apps failing on dGPU on ARM with DS 7.1 and throwing a segfault?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-am-i-getting-error-gpuassert-invalid-argument-when-setting-outputreidtensor-1-in-gst-nvtracker-low-level-config-file">Why am I getting error “GPUassert: invalid argument” when setting <code class="docutils literal notranslate"><span class="pre">outputReidTensor:</span> <span class="pre">1</span></code> in Gst-nvtracker low level config file?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#profiling-using-nvtx">Profiling using NVTX</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#smart-record">Smart Record</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#does-smart-record-module-work-with-local-video-streams">Does smart record module work with local video streams?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#are-multiple-parallel-records-on-same-source-supported">Are multiple parallel records on same source supported?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-if-i-forgot-to-stop-the-recording">What if I forgot to stop the recording?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#i-started-the-record-with-a-set-duration-can-i-stop-it-before-that-duration-ends">I started the record with a set duration. Can I stop it before that duration ends?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-if-i-dont-set-default-duration-for-smart-record">What if I don’t set default duration for smart record?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-if-i-dont-set-video-cache-size-for-smart-record">What if I don’t set video cache size for smart record?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-is-maximum-duration-of-data-i-can-cache-as-history-for-smart-record">What is maximum duration of data I can cache as history for smart record?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#can-i-record-the-video-with-bounding-boxes-and-other-information-overlaid">Can I record the video with bounding boxes and other information overlaid?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#triton">Triton</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#can-jetson-platform-support-the-same-features-as-dgpu-for-triton-plugin">Can Jetson platform support the same features as dGPU for Triton plugin?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-enable-tensorrt-optimization-for-tensorflow-and-onnx-models">How to enable TensorRT optimization for Tensorflow and ONNX models?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-tune-gpu-memory-for-tensorflow-models">How to tune GPU memory for Tensorflow models?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-am-i-getting-following-warning-when-running-deepstream-app-for-first-time">Why am I getting following warning when running deepstream app for first time?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-am-i-getting-warning-failed-to-query-video-capabilities-invalid-argument">Why am I getting warning “Failed to query video capabilities: Invalid argument”?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-handle-operations-not-supported-by-triton-inference-server">How to handle operations not supported by Triton Inference Server?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#can-gst-nvinferserver-support-models-across-processes-or-containers">Can Gst-nvinferserver support models across processes or containers?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#can-users-set-different-model-repos-when-running-multiple-triton-models-in-single-process">Can users set different model repos when running multiple Triton models in single process?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-is-the-script-prepare-classification-test-video-sh-failing-on-triton-docker-s">Why is the script <code class="docutils literal notranslate"><span class="pre">prepare_classification_test_video.sh</span></code> failing on Triton docker(s)?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-is-the-difference-between-deepstream-classification-and-triton-classification">What is the difference between DeepStream classification and Triton classification?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#why-is-max-batch-size-0-used-in-some-triton-model-config-files-samples-triton-model-repo-config-pbtxt">Why is <code class="docutils literal notranslate"><span class="pre">max_batch_size:</span> <span class="pre">0</span></code> used in some Triton model config files (<code class="docutils literal notranslate"><span class="pre">samples/triton_model_repo/*/config.pbtxt</span></code>)?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#how-to-support-triton-ensemble-model">How to support Triton ensemble model?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#does-gst-nvinferserver-support-triton-multiple-instance-groups">Does Gst-nvinferserver support Triton multiple instance groups?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#can-gst-nvinferserver-support-inference-on-multiple-gpus">Can Gst-nvinferserver support inference on multiple GPUs?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_FAQ.html#what-is-batch-size-differences-for-a-single-model-in-different-config-files-gie-group-in-source-config-inferserver-and-triton-models-config-pbtxt">What is batch-size differences for a single model in different config files (<code class="docutils literal notranslate"><span class="pre">gie</span> <span class="pre">group</span> <span class="pre">in</span> <span class="pre">source</span></code>, <code class="docutils literal notranslate"><span class="pre">config_inferserver..</span></code>, and Triton model’s <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code>)?</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">



  <p class="copyright">
    
      Copyright © 2024-2025, NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item"><p class="last-updated">
  Last updated on Sep 15, 2025.
  <br/>
</p></div>
      
        <div class="footer-item">
<div class="extra_footer">
  
      <script type="text/javascript">if (typeof _satellite !== "undefined") {_satellite.pageBottom();}</script>
    
  
</div></div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>