

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" type="text/javascript"
  data-document-language="true" charset="UTF-8" data-domain-script="3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5">
</script>
<script type="text/javascript">
  function OptanonWrapper() {
    var event = new Event('bannerLoaded');
    window.dispatchEvent(event);
  }
</script>
<script src="https://images.nvidia.com/aem-dam/Solutions/ot-js/ot-custom.js" type="text/javascript">
</script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Gst-nvtracker &#8212; DeepStream documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css%3Fdigest=dfe6caa3a7d634c4db9b.css" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css%3Fv=a746c00c.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/nvidia-sphinx-theme.css%3Fv=eb367b29.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css%3Fv=7abaf8bc.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css%3Fv=95c83b7e.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js%3Fdigest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js%3Fdigest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js%3Fv=22d9b4cb"></script>
    <script src="../_static/doctools.js%3Fv=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js%3Fv=dc90522c"></script>
    <script src="../_static/design-tabs.js%3Fv=f930bc37"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'text/DS_plugin_gst-nvtracker';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '../versions1.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '7.1';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="../_static/version-patch.js%3Fv=c24f8c5d"></script>
    <link rel="icon" href="../_static/Nvidia.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gst-nvstreammux" href="DS_plugin_gst-nvstreammux.html" />
    <link rel="prev" title="Gst-nvinferserver" href="DS_plugin_gst-nvinferserver.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Sep 15, 2025"/>

    <script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js" ></script>
    


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="DS_plugin_gst-nvtracker.html#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="DeepStream documentation - Home"/>
    <script>document.write(`<img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark" alt="DeepStream documentation - Home"/>`);</script>
  
  
    <p class="title logo__title">DeepStream documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="DeepStream documentation - Home"/>
    <script>document.write(`<img src="../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark" alt="DeepStream documentation - Home"/>`);</script>
  
  
    <p class="title logo__title">DeepStream documentation</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">


<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Overview.html">Welcome to the DeepStream Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Migration_guide.html">Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Quickstart.html">Quickstart Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_docker_containers.html">Docker Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Samples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_C_Sample_Apps.html">C/C++ Sample Apps Source Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Python_Sample_Apps.html">Python Sample Apps and Bindings Source Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_deepstream.html">DeepStream Reference Application - deepstream-app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_test5.html">DeepStream Reference Application - deepstream-test5 app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_nmos.html">DeepStream Reference Application - deepstream-nmos app</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_ref_app_github.html">DeepStream Reference Application on GitHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_sample_configs_streams.html">Sample Configurations and Streams</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_sample_custom_gstream.html">Implementing a Custom GStreamer Plugin with OpenCV Integration Example</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TAO toolkit Integration with DeepStream</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_TAO_integration.html">TAO Toolkit Integration with DeepStream</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials and How-to's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Custom_Manual.html">DeepStream-3D Custom Apps and Libs Tutorials</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Performance.html">Performance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Accuracy</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Accuracy.html">Accuracy Tuning Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Custom Model</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_using_custom_model.html">Using a Custom Model with DeepStream</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Key Features</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Sensor_Fusion.html">DeepStream-3D Sensor Fusion Multi-Modal Application and Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Camera_BEVFusion.html">DeepStream-3D Multi-Modal BEVFusion Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_MultiModal_Lidar_Camera_V2XFusion.html">DeepStream-3D Multi-Modal V2XFusion Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Smart_video.html">Smart Video Record</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_IoT.html">IoT</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_on_the_fly_model.html">On the Fly Model Update</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_NTP_Timestamp.html">NTP Timestamp in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_AVSync.html">AV Sync in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_RestServer.html">DeepStream With REST API Sever</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Action.html">DeepStream 3D Action Recognition App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Depth_Camera.html">DeepStream 3D Depth Camera App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_3D_Lidar_Inference.html">DeepStream 3D Lidar Inference App</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_library_nvdsnmos.html">Networked Media Open Specifications (NMOS) in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_postprocessing_plugin.html">Gst-nvdspostprocess in DeepStream</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_Can_Orientation.html">DeepStream Can Orientation App</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Application Migration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Application_migration.html">Application Migration to DeepStream 7.1 from DeepStream 7.0</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Plugin Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="DS_plugin_Intro.html">GStreamer Plugin Overview</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_metadata.html">MetaData in the DeepStream SDK</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdspreprocess.html">Gst-nvdspreprocess (Alpha)</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvinfer.html">Gst-nvinfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvinferserver.html">Gst-nvinferserver</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="DS_plugin_gst-nvtracker.html#">Gst-nvtracker</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreammux.html">Gst-nvstreammux</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreammux2.html">Gst-nvstreammux New</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvstreamdemux.html">Gst-nvstreamdemux</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmultistreamtiler.html">Gst-nvmultistreamtiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsosd.html">Gst-nvdsosd</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsmetautils.html">Gst-nvdsmetautils</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsvideotemplate.html">Gst-nvdsvideotemplate</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsaudiotemplate.html">Gst-nvdsaudiotemplate</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideoconvert.html">Gst-nvvideoconvert</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdewarper.html">Gst-nvdewarper</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvof.html">Gst-nvof</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvofvisual.html">Gst-nvofvisual</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvsegvisual.html">Gst-nvsegvisual</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideo4linux2.html">Gst-nvvideo4linux2</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvjpegdec.html">Gst-nvjpegdec</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvimagedec.html">Gst-nvimagedec</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvjpegenc.html">Gst-nvjpegenc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvimageenc.html">Gst-nvimageenc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmsgconv.html">Gst-nvmsgconv</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmsgbroker.html">Gst-nvmsgbroker</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsanalytics.html">Gst-nvdsanalytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsudpsrc.html">Gst-nvdsudpsrc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsudpsink.html">Gst-nvdsudpsink</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdspostprocess.html">Gst-nvdspostprocess (Alpha)</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dfilter.html">Gst-nvds3dfilter</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dbridge.html">Gst-nvds3dbridge</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvds3dmixer.html">Gst-nvds3dmixer</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsucx.html">Gst-NvDsUcx</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvdsxfer.html">Gst-nvdsxfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvvideotestsrc.html">Gst-nvvideotestsrc</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvmultiurisrcbin.html">Gst-nvmultiurisrcbin</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_plugin_gst-nvurisrcbin.html">Gst-nvurisrcbin</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Troubleshooting and FAQ</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_FAQ.html">Frequently Asked Questions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream On WSL2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_on_WSL2.html">DeepStream On WSL</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_WSL2_FAQ.html">FAQ for Deepstream On WSL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream API Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_API_Guide.html">DeepStream API Guides</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Service Maker</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_intro.html">What is Deepstream Service Maker</a></li>
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_cpp.html">Service Maker for C/C++ Developers</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="DS_service_maker_python.html">Service Maker for Python Developers(alpha)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_quick_start.html">Quick Start Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_into_to_flow_api.html">Introduction to Flow APIs</a></li>

<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_into_to_pipeline_api.html">Introduction to Pipeline APIs</a></li>

<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_python_advanced_features.html">Advanced Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="DS_service_maker_traditional_app_migration.html">Migrating Traditional Deepstream Apps to Service Maker Apps in Python</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="DS_service_maker_plugin.html">What is a Deepstream Service Maker Plugin</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deepstream Libraries</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Libraries.html">DeepStream Libraries (Developer Preview)</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph Composer</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_intro.html">Overview</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Platforms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Platforms.html">Supported platforms</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Getting_Started.html">Application Development Workflow</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_GraphComposer_Create_Graph.html">Creating an AI Application</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_Sample_Graphs.html">Reference graphs</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Dev_Workflow.html">Extension Development Workflow</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_Developing_Extension.html">Developing Extensions for DeepStream</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Zero_Coding_DS_Components.html">DeepStream Components</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Internals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Internals.html">GXF Internals</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph eXecution Engine</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Graph_Runtime.html">Graph Execution Engine</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graph Composer Containers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Containers.html">Graph Composer and GXF Containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Component Interfaces</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Component_Interfaces.html">GXF Component Interfaces</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Application API's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_App_C++_APIs.html">GXF App C++ APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_App_Python_APIs.html">GXF App Python APIs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">GXF Runtime API's</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_C++_APIs.html">GXF Core C++ APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_C_APIs.html">GXF Core C APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GXF_Core_Python_APIs.html">GXF Core Python APIs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Extension Manual</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="Extensionmanual_toc.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/CudaExtension.html">CudaExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/StreamSync.html">GXF Stream Sync</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/StandardExtension.html">StandardExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/Python_Codelet.html">Python Codelets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/NetworkExtension.html">NetworkExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/NvTritonExt.html">NvTritonExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/SerializationExtension.html">SerializationExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/MultimediaExtension.html">MultimediaExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/VideoEncoderExtension.html">VideoEncoderExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/VideoDecoderExtension.html">VideoDecoderExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/Behavior_Tree.html">Behavior Trees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/UcxExtension.html">UCX Extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/HttpExtension.html">HttpExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/GrpcExtension.html">GrpcExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../graphtools-docs/docs/text/ExtensionsManual/TensorrtExtension.html">TensorRTExtension</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDs3dProcessingExt.html">NvDs3dProcessingExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsActionRecognitionExt.html">NvDsActionRecognitionExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsAnalyticsExt.html">NvDsAnalyticsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsBaseExt.html">NvDsBaseExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsCloudMsgExt.html">NvDsCloudMsgExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsConverterExt.html">NvDsConverterExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsDewarperExt.html">NvDsDewarperExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInferenceExt.html">NvDsInferenceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInferenceUtilsExt.html">NvDsInferenceUtilsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsInterfaceExt.html">NvDsInterfaceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsMuxDemuxExt.html">NvDsMuxDemuxExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsOpticalFlowExt.html">NvDsOpticalFlowExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsOutputSinkExt.html">NvDsOutputSinkExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSampleExt.html">NvDsSampleExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSampleModelsExt.html">NvDsSampleModelsExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsSourceExt.html">NvDsSourceExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTemplateExt.html">NvDsTemplateExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTrackerExt.html">NvDsTrackerExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTranscodeExt.html">NvDsTranscodeExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsTritonExt.html">NvDsTritonExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsUcxExt.html">NvDsUcxExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsUdpExt.html">NvDsUdpExt</a></li>
<li class="toctree-l2"><a class="reference internal" href="ExtensionsManual/NvDsVisualizationExt.html">NvDsVisualizationExt</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tools</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Registry.html">Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Registry_CLI.html">Registry Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Composer.html">Composer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_Container_Builder.html">Container Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_gxf_CLI.html">GXF Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pipetuner-guide.html">Pipetuner Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">FAQ Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../graphtools-docs/docs/text/GraphComposer_FAQ.html">FAQ</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Legal Information</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DS_Legal.html">DeepStream End User License Agreement</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepStream Feedback</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="DeepStream_Main_Feedback_Form.html">Feedback form</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">


<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
  </div>
  
  <div id="rtd-footer-container"></div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="DS_plugin_Intro.html" class="nav-link">GStreamer Plugin Overview</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Gst-nvtracker</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="gst-nvtracker">
<span id="ds-plugin-nvtracker"></span><h1>Gst-nvtracker<a class="headerlink" href="DS_plugin_gst-nvtracker.html#gst-nvtracker" title="Link to this heading">#</a></h1>
<p>The Gst-nvtracker plugin allows the DS pipeline to use a low-level tracker library to track the detected objects over time persistently with unique IDs. It supports any low-level library that implements <code class="docutils literal notranslate"><span class="pre">NvDsTracker</span></code> API, including the reference implementations provided by the <cite>NvMultiObjectTracker</cite> library: IOU, NvSORT, NvDeepSORT and NvDCF trackers. As part of this API, the plugin queries the low-level library for capabilities and requirements concerning the input format, memory type, and additional feature support. Based on these queries, the plugin then converts the input frame buffers into the format requested by the low-level tracker library. For example, the NvDeepSORT and NvDCF trackers use NV12 or RGBA, while IOU and NvSORT requires no video frame buffers at all.</p>
<p>Based on the queries, the Gst-nvtracker plugin constructs the input data to the low-level tracker library, which consists of video frames and detected objects across multiple streams that are fed to the low-level library in a single data structure (i.e, a frame batch) through a single API call. This API design allows all the input data from multiple streams in the low-level tracker library processed in a batched processing mode (similar to the batch mode in <a class="reference external" href="https://developer.nvidia.com/cufft">cuFFT™</a>, <a class="reference external" href="https://developer.nvidia.com/cublas">cuBLAS™</a>, etc.) for potentially more efficient execution especially when accelerated on GPU. There for, it is required that the low-level tracker supports <cite>batch processing</cite> when using <code class="docutils literal notranslate"><span class="pre">NvMOT_Process</span></code> API.</p>
<p>The Gst-nvtracker plugin supports retrieval of the user-defined <cite>miscellaneous data</cite> from the low-level tracker library through <code class="docutils literal notranslate"><span class="pre">NvMOT_RetrieveMiscData</span></code> API, which includes useful object tracking information other than the default data for the current frame targets; for example, past-frame object data, targets in shadow tracking mode, full trajectory of terminated targets and re-identification features. More details on the types of miscellaneous data and what they means can be found in <a class="reference internal" href="DS_plugin_gst-nvtracker.html#miscellaneous-data-output">Miscellaneous Data Output</a> section. The users are allowed to define other types of miscellaneous data in <code class="docutils literal notranslate"><span class="pre">NvMOTTrackerMiscData</span></code>.</p>
<p>More details on all these tracker APIs is discussed in <a class="reference internal" href="DS_plugin_gst-nvtracker.html#nvdstracker-api-for-low-level-tracker-library">NvDsTracker API for Low-Level Tracker Library</a> section.</p>
<p>The plugin accepts NV12- or RGBA-formatted frame data from the upstream component and scales (and/or converts) the input buffer to a buffer in the tracker plugin based on the format required by the low-level library, with the frame resolution specified by <code class="docutils literal notranslate"><span class="pre">tracker-width</span></code> and <code class="docutils literal notranslate"><span class="pre">tracker-height</span></code> in the configuration file’s <code class="docutils literal notranslate"><span class="pre">[tracker]</span></code> section. The path to the low-level tracker library is to be specified via <code class="docutils literal notranslate"><span class="pre">ll-lib-file</span></code> configuration option in the same section. The low-level library to be used may also require its own configuration file, which can be specified via <code class="docutils literal notranslate"><span class="pre">ll-config-file</span></code> option. If <code class="docutils literal notranslate"><span class="pre">ll-config-file</span></code> is not specified, the low-level tracker library may proceed with its default parameter values.</p>
<p>The reference low-level tracker implementations provided by the <code class="docutils literal notranslate"><span class="pre">NvMultiObjectTracker</span></code> library support various types of multi-object tracking algorithms:</p>
<ul class="simple">
<li><p><strong>IOU Tracker</strong>: The Intersection-Over-Union (IOU) tracker uses the IOU values among the detector’s bounding boxes between the two consecutive frames to perform the association between them or assign a new target ID if no match found. This tracker includes a logic to handle false positives and false negatives from the object detector; however, this can be considered as the bare-minimum object tracker, which may serve as a baseline only.</p></li>
<li><p><strong>NvSORT</strong>: The NvSORT tracker is the NVIDIA®-enhanced Simple Online and Realtime Tracking (SORT) algorithm. Instead of a simple bipartite matching algorithm, NvSORT uses a cascaded data association based on bounding box (bbox) proximity for associating bboxes over consecutive frames and applies a Kalman filter to update the target states. It is computationally efficient since it does not involve any pixel data processing.</p></li>
<li><p><strong>NvDeepSORT</strong>: The NvDeepSORT tracker is the NVIDIA®-enhanced Online and Realtime Tracking with a Deep Association Metric (DeepSORT) algorithm, which uses the deep cosine metric learning with a Re-ID neural network for data association of multiple objects over frames. This implementation allows users to use any Re-ID network as long as it is supported by NVIDIA’s TensorRT™ framework. NvDeepSORT also uses a cascaded data association instead of a simple bipartite matching. The implementation is also optimized for efficient processing on GPU.</p></li>
<li><p><strong>NvDCF</strong>: The NvDCF tracker is an online multi-object tracker that employs a discriminative correlation filter for visual object tracking, which allows independent object tracking even when detection results are not available. It uses the combination of the correlation filter responses and bounding box proximity for data association.</p></li>
</ul>
<p>More details on each algorithm and its implementation details can be found in <a class="reference internal" href="DS_plugin_gst-nvtracker.html#nvmultiobjecttracker-a-reference-low-level-tracker-library">NvMultiObjectTracker : A Reference Low-Level Tracker Library</a> section.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The source code of the Gst-nvtracker plugin is provided as a part of DeepStream SDK package under <code class="docutils literal notranslate"><span class="pre">sources/gst-plugins/gst-nvtracker/</span></code> when installed on a system This is to allow users to make direct changes in the plugin whenever needed for their custom applications and also to show the users as to how the low-level libraries are managed and how the metadata is handled in the plugin.</p>
</div>
<img alt="Gst-nvtracker" class="align-center" src="../_images/DS_plugin_gst-nvtracker_6.0GA.png" />
<section id="sub-batching-alpha">
<h2>Sub-batching (Alpha)<a class="headerlink" href="DS_plugin_gst-nvtracker.html#sub-batching-alpha" title="Link to this heading">#</a></h2>
<p>The Gst-nvtracker plugin works in the batch processing mode by default. In this mode, the input frame batch is passed to and processed by a <cite>single</cite> instance of low-level tracker library. The advantage of batch processing mode is to allow GPUs to work on bigger amount of data at once, potentially increasing the GPU occupancy during execution and reducing the CUDA kernel launch overhead. Depending on the use cases, however, a potential issue is that there is a possibility that GPU could be idling (also referred to as GPU bubble) in some compute stages in the tracker unless the end-to-end operation within the module is carried out solely on the GPU. This is indeed the case if some of the compute modules in the tracker runs on CPU. If there are other components in the DeepStream pipeline that uses GPU (e.g., GPU-based inference in PGIE and SGIE), such CPU blocks in tracker can be <cite>hidden</cite> behind them, not affecting the overall throughput of the pipeline.</p>
<p>The newly-introduced <cite>Sub-batching</cite> feature allows the plugin to split the input frame batch into multiple sub-batches (for example, a four-stream pipeline can use two sub-batches in the tracker plugin, each of which takes care of two streams). Each sub-batch is assigned to a separate instance of low-level tracker library, where the input to the corresponding sub-batch is processed separately. Each instance of low-level tracker libraries runs on a dedicated thread running independently, allowing parallel processing of sub-batches and minimizing the GPU idling due to CPU compute blocks, which eventually results in higher resource utilization.</p>
<p>Because sub-batching assigns separate low-level tracker library instances to different sub-batches, it allows the user to configure each individual sub-batch differently with different low-level tracker library configuration files. This can be utilized in multiple ways like setting varied compute backends across sub-batches, using varied tracking algorithms across sub-batches or modifying any other configuration that is supported in low-level tracker configuration file. More detailed example use-cases are discussed in <a class="reference internal" href="DS_plugin_gst-nvtracker.html#setup-and-usage-of-sub-batching-alpha">Setup and Usage of Sub-batching (Alpha)</a> section.</p>
</section>
<section id="inputs-and-outputs">
<h2>Inputs and Outputs<a class="headerlink" href="DS_plugin_gst-nvtracker.html#inputs-and-outputs" title="Link to this heading">#</a></h2>
<p>This section summarizes the inputs, outputs, and communication facilities of the Gst-nvtracker plugin.</p>
<ul class="simple">
<li><p>Input</p>
<ul>
<li><p>Gst Buffer</p>
<ul>
<li><p>A frame batch from available source streams</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">NvDsBatchMeta</span></code></p>
<ul>
<li><p>Includes the detected object info from primary inference module</p></li>
<li><p>More details about <a class="reference external" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_metadata.html?highlight=nvdsbatchmeta/">NvDsBatchMeta</a> can be found in the link.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The color formats supported for the input video frame by the NvTracker plugin are NV12 and RGBA. A separate batch of video frames are created from the input video frames based on the color format and the resolution that is required to the low-level tracker library.</p>
<ul class="simple">
<li><p>Output</p>
<ul>
<li><p>Gst Buffer</p>
<ul>
<li><p>Same as the input. Unmodified.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">NvDsBatchMeta</span></code></p>
<ul>
<li><p>Updated with additional data from tracker low-level library</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>There’s no separate data structure for the output. Instead, the tracker plugin just adds/updates the data in the existing <code class="docutils literal notranslate"><span class="pre">NvDsBatchMeta</span></code> (and its <code class="docutils literal notranslate"><span class="pre">NvDsObjectMeta</span></code>) with the output data from the tracker low-level library, including tracked object coordinates, tracker confidence, and object IDs. There are some other miscellaneous data that can be attached as user-meta, which is covered in <a class="reference internal" href="DS_plugin_gst-nvtracker.html#miscellaneous-data-output">Miscellaneous Data Output</a> section.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the tracker algorithm does not generate confidence value, then tracker confidence value will be set to the default value (i.e., <code class="docutils literal notranslate"><span class="pre">1.0</span></code>) for tracked objects. For IOU, NvSORT and NvDeepSORT trackers, <code class="docutils literal notranslate"><span class="pre">tracker_confidence</span></code> is set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code> as these algorithms do not generate confidence values for tracked objects. NvDCF tracker, on the other hand, generates confidence for the tracked objects due to its visual tracking capability, and its value is set in <code class="docutils literal notranslate"><span class="pre">tracker_confidence</span></code> field in <code class="docutils literal notranslate"><span class="pre">NvDsObjectMeta</span></code> structure.</p>
<p>Note that there are separate parameters in <code class="docutils literal notranslate"><span class="pre">NvDsObjectMeta</span></code> for detector’s confidence and tracker’s confidence, which are <code class="docutils literal notranslate"><span class="pre">confidence</span></code> and <code class="docutils literal notranslate"><span class="pre">tracker_confidence</span></code>, respectively. More details can be found in <a class="reference external" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_metadata.html?highlight=tracker_confidence#new-metadata-fields">New metadata fields</a></p>
</div>
<p>The following table summarizes the features of the plugin.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id14">
<caption><span class="caption-text">Gst-nvtracker plugin features</span><a class="headerlink" href="DS_plugin_gst-nvtracker.html#id14" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 40.0%" />
<col style="width: 40.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Release</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Configurable tracker width/height</p></td>
<td><p>Frames are internally scaled in NvTracker plugin to the specified resolution for tracking and passed to the low-level lib</p></td>
<td><p>DS 2.0</p></td>
</tr>
<tr class="row-odd"><td><p>Multi-stream CPU/GPU tracker</p></td>
<td><p>Supports tracking on batched buffers consisting of frames from multiple sources</p></td>
<td><p>DS 2.0</p></td>
</tr>
<tr class="row-even"><td><p>NV12 Input</p></td>
<td></td>
<td><p>DS 2.0</p></td>
</tr>
<tr class="row-odd"><td><p>RGBA Input</p></td>
<td></td>
<td><p>DS 3.0</p></td>
</tr>
<tr class="row-even"><td><p>Configurable GPU device</p></td>
<td><p>User can select GPU for internal scaling/color format conversions and tracking</p></td>
<td><p>DS 2.0</p></td>
</tr>
<tr class="row-odd"><td><p>Dynamic addition/deletion of sources at runtime</p></td>
<td><p>Supports tracking on new sources added at runtime and cleanup of resources when sources are removed</p></td>
<td><p>DS 3.0</p></td>
</tr>
<tr class="row-even"><td><p>Support for user’s choice of low-level library</p></td>
<td><p>Dynamically loads user selected low-level library</p></td>
<td><p>DS 4.0</p></td>
</tr>
<tr class="row-odd"><td><p>Support for batch processing exclusively</p></td>
<td><p>Supports sending frames from multiple input streams to the low-level library</p></td>
<td><p>DS 4.0</p></td>
</tr>
<tr class="row-even"><td><p>Multiple buffer formats as input to low-level library</p></td>
<td><p>Converts input buffer to formats requested by the low-level library, for up to 4 formats per frame</p></td>
<td><p>DS 4.0</p></td>
</tr>
<tr class="row-odd"><td><p>Enabling tracking-id display</p></td>
<td><p>Supports enabling or disabling display of tracking-id</p></td>
<td><p>DS 5.0</p></td>
</tr>
<tr class="row-even"><td><p>Tracking ID reset based on event</p></td>
<td><p>Based on the pipeline event (i.e., GST_NVEVENT_STREAM_EOS and GST_NVEVENT_STREAM_RESET),  the tracking IDs on a particular stream can be reset to start from 0 or new IDs.</p></td>
<td><p>DS 6.0</p></td>
</tr>
<tr class="row-odd"><td><p>Miscellaneous data</p></td>
<td><p>Supports outputting user defined miscellaneous data (including the past-frame data, a list of terminated tracks, etc. ) if the low-level library supports the capability</p></td>
<td><p>DS 6.3</p></td>
</tr>
<tr class="row-even"><td><p>Re-ID tensor output</p></td>
<td><p>Supports outputting objects’ Re-ID features (i.e., embeddings) if the low-level library uses a Re-ID model</p></td>
<td><p>DS 6.3</p></td>
</tr>
<tr class="row-odd"><td><p>Support for NVIDIA’s <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a> based Crop-scaler and DCF-Tracker algorithms in NvDCF tracker (Alpha feature)</p></td>
<td><p>Configuration options provided in NvDCF tracker which allow the user to switch to NVIDIA’s <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a> implementation of Crop-scaler and DCF-Tracker. The user can also configure the compute backend to be used amongst the backends supported by <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a></p></td>
<td><p>DS 6.4</p></td>
</tr>
<tr class="row-even"><td><p>PVA-backend for NvDCF via <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a>’s unified API (Alpha feature)</p></td>
<td><p>Allow PVA-based execution of a significant part of NvDCF on Jetson, resulting in lower GPU utilization</p></td>
<td><p>DS 6.4</p></td>
</tr>
<tr class="row-odd"><td><p>Sub-batching (Alpha feature)</p></td>
<td><p>Supports splitting of a batch of frames in sub-batches which are internally processed in parallel resulting in higher resource utilization. This feature also enables specification of a different config file for each sub-batch.</p></td>
<td><p>DS 6.4</p></td>
</tr>
<tr class="row-even"><td><p>Single-View 3D Tracking (Alpha feature)</p></td>
<td><p>Allow 3D world coordinate system based object tracking when camera/model info (3x4 projection matrix and 3D human model info) is provided for better handling of partial occlusion</p></td>
<td><p>DS 6.4</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="gst-properties">
<span id="nvtracker-gst-properties"></span><h2>Gst Properties<a class="headerlink" href="DS_plugin_gst-nvtracker.html#gst-properties" title="Link to this heading">#</a></h2>
<p>The following table describes the Gst properties of the Gst-nvtracker plugin.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id15">
<caption><span class="caption-text">Gst-nvtracker plugin Gst Properties</span><a class="headerlink" href="DS_plugin_gst-nvtracker.html#id15" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Example Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>tracker-width</p></td>
<td><p>Frame width at which the tracker is to operate, in pixels. (To be a multiple of 32 when visualTrackerType: 1 or reidType is non-zero with useVPICropScaler: 0)</p></td>
<td><p>Integer, 0 to 4,294,967,295</p></td>
<td><p>tracker-width=640</p></td>
</tr>
<tr class="row-odd"><td><p>tracker-height</p></td>
<td><p>Frame height at which the tracker is to operate, in pixels. (To be a multiple of 32 when visualTrackerType: 1 or reidType is non-zero with useVPICropScaler: 0)</p></td>
<td><p>Integer, 0 to 4,294,967,295</p></td>
<td><p>tracker-height=384</p></td>
</tr>
<tr class="row-even"><td><p>ll-lib-file</p></td>
<td><p>Pathname of the low-level tracker library to be loaded by Gst-nvtracker.</p></td>
<td><p>String</p></td>
<td><p>ll-lib-file=­/opt/nvidia/­deepstream/­deepstream/­lib/libnvds_nvmultiobjecttracker.so</p></td>
</tr>
<tr class="row-odd"><td><p>ll-config-file</p></td>
<td><p>Configuration file for the low-level library if needed.</p>
<p>(Alpha feature) A list of configuration files can be specified when the property <cite>sub-batches</cite> is configured.</p>
</td>
<td><p>Path to configuration file OR</p>
<p>A list of paths to configuration files delimited by semicolon</p>
</td>
<td><p>ll-config-file=config_tracker_NvDCF_perf.yml</p>
<p>ll-config-file=config_tracker_NvDCF_perf1.yml;
config_tracker_NvDCF_perf2.yml</p>
</td>
</tr>
<tr class="row-even"><td><p>gpu-id</p></td>
<td><p>ID of the GPU on which device/unified memory is to be allocated, and with which buffer copy/scaling is to be done. (dGPU only.)</p></td>
<td><p>Integer, 0 to 4,294,967,295</p></td>
<td><p>gpu-id=0</p></td>
</tr>
<tr class="row-odd"><td><p>tracking-surface-type</p></td>
<td><p>Set surface stream type for tracking. (default value is 0)</p></td>
<td><p>Integer, ≥0</p></td>
<td><p>tracking-surface-type=0</p></td>
</tr>
<tr class="row-even"><td><p>display-tracking-id</p></td>
<td><p>Enables tracking ID display on OSD.</p></td>
<td><p>Boolean</p></td>
<td><p>display-tracking-id=1</p></td>
</tr>
<tr class="row-odd"><td><p>compute-hw</p></td>
<td><p>Compute engine to use for scaling.</p>
<p>0 - Default</p>
<p>1 - GPU</p>
<p>2 - VIC (Jetson only)</p>
</td>
<td><p>Integer, 0 to 2</p></td>
<td><p>compute-hw=1</p></td>
</tr>
<tr class="row-even"><td><p>tracking-id-reset-mode</p></td>
<td><p>Allow force-reset of tracking ID based on pipeline event. Once tracking ID reset is enabled and such event happens, the lower 32-bit of the tracking ID will be reset to 0</p>
<p>0: Not reset tracking ID when stream reset or EOS event happens</p>
<p>1: Terminate all existing trackers and assign new IDs for a stream when the stream reset happens (i.e., GST_NVEVENT_STREAM_RESET)</p>
<p>2: Let tracking ID start from 0 after receiving EOS event  (i.e., GST_NVEVENT_STREAM_EOS) (Note: Only the lower 32-bit of tracking ID to start from 0)</p>
<p>3: Enable both option 1 and 2</p>
</td>
<td><p>Integer, 0 to 3</p></td>
<td><p>tracking-id-reset-mode=0</p></td>
</tr>
<tr class="row-odd"><td><p>input-tensor-meta</p></td>
<td><p>Use the tensor-meta from Gst-nvdspreprocess if available for tensor-meta-gie-id</p></td>
<td><p>Boolean</p></td>
<td><p>input-tensor-meta=1</p></td>
</tr>
<tr class="row-even"><td><p>tensor-meta-gie-id</p></td>
<td><p>Tensor Meta GIE ID to be used, property valid only if input-tensor-meta is TRUE</p></td>
<td><p>Unsigned Integer, ≥0</p></td>
<td><p>tensor-meta-gie-id=5</p></td>
</tr>
<tr class="row-odd"><td><p>sub-batches (Alpha feature)</p></td>
<td><p>Configures splitting of a batch of frames in sub-batches. There are two ways to configure sub-batches.</p>
<p>First option allows static mapping of each source id to individual sub-batch.</p>
<p>Second option lets the user configure the sub-batch sizes. Mapping of individual streams to sub-batch happens dynamically at runtime.</p>
</td>
<td><p>Option 1 : Semicolon delimited integer array where each number corresponds to source id.</p>
<p>Must include all values from 0 to (batch-size -1) where batch-size is configured in <code class="docutils literal notranslate"><span class="pre">[streammux]</span></code>.</p>
<p>Option 2 : Colon delimited integer array where each number corresponds to size of a sub-batch (i.e. max number of stream a sub-batch can accommodate)</p>
</td>
<td><p>Option 1 : sub-batches=0,1;2,3</p>
<p>In this example, a batch size of 4 is split into two sub-batches where the first sub-batch consists of source ids 0 &amp; 1 and second sub-batch consists of source ids 2 &amp; 3</p>
<p>Option 2 : sub-batches=2:1</p>
<p>The above example indicates that there are two sub-batches, first one can accommodate 2 streams and second one can accommodate 1.</p>
</td>
</tr>
<tr class="row-even"><td><p>sub-batch-err-recovery-trial-cnt (Alpha feature)</p></td>
<td><p>Configure the number of times the plugin can try to recover when the low level tracker in a sub-batch returns with a fatal error.</p>
<p>To recover from the error, the plugin reinitializes the low level tracker library.</p>
</td>
<td><p>Integer,≥-1  where,</p>
<p>-1 corresponds to infinite trials</p>
</td>
<td><p>sub-batch-err-recovery-trial-cnt=3</p></td>
</tr>
<tr class="row-odd"><td><p>user-meta-pool-size</p></td>
<td><p>The size of tracker miscellaneous data buffer pool</p></td>
<td><p>Unsigned Integer, &gt;0</p></td>
<td><p>user-meta-pool-size=32</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="nvdstracker-api-for-low-level-tracker-library">
<h2>NvDsTracker API for Low-Level Tracker Library<a class="headerlink" href="DS_plugin_gst-nvtracker.html#nvdstracker-api-for-low-level-tracker-library" title="Link to this heading">#</a></h2>
<p>A low-level tracker library can be implemented using the API defined in <code class="docutils literal notranslate"><span class="pre">sources/includes/nvdstracker.h</span></code>. Parts of the API refer to <code class="docutils literal notranslate"><span class="pre">sources/includes/nvbufsurface.h</span></code>. The names of API functions and data structures are prefixed with <code class="docutils literal notranslate"><span class="pre">NvMOT</span></code>, which stands for NVIDIA Multi-Object Tracker. Below is the general flow of the API from a low-level library’s perspective:</p>
<ol class="arabic">
<li><p>The first required function is:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_Query</span><span class="w"> </span><span class="p">(</span>
<span class="w">     </span><span class="kt">uint16_t</span><span class="w"> </span><span class="n">customConfigFilePathSize</span><span class="p">,</span>
<span class="w">     </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">pCustomConfigFilePath</span><span class="p">,</span>
<span class="w">     </span><span class="n">NvMOTQuery</span><span class="w"> </span><span class="o">*</span><span class="n">pQuery</span>
<span class="p">);</span>
</pre></div>
</div>
<p>The plugin uses this function to query the low-level library’s capabilities and requirements before it starts any processing sessions (i.e., contexts) with the library. Queried properties include the input frame’s color format (e.g., RGBA or NV12) and memory type (e.g., NVIDIA<sup>®</sup> CUDA<sup>®</sup> device or CPU-mapped NVMM).</p>
<blockquote>
<div></div></blockquote>
<p>The plugin performs this query once during initialization stage, and its results are applied to all contexts established with the low-level library. If a low-level library configuration file is specified, it is provided in the query for the library to consult.
The query reply structure, <code class="docutils literal notranslate"><span class="pre">NvMOTQuery</span></code>, contains the following fields:</p>
<blockquote>
<div><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">NvMOTCompute</span> <span class="pre">computeConfig</span></code>: Report compute targets supported by the library. The plugin currently only echoes the reported value when initiating a context.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">uint8_t</span> <span class="pre">numTransforms</span></code>: The number of color formats required by the low-level library. The valid range for this field is <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">NVMOT_MAX_TRANSFORMS</span></code>. Set this to <code class="docutils literal notranslate"><span class="pre">0</span></code> if the library does not require any visual data.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">0</span></code> does not mean that untransformed data will be passed to the library.</p>
</div>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">NvBufSurfaceColorFormat</span> <span class="pre">colorFormats[NVMOT_MAX_TRANSFORMS]</span></code>: The list of color formats required by the low-level library. Only the first <code class="docutils literal notranslate"><span class="pre">numTransforms</span></code> entries are valid.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NvBufSurfaceMemType</span> <span class="pre">memType</span></code>: Memory type for the transform buffers. The plugin allocates buffers of this type to store color- and scale-converted frames, and the buffers are passed to the low-level library for each frame.
The support is currently limited to the following types:</p>
<p>dGPU:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">NVBUF_MEM_CUDA_PINNED</span>
<span class="n">NVBUF_MEM_CUDA_UNIFIED</span>
</pre></div>
</div>
<p>Jetson:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">NVBUF_MEM_SURFACE_ARRAY</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">bool</span> <span class="pre">supportBatchProcessing</span></code>: True if the low-level library supports the batch processing across multiple streams; otherwise false.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bool</span> <span class="pre">supportPastFrame</span></code>: True if the low-level library supports outputting the past-frame data; otherwise false.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>After the query, and before any frames arrive, the plugin must initialize a context with the low-level library by calling:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_Init</span><span class="w"> </span><span class="p">(</span>
<span class="w">     </span><span class="n">NvMOTConfig</span><span class="w"> </span><span class="o">*</span><span class="n">pConfigIn</span><span class="p">,</span>
<span class="w">     </span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="o">*</span><span class="n">pContextHandle</span><span class="p">,</span>
<span class="w">     </span><span class="n">NvMOTConfigResponse</span><span class="w"> </span><span class="o">*</span><span class="n">pConfigResponse</span>
<span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>The context handle is opaque outside the low-level library. In the batch processing mode, the plugin requests a single context for all input streams. In per-stream processing mode, on the other hand, the plugin makes this call for each input stream so that each stream has its own context.
This call includes a configuration request for the context. The low-level library has an opportunity to:</p>
<blockquote>
<div><ul class="simple">
<li><p>Review the configuration and create a context only if the request is accepted. If any part of the configuration request is rejected, no context is created, and the return status must be set to <code class="docutils literal notranslate"><span class="pre">NvMOTStatus_Error</span></code>. The <code class="docutils literal notranslate"><span class="pre">pConfigResponse</span></code> field can optionally contain status for specific configuration items.</p></li>
<li><p>Pre-allocate resources based on the configuration.</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>In the <code class="docutils literal notranslate"><span class="pre">NvMOTMiscConfig</span></code> structure, the <code class="docutils literal notranslate"><span class="pre">logMsg</span></code> field is currently unsupported and uninitialized.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">customConfigFilePath</span></code> pointer is only valid during the call.</p></li>
</ul>
</div>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>Once a context is initialized, the plugin sends frame data along with detected object bounding boxes to the low-level library whenever it receives such data from upstream. It always presents the data as a batch of frames, although the batch can contain only a single frame in per-stream processing contexts. Note that depending on the frame arrival timings to the tracker plugin, the composition of frame batches could either be a <cite>full batch</cite> (that contains a frame from every stream) or a <cite>partial batch</cite> (that contains a frame from only a subset of the streams). In either case, each batch is guaranteed to contain <cite>at most one frame</cite> from each stream.</p></li>
</ol>
<blockquote>
<div><p>The function call for this processing is:</p>
<blockquote>
<div><div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_Process</span><span class="w"> </span><span class="p">(</span>
<span class="w">     </span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">,</span>
<span class="w">     </span><span class="n">NvMOTProcessParams</span><span class="w"> </span><span class="o">*</span><span class="n">pParams</span><span class="p">,</span>
<span class="w">     </span><span class="n">NvMOTTrackedObjBatch</span><span class="w"> </span><span class="o">*</span><span class="n">pTrackedObjectsBatch</span>
<span class="p">);</span>
</pre></div>
</div>
</div></blockquote>
<p>, where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">pParams</span></code> is a pointer to the input batch of frames to process. The structure contains a list of one or more frames, with at most one frame from each stream. Thus, no two frame entries have the same <code class="docutils literal notranslate"><span class="pre">streamID</span></code>. Each entry of frame data contains a list of one or more buffers in the color formats required by the low-level library, as well as a list of object attribute data for the frame. Most libraries require at most one-color format.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pTrackedObjectsBatch</span></code> is a pointer to the output batch of object attribute data. It is pre-populated with a value for <code class="docutils literal notranslate"><span class="pre">numFilled</span></code>, which is the same as the number of frames included in the input parameters.</p></li>
<li><p>If a frame has no output object attribute data, it is still counted in <code class="docutils literal notranslate"><span class="pre">numFilled</span></code> and is represented with an empty list entry (<code class="docutils literal notranslate"><span class="pre">NvMOTTrackedObjList</span></code>). An empty list entry has the correct <code class="docutils literal notranslate"><span class="pre">streamID</span></code> set and numFilled set to <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The output object attribute data <code class="docutils literal notranslate"><span class="pre">NvMOTTrackedObj</span></code> contains a pointer to the detector object (provied in the input) that is associated with a tracked object, which is stored in <code class="docutils literal notranslate"><span class="pre">associatedObjectIn</span></code>. You must set this to the associated input object only for the frame where the input object is passed in. For a pipeline with PGIE <code class="docutils literal notranslate"><span class="pre">interval=1</span></code>, for example:</p>
<blockquote>
<div><ul class="simple">
<li><p>Frame 0: <code class="docutils literal notranslate"><span class="pre">NvMOTObjToTrack</span></code> <code class="docutils literal notranslate"><span class="pre">X</span></code> is passed in. The tracker assigns it ID 1, and the output object’s <code class="docutils literal notranslate"><span class="pre">associatedObjectIn</span></code> points to <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p></li>
<li><p>Frame 1: Inference is skipped, so there is no input object from detector to be associated with. The tracker finds Object 1, and the output object’s <code class="docutils literal notranslate"><span class="pre">associatedObjectIn</span></code> points to <code class="docutils literal notranslate"><span class="pre">NULL</span></code>.</p></li>
<li><p>Frame 2: <code class="docutils literal notranslate"><span class="pre">NvMOTObjToTrack</span></code> <code class="docutils literal notranslate"><span class="pre">Y</span></code> is passed in. The tracker identifies it as Object 1. The output Object 1 has <code class="docutils literal notranslate"><span class="pre">associatedObjectIn</span></code> pointing to <code class="docutils literal notranslate"><span class="pre">Y</span></code>.</p></li>
</ul>
</div></blockquote>
</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<ol class="arabic" start="4">
<li><p>Depending on the capability of the low-level tracker, there could be some user-defined miscellaneous data to report to tracker plugin. <code class="docutils literal notranslate"><span class="pre">batch_user_meta_list</span></code> in <code class="docutils literal notranslate"><span class="pre">NvDsBatchMeta</span></code> as a user-meta:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_RetrieveMiscData</span><span class="w"> </span><span class="p">(</span>
<span class="w">     </span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">,</span>
<span class="w">     </span><span class="n">NvMOTProcessParams</span><span class="w"> </span><span class="o">*</span><span class="n">pParams</span><span class="p">,</span>
<span class="w">     </span><span class="n">NvMOTTrackerMiscData</span><span class="w"> </span><span class="o">*</span><span class="n">pTrackerMiscData</span>
<span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pParams</span></code> is a pointer to the input batch of frames to process. This structure is needed to check the list of stream ID in the batch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pTrackerMiscData</span></code> is a pointer to the output miscellaneous data for the current batch to be filled by the low-level tracker. The data structure <code class="docutils literal notranslate"><span class="pre">NvMOTTrackerMiscData</span></code> is defined in <code class="docutils literal notranslate"><span class="pre">nvdstracker.h</span></code>.</p></li>
</ul>
</div></blockquote>
<ol class="arabic" start="5">
<li><p>In case that a video stream source is removed on the fly, the plugin calls the following function so that the low-level tracker library can remove it as well. Note that this API is optional and valid only when the batch processing mode is enabled, meaning that it will be executed only when the low-level tracker library has an actual implementation for the API. If called, the low-level tracker library can release any per-stream resource that it may be allocated:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">NvMOT_RemoveStreams</span><span class="w"> </span><span class="p">(</span>
<span class="w">     </span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">,</span>
<span class="w">     </span><span class="n">NvMOTStreamId</span><span class="w"> </span><span class="n">streamIdMask</span>
<span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>When all processing is complete, the plugin calls this function to clean up the context and deallocate its resources:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">NvMOT_DeInit</span><span class="w"> </span><span class="p">(</span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="nvmultiobjecttracker-a-reference-low-level-tracker-library">
<h2><em>NvMultiObjectTracker</em> : A Reference Low-Level Tracker Library<a class="headerlink" href="DS_plugin_gst-nvtracker.html#nvmultiobjecttracker-a-reference-low-level-tracker-library" title="Link to this heading">#</a></h2>
<p>Multi-object tracking (MOT) is a key building block for a large number of intelligent video analytics (IVA) applications that requires spatio-temporal analysis of objects of interest to draw further insights about the objects’ behaviors in long term. Given a set of detected objects from the Primary GIE (PGIE) module on a single or multiple streams and with the APIs defined to work with the tracker plugin, the low-level tracker library is expected to carry out actual multi-object tracking operations to keep persistent IDs to the same objects over time.</p>
<p>DeepStream SDK provides a single reference low-level tracker library, called <cite>NvMultiObjectTracker</cite>, that implements all four low-level tracking algorithms (i.e., IOU, NvSORT, NvDeepSORT, and NvDCF) in a unified architecture. It supports multi-stream, multi-object tracking in the batch processing mode for efficient processing on CPU and GPU (and <a class="reference external" href="https://docs.nvidia.com/vpi/architecture.html#autotoc_md11">PVA</a> for Jetson). The following sections will cover the unified tracker architecture and the details of each reference tracker implementation.</p>
</section>
<section id="unified-tracker-architecture-for-composable-multi-object-tracker">
<h2>Unified Tracker Architecture for Composable Multi-Object Tracker<a class="headerlink" href="DS_plugin_gst-nvtracker.html#unified-tracker-architecture-for-composable-multi-object-tracker" title="Link to this heading">#</a></h2>
<p>In <cite>NvMultiObjectTracker</cite> low-level tracker library, different types of multi-object trackers share common modules when it comes to basic functionalities (e.g., data association, target management, state estimation, etc.), while differing in other core functionalities (e.g., visual tracking for NvDCF and deep association metric for NvDeepSORT). The <cite>NvMultiObjectTracker</cite> library employs a unified architecture to allow the <cite>composition</cite> of a multi-object tracker through configuration by enabling only the modules required for a particular object tracker. The IOU tracker, for example, requires a minimum set of modules that consist of data association and target management modules. On top of that, NvSORT adds a state estimator for more accurate motion estimation &amp; prediction, and NvDeepSORT further introduces a deep Re-ID network to integrate appearance information into data association. Instead of the deep neural network-based Re-ID features in NvDeepSORT, NvDCF employs a Discriminative Correlation Filter (DCF)-based visual tracking module that uses conventional feature descriptors for more efficient tracking. However, NvDCF can still allow the use of Re-ID module for target re-association for longer-term robustness.</p>
<p>The table below summarizes what modules are used to compose each object tracker, showing what modules are shared across different object trackers and how each object tracker differs in module composition:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head" rowspan="2"><p>Tracker Type</p></th>
<th class="head" rowspan="2"><p>State</p>
<p>Estimator</p>
</th>
<th class="head" rowspan="2"><p>Target</p>
<p>Management</p>
</th>
<th class="head" rowspan="2"><p>Visual</p>
<p>Tracker</p>
</th>
<th class="head" colspan="2"><p>Target
Re-Association</p></th>
<th class="head" colspan="3"><p>Data Association Metric</p></th>
</tr>
<tr class="row-even"><th class="head"><p>Spatio-
temporal</p></th>
<th class="head"><p>Re-ID</p></th>
<th class="head"><p>Proximity
&amp; Size</p></th>
<th class="head"><p>Visual
Similarity</p></th>
<th class="head"><p>Re-ID</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>IOU</p></td>
<td></td>
<td><p>O</p></td>
<td></td>
<td></td>
<td></td>
<td><p>O</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>NvSORT</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td></td>
<td></td>
<td></td>
<td><p>O</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>NvDeepSORT</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td></td>
<td></td>
<td></td>
<td><p>O</p></td>
<td></td>
<td><p>O</p></td>
</tr>
<tr class="row-even"><td><p>NvDCF</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td><p>O</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>In the following sections, we will first discuss the general work flow of the NvMultiObjectTracker library and its core modules, and then each type of object trackers in more details with explanations on the config params in each module.</p>
</section>
<section id="workflow-and-core-modules-in-the-nvmultiobjecttracker-library">
<h2>Workflow and Core Modules in The <em>NvMultiObjectTracker</em> Library<a class="headerlink" href="DS_plugin_gst-nvtracker.html#workflow-and-core-modules-in-the-nvmultiobjecttracker-library" title="Link to this heading">#</a></h2>
<p>The input to a low-level tracker library consists of (1) a batch of video frames from a single or multiple streams and (2) a list of detector objects for each video frame. If the detection interval (i.e., <code class="docutils literal notranslate"><span class="pre">interval</span></code> in Primary GIE section) is set larger than 0, the input data to the low-level tracker would have the detector object data only when the inferencing for object detection is performed for a video frame batch (i.e., the <em>inferenced</em> frame batch). For the frame batches where the inference is skipped (i.e., the <em>uninferenced</em> frame batch), the input data would include only the video frames.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>A <em>detector object</em> refers to an object that is detected by the detector in PGIE module, which is provided to the multi-object tracker module as an input.</p></li>
<li><p>A <em>target</em> refers to an object that is being tracked by the object tracker.</p></li>
<li><p>An <em>inferenced</em> frame is a video frame where an inference is carried out for object detection. Since the inference interval can be configured in setting for PGIE and can be larger than zero, the <code class="docutils literal notranslate"><span class="pre">frameNum</span></code> of two consecutive inferenced frames may not be contiguous.</p></li>
</ul>
</div>
</div></blockquote>
<p>For carrying out multi-object tracking operations with the given input data, below are the essential functionalities to be performed. Multithreading is deployed to optimize their performance on CPU.</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>Data association</cite> between the detector objects from a new video frame and the existing targets for the same video stream</p></li>
<li><p><cite>Target management</cite> based on the data association results, including the target state update and the creation and termination of targets</p></li>
</ul>
</div></blockquote>
<p>Depending on the tracker types, there could be some addition processing before data association. For example, NvDeepSORT extracts Re-ID features from all the detector objects and computes the similarity, while NvDCF performs the visual tracker based localization so the targets’ predicted locations in a new frame can be used for data association. More details will be covered in each tracker’s section.</p>
<section id="data-association">
<h3>Data Association<a class="headerlink" href="DS_plugin_gst-nvtracker.html#data-association" title="Link to this heading">#</a></h3>
<p>For data association, various types of similarity metrics are used to calculate the matching score between the detector objects and the existing targets, including:</p>
<ul class="simple">
<li><p>Location similarity (i.e., proximity)</p></li>
<li><p>Bounding box size similarity</p></li>
<li><p>Re-ID feature similarity  (specific to NvDeepSORT tracker)</p></li>
<li><p>Visual appearance similarity (specific to NvDCF tracker)</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For the proximity between detector objects and targets, IOU is a typical metric that is widely used, but it also depends on the size similarity between them. The similarity of the box size between two objects can be used explicitly, which is calculated as the ratio of the size of the smaller box over the larger one.</p>
<p>The total association score for a pair of detector object and target is the weighted sum of all the metrics:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[totalScore=w_1*IOU+w_2*sizeSimilarity+w_3*reidSimilarity+w_4*visualSimilarity\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(w_i\)</span> is the weight for each metric set in config file. Users can also set a minimum threshold for each similarity and the total score.</p>
<p>During the matching, a detector object is associated with a target that belongs to the same class by default to minimize the false matching. However, this can be disabled by setting <code class="docutils literal notranslate"><span class="pre">checkClassMatch:</span> <span class="pre">0</span></code>, allowing objects can be associated regardless of their object class IDs. This can be useful when employing a detector like YOLO, which can detect many classes of objects, where there could be false classification on the same object over time.</p>
<p>Regarding the matching algorithm, users can set <code class="docutils literal notranslate"><span class="pre">associationMatcherType</span></code> as <code class="docutils literal notranslate"><span class="pre">0</span></code> to employ an efficient greedy algorithm for optimal bipartite matching with similarity metrics defined above, or <code class="docutils literal notranslate"><span class="pre">1</span></code> for a newly introduced method named <cite>cascaded data association</cite> for higher accuracy.
The cascaded data association consists of multi-stage matching, assigning different priorities and similarity metrics based on detection and target confidence. Detector objects are split into two sets, confirmed (confidence between [<code class="docutils literal notranslate"><span class="pre">tentativeDetectorConfidence</span></code>, 1.0]) and tentative (confidence between [<code class="docutils literal notranslate"><span class="pre">minDetectorConfidence</span></code>, <code class="docutils literal notranslate"><span class="pre">tentativeDetectorConfidence</span></code>]). Then three stage matching are performed sequentially:</p>
<ul class="simple">
<li><p>Confirmed detections and validated (both active and inactive) targets</p></li>
<li><p>Tentative detections and active targets left</p></li>
<li><p>Confirmed detections left and tentative targets</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The first stage uses the joint-similarity metrics defined above, while the later two stages only considers the IOU similarity, because proximity can be a more reliable metric than visual similarity or Re-ID when the detection confidence is low due to, say, partial occlusions or noise. Each stage takes different sets of bboxes as candidates and uses the efficient greedy algorithm for matching. The matched pairs are produced from each stage and combined together.</p>
<p>The output of the data association module consists of three sets of objects/targets:</p>
<ul class="simple">
<li><p>The unmatched detector objects</p></li>
<li><p>The matched pairs of the detector objects and the existing targets</p></li>
<li><p>The unmatched targets</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The unmatched detector objects are among the objects detected by a PGIE detector, yet not associated with any of the existing targets. An unmatched detector object is considered as a newly observed object that needs to be tracked, unless they are determined to be duplicates to any of the existing target. If the maximum IOU score of a new detector object to any of the existing targets is lower than <code class="docutils literal notranslate"><span class="pre">minIouDiff4NewTarget</span></code>, a new target tracker would be created to track the object since it is not a duplicate to an existing target.</p>
</section>
<section id="target-management-and-error-handling">
<h3>Target Management and Error Handling<a class="headerlink" href="DS_plugin_gst-nvtracker.html#target-management-and-error-handling" title="Link to this heading">#</a></h3>
<p>Although a new object is detected by the detector (i.e., a detector object), there is a possibility that this may be a false positive. To suppress such noise in detection, the <cite>NvMultiObjectTracker</cite> tracker library employs a technique called <strong>Late Activation</strong>, where a newly detected object is examined for a period of time and activated for long-term tracking <cite>only if</cite> it survives such a period. To be more specific, whenever a new object is detected, a new tracker is created to track the object, but the target is initially put into the <cite>Tentative</cite> mode, which is a probationary period, whose length is defined by <code class="docutils literal notranslate"><span class="pre">probationAge</span></code> under <code class="docutils literal notranslate"><span class="pre">TargetManagement</span></code> section of the config file. During this probationary period, the tracker output will not be reported to the downstream, since the target is not validated yet; however, those unreported tracker output data (i.e., <cite>the past-frame data</cite>) are stored within the low-level tracker for later report.</p>
<p>The same target may be detected for the next frame; however, there could be <cite>false negative</cite> by the detector (i.e., missed detection), resulting in a unsuccessful data association to the target. The NvMultiObjectTracker library employs another technique called <strong>Shadow Tracking</strong>, where a target is still being tracked in the background for a period of time even when the target is <em>not</em> associated with a detector object. Whenever a target is not associated with a detector object for a given time frame, an internal variable of the target called <cite>shadowTrackingAge</cite> is incremented. Once the target is associated with a detector object, then <cite>shadowTrackingAge</cite> will be reset to zero.</p>
<p>If the target is in the Tentative mode and the <cite>shadowTrackingAge</cite> reaches <code class="docutils literal notranslate"><span class="pre">earlyTerminationAge</span></code> specified in the config file, the target will be terminated prematurely (which is referred to as <strong>Early Termination</strong>). If the target is not terminated during the Tentative mode and successfully assocciated with a detector object, the target is <em>activated</em> and put into the <cite>Active</cite> mode, starting to report the tracker outputs to the downstream. If the past-frame data is enabled, the tracked data during the Tentative mode will be reported as well, since they were not reported yet. Once a target is activated (i.e., in Active mode), if the target is not associated for a given time frame (or the tracker confidence gets lower than a threshold), it will be put into the <cite>Inactive</cite> mode, and its <cite>shadowTrackingAge</cite> will be incremented, yet still be tracked in the background. However, the target will be terminated if the <cite>shadowTrackingAge</cite> exceeds <code class="docutils literal notranslate"><span class="pre">maxShadowTrackingAge</span></code>.</p>
<p>The state transitions of a target tracker are summarized in the following diagram:</p>
<img alt="Gst-nvtracker" class="align-center" src="../_images/DS_NvMultiObjectTracker_state_transition.png" />
<p>The NvMultiObjectTracker library can generate a unique ID to some extent. If enabled by setting <code class="docutils literal notranslate"><span class="pre">useUniqueID:</span> <span class="pre">1</span></code>, each video stream will be assigned a 32-bit long random number during the initialization stage. All the targets created from the same video stream will have the same upper 32-bit of the <code class="docutils literal notranslate"><span class="pre">uint64_t</span></code>-type target ID set by the per-stream random number. In the meantime, the lower 32-bit of the target ID starts from 0. The randomly generated upper 32-bit number allows the target IDs from a particular video stream to increment from a random position in the possible ID space. If disabled (i.e., <code class="docutils literal notranslate"><span class="pre">useUniqueID:</span> <span class="pre">0</span></code>, which is the default value), both the upper and lower 32-bit will start from 0, resulting in the target ID to be incremented from 0 for every run.</p>
<p>Note that the incrementation of the lower 32-bit of the target ID is done across the whole video streams in the same NvMultiObjectTracker library instantiation. Thus, even if the unique ID generation is disabled, the tracker IDs will be unique for the same pipeline run. If the unique ID generation is disabled, and if there are three objects for Stream 1 and two objects for Stream 2, for example, the target IDs will be assigned from 0 to 4 (instead of 0 to 2 for Stream 1 and 0 to 1 for Stream 2) as long as the two streams are being processed by the same library instantiation.</p>
<p><code class="docutils literal notranslate"><span class="pre">preserveStreamUpdateOrder</span></code> controls whether to use single or multiple threads to update targets. If it is enabled, new IDs are generated sequentially following input stream ID order in each batch using a single thread, i.e. the objects for Stream 1 and 2 will have IDs from 0 to 2 and 3 to 4 respectively. By default, this option is disabled so target management is done with multi-threads to enable better performance but the ID order is not preserved. If the user needs consistent IDs over multiple runs for the same video source, please set <code class="docutils literal notranslate"><span class="pre">preserveStreamUpdateOrder:</span> <span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">batched-push-timeout=-1</span></code> in deepstream-app config.</p>
<p>The NvMultiObjectTracker library <cite>pre-allocates</cite> all the GPU memories during initialization based on:</p>
<ul class="simple">
<li><p>The number of streams to be processed</p></li>
<li><p>The maximum number of objects to be tracked per stream (denoted as <code class="docutils literal notranslate"><span class="pre">maxTargetsPerStream</span></code>)</p></li>
</ul>
<p>Thus, the CPU/GPU memory usage by the NvMultiObjectTracker library is almost linearly proportional to the total number of objects being tracked, which is <cite>(number of video streams) × (maxTargetsPerStream)</cite>, except the scratch memory space used by dependent libraries (such as cuFFT™, TensorRT™, etc.). Thanks to the pre-allocation of all the necessary memory, the NvMultiObjectTracker library is not expected to have memory growth during long-term run even when the number of objects increases over time.</p>
<p>Once the number of objects being tracked reaches the configured maximum value (i.e., <code class="docutils literal notranslate"><span class="pre">maxTargetsPerStream</span></code>), any new objects will be discarded until some of the existing targets are terminated. Note that the number of objects being tracked includes the targets that are being tracked in the shadow tracking mode. Therefore, NVIDIA recommends that users set <code class="docutils literal notranslate"><span class="pre">maxTargetsPerStream</span></code> large enough to accommodate the maximum number of objects of interest that may appear in a frame, as well as the objects that may have been tracked from the past frames in the shadow tracking mode.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">minDetectorConfidence</span></code> property under <code class="docutils literal notranslate"><span class="pre">BaseConfig</span></code> section in a low-level tracker config file sets the confidence level below which the detector objects are filtered out.</p>
</section>
<section id="state-estimation">
<h3>State Estimation<a class="headerlink" href="DS_plugin_gst-nvtracker.html#state-estimation" title="Link to this heading">#</a></h3>
<p>The NvMultiObjectTracker library employs two types of state estimators, both of which are based on Kalman Filter (KF): Simple-bbox KF, Regular-bbox KF, and Simple-location KF. The <cite>Simple-bbox KF</cite> has <code class="docutils literal notranslate"><span class="pre">6</span></code> states defined, which are <code class="docutils literal notranslate"><span class="pre">{x,</span> <span class="pre">y,</span> <span class="pre">w,</span> <span class="pre">h,</span> <span class="pre">dx,</span> <span class="pre">dy}</span></code>, where <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> indicate the coordinates of the top-left corner of a target bbox, while <code class="docutils literal notranslate"><span class="pre">w</span></code> and <code class="docutils literal notranslate"><span class="pre">h</span></code> the width and the height of the bbox, respectively. <code class="docutils literal notranslate"><span class="pre">dx</span></code> and <code class="docutils literal notranslate"><span class="pre">dy</span></code> denote the velocity of <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> states. The <cite>Regular-bbox KF</cite>, on the other hand, have <code class="docutils literal notranslate"><span class="pre">8</span></code> states defined, which are <code class="docutils literal notranslate"><span class="pre">{x,</span> <span class="pre">y,</span> <span class="pre">w,</span> <span class="pre">h,</span> <span class="pre">dx,</span> <span class="pre">dy,</span> <span class="pre">dw,</span> <span class="pre">dh}</span></code>, where <code class="docutils literal notranslate"><span class="pre">dw</span></code> and <code class="docutils literal notranslate"><span class="pre">dh</span></code> are the velocity of <code class="docutils literal notranslate"><span class="pre">w</span></code> and <code class="docutils literal notranslate"><span class="pre">h</span></code> states and the rest is the same as the Simple-bbox KF. The <cite>Simple-location KF</cite> has <code class="docutils literal notranslate"><span class="pre">4</span></code> states only, which are <code class="docutils literal notranslate"><span class="pre">{x,</span> <span class="pre">y,</span> <span class="pre">dx,</span> <span class="pre">dy}</span></code>. Unlike the the two KFs that estimate the bbox attributes, note that the Simple-location KF is meant to estimate the object location in either 2D camera image plane or 3D world ground plane.</p>
<p>All the Kalman Filter types employ a constant velocity model for generic use. The measurement vector for the Simple-bbox and Regular-bbox KF is defined as <code class="docutils literal notranslate"><span class="pre">{x,</span> <span class="pre">y,</span> <span class="pre">w,</span> <span class="pre">h}</span></code>, which represents the bbox attributes, while that for the Simple-location KF is defined as <code class="docutils literal notranslate"><span class="pre">{x,</span> <span class="pre">y}</span></code>. There is an option to use bbox aspect ratio <code class="docutils literal notranslate"><span class="pre">a</span></code> and its velocity <code class="docutils literal notranslate"><span class="pre">da</span></code> instead of <code class="docutils literal notranslate"><span class="pre">w</span></code> and <code class="docutils literal notranslate"><span class="pre">dw</span></code> when <code class="docutils literal notranslate"><span class="pre">useAspectRatio</span></code> is enabled, which is specially used by NvDeepSORT. In case the state estimator is used for a generic use case (like in the NvDCF tracker), the process noise variance for <code class="docutils literal notranslate"><span class="pre">{x,</span> <span class="pre">y}</span></code>, <code class="docutils literal notranslate"><span class="pre">{w,</span> <span class="pre">h}</span></code>, and <code class="docutils literal notranslate"><span class="pre">{dx,</span> <span class="pre">dy,</span> <span class="pre">dw,</span> <span class="pre">dh}</span></code> can be configured by <code class="docutils literal notranslate"><span class="pre">processNoiseVar4Loc</span></code>, <code class="docutils literal notranslate"><span class="pre">processNoiseVar4Size</span></code>, and <code class="docutils literal notranslate"><span class="pre">processNoiseVar4Vel</span></code>, respectively.</p>
<p>When a visual tracker module is enabled (like in the NvDCF tracker), there could be two different measurements from the state estimator’s point of view: (1) the bbox (or location) from the detector at PGIE and (2) the bbox (or location) from the tracker’s localization. This is because the NvDCF tracker module is capable of localizing targets using its own learned filter. The measurement noise variance for these two different types of measurements can be configured by <code class="docutils literal notranslate"><span class="pre">measurementNoiseVar4Detector</span></code> and <code class="docutils literal notranslate"><span class="pre">measurementNoiseVar4Tracker</span></code>. These parameters are expected to be tuned or optimized based on the detector’s and the tracker’s characteristics for better measurement fusion.</p>
<p>The usage of the state estimator in the NvDeepSORT tracker slightly differs from that for the aforementioned generic use case in that it is basically a <em>Regular KF</em>, yet with a couple of differences as per the original paper and the implementation (Check the references in <a class="reference internal" href="DS_plugin_gst-nvtracker.html#nvdeepsort-tracker">NvDeepSORT Tracker</a> section):</p>
<ul class="simple">
<li><p>Use of the aspect ratio <code class="docutils literal notranslate"><span class="pre">a</span></code> and the height <code class="docutils literal notranslate"><span class="pre">h</span></code> (instead of <code class="docutils literal notranslate"><span class="pre">w</span></code> and <code class="docutils literal notranslate"><span class="pre">h</span></code>) to estimate the bbox size</p></li>
<li><p>The process and measurement noises that are proportional to the bounding box height (instead of constant values)</p></li>
</ul>
<p>To allow these differences, the state estimator module in the NvMultiObjectTracker library has a set of additional config parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">useAspectRatio</span></code> to enable the use of <code class="docutils literal notranslate"><span class="pre">a</span></code> (instead of <code class="docutils literal notranslate"><span class="pre">w</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">noiseWeightVar4Loc</span></code> and <code class="docutils literal notranslate"><span class="pre">noiseWeightVar4Vel</span></code> as the proportion coefficients for the measurement and velocity noise, respectively</p></li>
</ul>
<p>Note that if these two parameters are set, the fixed process noise and measurement noise parameters for the generic use cases will be ignored.</p>
</section>
<section id="object-re-identification">
<h3>Object Re-Identification<a class="headerlink" href="DS_plugin_gst-nvtracker.html#object-re-identification" title="Link to this heading">#</a></h3>
<p>Re-identification (Re-ID) uses TensorRT™-accelerated deep neural networks to extract unique feature vectors from detected objects that are robust to spatial-temporal variance and occlusion. It has two use-cases in <em>NvMultiObjectTracker</em>: (1) In NvDeepSORT, the Re-ID similarity is used for data association of objects over consecutive frames.; (2) In target re-association (which will be described in more detail in the following section), the Re-ID features of targets are extracted and kept, so that they can be used for re-association with the same target if they are seemingly lost. <code class="docutils literal notranslate"><span class="pre">reidType</span></code> selects the mode for each aforementioned use-case.</p>
<p>In the Re-ID module, the detector objects are cropped and resized into the configured input size of the Re-ID network. The parameter <code class="docutils literal notranslate"><span class="pre">keepAspc</span></code> controls whether the object’s aspect ratio is preserved after cropping. Then NVIDIA TensorRT™ creates an engine from the network, which processes the input in batches and outputs a fixed-dimensional vector for each detector object as the Re-ID feature. The cosine similarity function requires each feature’s L2 norm normalized to 1. Check <a class="reference internal" href="DS_plugin_gst-nvtracker.html#re-id-feature-output">Re-ID Feature Output</a> on how to retrieve these features in the tracker plugin and downstream modules. For each target, a gallery of its Re-ID features in most recent frames are kept internally. The size of the feature gallery can be set by <code class="docutils literal notranslate"><span class="pre">reidHistorySize</span></code>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">config_tracker_NvDeepSORT.yml</span></code> and <code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_accuracy.yml</span></code> configs use ReIdentificationNet by default, which is a ResNet-50 Re-ID network in NVIDIA TAO toolkit on <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/reidentificationnet">NGC</a>. Users need to follow instructions in <a class="reference internal" href="DS_plugin_gst-nvtracker.html#setup-sample-re-id-models">Setup Sample Re-ID Models</a> to setup, or check <a class="reference internal" href="DS_plugin_gst-nvtracker.html#customize-re-id-model">Customize Re-ID Model</a> for more information on adding a custom Re-ID model for object tracking with different architectures and datasets.</p>
</div>
</div></blockquote>
<p>The Re-ID similarity between a detector object and a target is the cosine similarity between the detector object’s Re-ID feature and its nearest neighbor in the target’s featue gallery, whose value is in range <code class="docutils literal notranslate"><span class="pre">[0.0,</span> <span class="pre">1.0]</span></code>. Specifically, each Re-ID feature in the target’s gallery takes the dot product with the detector object’s Re-ID feature. The maximum of all the dot products is the similarity score, i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[score_{ij}=\max_{k}(feature\_det_{i}\cdot feature\_track_{jk})\]</div>
</div></blockquote>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\cdot\)</span> denotes the dot product.</p></li>
<li><p><span class="math notranslate nohighlight">\(feature\_det_{i}\)</span> denotes the i-th detector object’s feature.</p></li>
<li><p><span class="math notranslate nohighlight">\(feature\_track_{jk}\)</span> denotes the k-th Re-ID feature in the j-th target’s feature gallery. <span class="math notranslate nohighlight">\(k\)</span> =[1, <code class="docutils literal notranslate"><span class="pre">reidHistorySize</span></code>].</p></li>
</ul>
<p>The Re-ID has a spatial-temporal constraint. If an object moves out of frame or gets occluded beyond <code class="docutils literal notranslate"><span class="pre">maxShadowTrackingAge</span></code>, it will be assigned a new ID even if it returns into the frame.</p>
<p>The extracted Re-ID features (i.e., embeddings) can be exported to the metadata, which is explained in a separate section in <a class="reference internal" href="DS_plugin_gst-nvtracker.html#re-id-feature-output">Re-ID Feature Output</a>.</p>
</section>
<section id="target-re-association">
<h3>Target Re-Association<a class="headerlink" href="DS_plugin_gst-nvtracker.html#target-re-association" title="Link to this heading">#</a></h3>
<p>The <em>target re-association</em> algorithm enhances the long-term robustness of multi-object tracking by jointly using the Re-ID and spatio-temporal (i.e., motion) features. It addresses one of the major tracking failure cases that occurs in the situation where objects undergo partial- or full-occlusions in a gradual or abrupt manner. During this course of action, the detector at PGIE module may capture only some part of the objects (due to partial visibility), resulting in ill-sized, ill-centered boxes on the target. Later, the target cannot be associated with the object appearing again due to the size and location prediction errors, potentially causing tracking failures and ID switches. Such a re-association problem can typically be handled as a post-processing; however, for real-time analytics applications, this is often expected to be handled seamlessly as a part of the real-time multi-object tracking.</p>
<p>The target re-association takes advantage of the <em>Late Activation</em> and <em>Shadow Tracking</em> in target management module. It tries to associate the newly-appeared targets with previously lost targets based on motion and Re-ID similarity in a seamless, real-time manner by the following steps:</p>
<p><strong>Tracklet Prediction</strong>: Whenever an existing target is not associated with a detector object for a prolonged period (same as <code class="docutils literal notranslate"><span class="pre">probationAge</span></code>), it is considered that the target is lost. While the visual tracker module keeps track of the target in the shadow tracking mode, a length of the predicted tracklet (configured by <code class="docutils literal notranslate"><span class="pre">trajectoryProjectionLength</span></code>) is generated using some of the recently matched tracklet points (whose length is set by <code class="docutils literal notranslate"><span class="pre">prepLength4TrajectoryProjection</span></code>) and stored into an internal database until it is matched again with a detector object or re-associated with another target.</p>
<p><strong>Re-ID Feature Extraction</strong>: Before a target is lost, the Re-ID network extracts its Re-ID feature with the frame interval of <code class="docutils literal notranslate"><span class="pre">reidExtractionInterval</span></code> and stores them in the feature gallery. These features will be used to identify target re-appearance in the tracklet matching stage.</p>
<p><strong>Target ID Acquisition</strong>: When a new target is instantiated, its validity is examined for a few frames (i.e., <code class="docutils literal notranslate"><span class="pre">probationAge</span></code>) and a target ID is assigned only if validated (i.e., Late Activation), after which the target state report starts. During the target ID acquisition, the new target is examined if it matches with one of the predicted <cite>tracklets</cite> from the existing targets in the internal database where the aforementioned predicted <cite>tracklets</cite> are stored. If matched, it would mean that the new target is actually the re-appearance of a disappeared target in the past. Then, the new target is re-associated with the existing target and its <cite>tracklet</cite> is fused into that as well. Otherwise, a new target ID is assigned.</p>
<p><strong>Tracklet Matching</strong>: During the <cite>tracklet</cite> matching process in the previous step, the valid candidate <cite>tracklets</cite> are queried from the database based on the feasible time window configured by <code class="docutils literal notranslate"><span class="pre">maxTrackletMatchingTimeSearchRange</span></code>. For the new target and each candidate, both the motion and Re-ID similarity are taken into account for tracklet matching. The motion similarity is the average IOU along the <cite>tracklet</cite> with various criteria including the minimum average IOU score (i.e., <code class="docutils literal notranslate"><span class="pre">minTrackletMatchingScore</span></code>), maximum angular difference in motion (i.e., <code class="docutils literal notranslate"><span class="pre">maxAngle4TrackletMatching</span></code>), minimum speed similarity (i.e., <code class="docutils literal notranslate"><span class="pre">minSpeedSimilarity4TrackletMatching</span></code>), and minimum bbox size similarity (i.e., <code class="docutils literal notranslate"><span class="pre">minBboxSizeSimilarity4TrackletMatching</span></code>) computed by a Dynamic Time Warping (DTW)-like algorithm. The Re-ID similarity is the cosine distance between the new target’s Re-ID feature and its nearest neighbor in the candidate’s feature gallery. The total similarity score is the weighted sum of both metrics:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[totalScore=w_1*IOU+w_2*reidSimilarity\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(w_i\)</span> is the weight for each metric set in config file. Users can also set a minimum threshold for each similarity and the total score.</p>
<p><strong>Tracklet Fusion</strong>: Once two <cite>tracklets</cite> are associated, they are fused together to generate one smooth <cite>tracklet</cite> based on the matching status with detector and the confidence at each point.</p>
<p><code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_accuracy.yml</span></code> provides an example to enable this feature. Since Re-ID is computationally expensive, users may choose to increase <code class="docutils literal notranslate"><span class="pre">reidExtractionInterval</span></code> to improve performance or set the parameters like below (i.e., disabling Re-ID feature extraction) to use motion-only target re-association without Re-ID.</p>
<blockquote>
<div><blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">TrajectoryManagement</span><span class="p">:</span>
<span class="w">  </span><span class="nt">useUniqueID</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w">      </span><span class="c1"># Use 64-bit long Unique ID when assignining tracker ID. Default is [true]</span>
<span class="w">  </span><span class="nt">enableReAssoc</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w">    </span><span class="c1"># Enable Re-Assoc</span>
<span class="w">  </span><span class="nt">minMatchingScore4Overall</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w">    </span><span class="c1"># min matching score for overall</span>
<span class="w">  </span><span class="nt">minTrackletMatchingScore</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5644</span><span class="w">    </span><span class="c1"># min tracklet similarity score for re-assoc</span>
<span class="w">  </span><span class="nt">matchingScoreWeight4TrackletSimilarity</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span><span class="w">    </span><span class="c1"># weight for tracklet similarity score</span>
<span class="w">  </span><span class="nt">minTrajectoryLength4Projection</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">36</span><span class="w">    </span><span class="c1"># min trajectory length required to make projected trajectory</span>
<span class="w">  </span><span class="nt">prepLength4TrajectoryProjection</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">50</span><span class="w">    </span><span class="c1"># the length of the trajectory during which the state estimator is updated to make projections</span>
<span class="w">  </span><span class="nt">trajectoryProjectionLength</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">94</span><span class="w">    </span><span class="c1"># the length of the projected trajectory</span>
<span class="w">  </span><span class="nt">maxAngle4TrackletMatching</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">106</span><span class="w">    </span><span class="c1"># max angle difference for tracklet matching [degree]</span>
<span class="w">  </span><span class="nt">minSpeedSimilarity4TrackletMatching</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0967</span><span class="w">    </span><span class="c1"># min speed similarity for tracklet matching</span>
<span class="w">  </span><span class="nt">minBboxSizeSimilarity4TrackletMatching</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5577</span><span class="w">    </span><span class="c1"># min bbox size similarity for tracklet matching</span>
<span class="w">  </span><span class="nt">maxTrackletMatchingTimeSearchRange</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20</span><span class="w">    </span><span class="c1"># the search space in time for max tracklet similarity</span>
<span class="w">  </span><span class="nt">trajectoryProjectionProcessNoiseScale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0100</span><span class="w">    </span><span class="c1"># trajectory projector&#39;s process noise scale w.r.t. state estimator</span>
<span class="w">  </span><span class="nt">trajectoryProjectionMeasurementNoiseScale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span><span class="w">    </span><span class="c1"># trajectory projector&#39;s measurement noise scale w.r.t. state estimator</span>
<span class="w">  </span><span class="nt">trackletSpacialSearchRegionScale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.2598</span><span class="w">    </span><span class="c1"># the search region scale for peer tracklet</span>

<span class="nt">ReID</span><span class="p">:</span>
<span class="w">  </span><span class="nt">reidType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w">    </span><span class="c1"># The type of reid among { DUMMY=0, NvDEEPSORT=1, Reid based reassoc=2, both NvDEEPSORT and reid based reassoc=3}</span>
</pre></div>
</div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Target re-association can be effective only when the state estimator is enabled, otherwise the <cite>tracklet</cite> prediction will not be made properly. The parameters provided above is tuned for PeopleNet v2.6.2, and it may not work as expected for other types of detectors.</p>
</div>
</div></blockquote>
</section>
<section id="bounding-box-unclipping">
<h3>Bounding-box Unclipping<a class="headerlink" href="DS_plugin_gst-nvtracker.html#bounding-box-unclipping" title="Link to this heading">#</a></h3>
<p>Another small experimental feature is the bounding box unclipping. If a target is fully visible within the field-of-view (FOV) of the camera but starts going out of the FOV, the target would be partially visible and the bounding box (i.e., <cite>bbox</cite>) may capture only a part of the target (i.e., clipped by the FOV) until it fully exits the scene. If it is expected that the size of the <cite>bbox</cite> doesn’t change much around the border of the video frame, the full <cite>bbox</cite> can be estimated beyond the FOV limit using the <cite>bbox</cite> size estimated when the target was fully visible. This feature can be enabled by setting <code class="docutils literal notranslate"><span class="pre">enableBboxUnClipping:</span> <span class="pre">1</span></code> under <code class="docutils literal notranslate"><span class="pre">TargetManagement</span></code> module in the low-level config file.</p>
</section>
<section id="single-view-3d-tracking-alpha">
<h3>Single-View 3D Tracking (Alpha)<a class="headerlink" href="DS_plugin_gst-nvtracker.html#single-view-3d-tracking-alpha" title="Link to this heading">#</a></h3>
<p>As mentioned earlier, partial occlusion is one of the most challenging problems that object trackers have to deal with and often lead to tracking failures. If the object detectors capture only the visible part of the object (which is often the case), the partial occlusion would cause the detection bboxes to have abrupt or gradual changes in attributes in terms of bbox location, size, aspect ratio, confidence, and most importantly the visual appearance within the bbox. Considering the object trackers rely on the bbox attributes as spatio-temporal measure and the visual appearance (e.g., ReID embedding) extracted within the bbox as visual similarity measure, such changes in bbox attributes is a major source of tracking failures, resulting in more frequent ID switches.</p>
<p>To tackle these challenging problems, DeepStream SDK introduced a new feature called the <cite>Single-View 3D Tracking (SV3DT)</cite> that allows the object tracking to be carried out in a 3D world coordinate system (instead of the 2D camera image plane) when (1) a 3x4 projection matrix and (2) a 3D model info are provided for a video stream in a camera info file like below.</p>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># camInfo-01.yml</span>

<span class="c1"># The 3x4 camera projection matrix (in row-major):</span>
<span class="c1">#    996.229 -202.405  -9.121 -1.185</span>
<span class="c1">#    105.309  478.174 890.944 1.743</span>
<span class="c1">#    -0.170   -0.859   0.481  -1085.484</span>
<span class="nt">projectionMatrix_3x4</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">996.229</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-202.405</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-9.121</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1.185</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">105.309</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">478.174</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">890.944</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.743</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-0.170</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-0.859</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.481</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-1085.484</span>

<span class="c1"># The cylindrical human model</span>
<span class="nt">modelInfo</span><span class="p">:</span>
<span class="w">  </span><span class="nt">height</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">250.0</span>
<span class="w">  </span><span class="nt">radius</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30.0</span>
</pre></div>
</div>
</div></blockquote>
<p>There are two options (<code class="docutils literal notranslate"><span class="pre">projectionMatrix_3x4</span></code> and <code class="docutils literal notranslate"><span class="pre">projectionMatrix_3x4_w2p</span></code>) in which users can provide corresponding 3x4 camera projection matrices to support different usecase. Please refer to <a class="reference internal" href="DS_plugin_gst-nvtracker.html#the-3x4-camera-projection-matrix">The 3x4 Camera Projection Matrix</a> section for more details.</p>
<p>Note that there are a few assumptions that this algorithm requires:</p>
<ul class="simple">
<li><p>A human is modeled as a cylinder with height and radius in 3D world coordinate system, and the height and radius are provided as 3D model info.</p></li>
<li><p>A 3x4 projection matrix (that transforms a 3D world coordinate point to a 2D camera image coordinate point) is provided for a video stream or a camera.</p></li>
<li><p>Video streams are captured from cameras that are mounted higher than the human height.</p></li>
</ul>
<p>The third assumption ensures that when a human is partially-occluded, the head is still visible, allowing us to use the top bbox edge as an anchor, which we will touch upon shortly.</p>
<p>For each detection bounding box for a person, SV3DT algorithm tries to fit the 3D human model to the detection bbox in such a way that the bounding box of the projected 3D human model from the world coordinate system to the camera image plane matches with the detection bbox.</p>
<p>A figure below shows how cylindrical 3D human models can be fitted to the input detection bboxes.</p>
<img alt="Showing 3D human model fitting to un-occluded cases" class="align-center" src="../_images/DS_NvMultiObjectTracker-SV3DT-01.png" />
<p>In the cases where a person is partially occluded, the top edge of the detection bbox is used as an anchor to align the bbox of the projected 3D human model. Once aligned, we can recover the full-body bbox using the projected 3D human model, as if the person is not occluded. Therefore, if SV3DT is enabled, the input detection bboxes are always first <em>recovered</em> to the full-body bboxes based on the provided 3D model info especially when the input detection bboxes capture only the visible part of the person due to partial occlusions. This greatly enhances the multi-object tracking accuracy and robustness, since the bbox attributes are not altered during the course of partial occlusions.</p>
<p>An animated image below shows how cylindrical 3D human models can be fitted into the input detection bboxes when the persons are partially occluded. The thin, gray bboxes on the persons indicate the input detection bboxes, which capture only the visible part of the objects. The figure demonstrates that the SV3DT algorithm is still able to estimate the accurate foot location of each person. The person trajectories are drawn based on the estimated foot locations, allowing robust spatio-temporal behavior analytics of persons in the scene despite varying degree of occlusions. Some of the persons in this example are barely seen only on the head and shoulders, but they are being successfully tracked as if not occluded at all.</p>
<img alt="Showing 3D human model fitting to partially-occluded cases" class="align-center" src="../_images/DS_NvMultiObjectTracker-SV3DT-02.gif" />
<p>As a derived metric, the ratio between the bbox for the visible part and the bbox for the projected 3D human model can be considered as an approximated <cite>visibility</cite> of the object, which could be a useful information.</p>
<p>Users can still get access to the corresponding detection bboxes by checking out <code class="docutils literal notranslate"><span class="pre">detector_bbox_info</span></code> in <a class="reference external" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_metadata.html#new-metadata-fields">NvDsObjectMeta</a>.</p>
<p>To enable SV3DT feature, we introduced a new section in tracker config files, <code class="docutils literal notranslate"><span class="pre">ObjectModelProjection</span></code>, like below:</p>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">ObjectModelProjection</span><span class="p">:</span>
<span class="w">  </span><span class="nt">cameraModelFilepath</span><span class="p">:</span><span class="w">  </span><span class="c1"># In order of the source streams</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;camInfo-01.yml&#39;</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;camInfo_02.yml&#39;</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span>
</pre></div>
</div>
</div></blockquote>
<p>Every camera view is different, so the 3x4 projection matrix is supposed to be unique to each camera. Therefore, the camera info file (e.g., <code class="docutils literal notranslate"><span class="pre">camInfo-01.yml</span></code>) is to be provided for each stream, which include the 3x4 projection matrix and the model info that are shown at the beginning of the section.</p>
<p>Once a 3D human model corresponding to an input detection bbox is estimated and located in the world coordinate system, the foot location (i.e., the center of the base of the cylindrical model) of a person on the world ground plane is what we want to keep estimating because it is a physical state that better follows the motion dynamics modeling than the motion of the object on 2D camera image plane. To perform the state estimation of the foot location of the objects on a 3D world ground plane, users need to set the state estimator type as <code class="docutils literal notranslate"><span class="pre">stateEstimatorType:</span> <span class="pre">3</span></code> like below:</p>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">StateEstimator</span><span class="p">:</span>
<span class="w">  </span><span class="nt">stateEstimatorType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class="w">    </span><span class="c1"># the type of state estimator among { DUMMY_ESTIMATOR=0, SIMPLE_BBOX_KF=1, REGULAR_BBOX_KF=2, SIMPLE_LOCATION_KF=3 }</span>

<span class="w">  </span><span class="c1"># [Dynamics Modeling]</span>
<span class="w">  </span><span class="nt">processNoiseVar4Loc</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">6810.866</span><span class="w">    </span><span class="c1"># Process noise variance for location</span>
<span class="w">  </span><span class="nt">processNoiseVar4Vel</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1348.487</span><span class="w">    </span><span class="c1"># Process noise variance for velocity</span>
<span class="w">  </span><span class="nt">measurementNoiseVar4Detector</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100.000</span><span class="w">   </span><span class="c1"># Measurement noise variance for detector&#39;s detection</span>
<span class="w">  </span><span class="nt">measurementNoiseVar4Tracker</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">293.323</span><span class="w">    </span><span class="c1"># Measurement noise variance for tracker&#39;s localization</span>
</pre></div>
</div>
</div></blockquote>
<p>An additional miscellaneous data that can be useful for debugging or visualization is the convex hull of the projected 3D human model on 2D camera image plane. By using the convex hull data for each object, users can also create the visualization like the figures above.</p>
<p>The additional miscellaneous data that is generated when SV3DT is enabled include (1) visibility, (2) foot location in both world plane and 2D image, and (3) convex hull (human cylinders projected on 2D image). These data can be saved in text files and/or outputted to object meta for downstream usage. To do that, users would need to set <code class="docutils literal notranslate"><span class="pre">outputVisibility:</span> <span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">outputFootLocation:</span> <span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">outputConvexHull:</span> <span class="pre">1</span></code> in <code class="docutils literal notranslate"><span class="pre">ObjectModelProjection</span></code> section respectively. The sample use cases includes saving in terminated track dump for low level tracker, attaching in KITTI track dump for deepstream-app, and converting them in schema through <em>Gst-nvmsgconv</em>.</p>
<blockquote>
<div><p>To allow users to easily try out and experience SV3DT, <a class="reference external" href="https://github.com/NVIDIA-AI-IOT/deepstream_reference_apps/tree/master/deepstream-tracker-3d">a sample usecase for SV3DT</a> has been hosted on GitHub. So, users can just clone and run it with the sample data provided.</p>
</div></blockquote>
<section id="the-3x4-camera-projection-matrix">
<h4>The 3x4 Camera Projection Matrix<a class="headerlink" href="DS_plugin_gst-nvtracker.html#the-3x4-camera-projection-matrix" title="Link to this heading">#</a></h4>
<p>The 3x4 Camera Projection Matrix is also called as simply the camera matrix, which is a 3x4 matrix that converts a 3D world point to a 2D point on camera image plane based on a pinhole camera model like shown in the figure below:</p>
<figure class="align-center" id="id16">
<img alt="Gst-nvtracker" src="../_images/pinhole_camera_model.png" />
<figcaption>
<p><span class="caption-text">Pinhole Camera Model. Source: <a class="reference external" href="https://docs.opencv.org/4.x/pinhole_camera_model.png">OpenCV Doc</a> and <a class="reference external" href="https://github.com/opencv/opencv/blob/4.x/LICENSE">License</a></span><a class="headerlink" href="DS_plugin_gst-nvtracker.html#id16" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>More detailed and general information about the camera matrix  can be found in various sources that deal with the computer vision geometries and camera calibration, including <a class="reference external" href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html">OpenCV’s documentation on Camera Calibration</a>.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">projectionMatrix_3x4</span></code> in a camera model file (e.g., <code class="docutils literal notranslate"><span class="pre">camInfo-01.yml</span></code>), the principal point (i.e., <code class="docutils literal notranslate"><span class="pre">(Cx,</span> <span class="pre">Cy)</span></code>) in the camera matrix is assumed to be at <code class="docutils literal notranslate"><span class="pre">(0,</span> <span class="pre">0)</span></code> as image coordinates. But, the optical center (i.e., <code class="docutils literal notranslate"><span class="pre">(Cx,</span> <span class="pre">Cy)</span></code>) is located at the image center (i.e., <code class="docutils literal notranslate"><span class="pre">(img_width/2,</span> <span class="pre">img_height/2)</span></code>). Thus, to move the origin to the left-top of the camera image (i.e., the pixel coordinates), SV3DT internally adds <code class="docutils literal notranslate"><span class="pre">(img_width/2,</span> <span class="pre">img_height/2)</span></code> after the transformation using the camera matrix provided in <code class="docutils literal notranslate"><span class="pre">projectionMatrix_3x4</span></code>.</p>
<p>In case that the 3x4 camera projection matrix already accounts for such translation of the principal point, users can provide the camera matrix in <code class="docutils literal notranslate"><span class="pre">projectionMatrix_3x4_w2p</span></code> instead. This assumes that the 3x4 camera projection matrix transforms a 3D world point directly into an actual pixel point whose origin is at the left-top corner of the image, so it does not require any further translation of the principal point.</p>
</section>
</section>
<section id="configuration-parameters">
<h3>Configuration Parameters<a class="headerlink" href="DS_plugin_gst-nvtracker.html#configuration-parameters" title="Link to this heading">#</a></h3>
<p>The following table summarizes the configuration parameters for the common modules in the NvMultiObjectTracker low-level tracker library.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id17">
<caption><span class="caption-text">Configuration properties in Common Modules in NvMultiObjectTracker low-level tracker library</span><a class="headerlink" href="DS_plugin_gst-nvtracker.html#id17" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 1.0%" />
<col style="width: 1.0%" />
<col style="width: 96.2%" />
<col style="width: 1.0%" />
<col style="width: 1.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>Property</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Default value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Base Config</p></td>
<td><p>minDetectorConfidence</p></td>
<td><p>Minimum detector confidence for a valid object</p></td>
<td><p>Float,
-inf to inf</p></td>
<td><p>minDetectorConfidence: 0.0</p></td>
</tr>
<tr class="row-odd"><td><p>Target Management</p></td>
<td><p>preserveStreamUpdateOrder</p></td>
<td><p>Whether to ensure target ID update order the same as input stream ID order</p></td>
<td><p>Boolean</p></td>
<td><p>preserveStreamUpdateOrder: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>maxTargetsPerStream</p></td>
<td><p>Max number of targets to track per stream</p></td>
<td><p>Integer, 0 to 65535</p></td>
<td><p>maxTargetsPerStream: 30</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minIouDiff4NewTarget</p></td>
<td><p>Min IOU to existing targets for discarding new target</p></td>
<td><p>Float,
0 to 1</p></td>
<td><p>minIouDiff4NewTarget: 0.5</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>enableBboxUnClipping</p></td>
<td><p>Enable bounding-box unclipping</p></td>
<td><p>Boolean</p></td>
<td><p>enableBboxUnClipping: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>probationAge</p></td>
<td><p>Length of probationary period in #of frames</p></td>
<td><p>Integer, ≥0</p></td>
<td><p>probationAge: 5</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>maxShadowTrackingAge</p></td>
<td><p>Maximum length of shadow tracking</p></td>
<td><p>Integer, ≥0</p></td>
<td><p>maxShadowTrackingAge: 38</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>earlyTerminationAge</p></td>
<td><p>Early termination age</p></td>
<td><p>Integer, ≥0</p></td>
<td><p>earlyTerminationAge: 2</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>outputTerminatedTracks</p></td>
<td><p>Output total frame history for terminated tracks to the tracker plugin for downstream usage</p></td>
<td><p>Boolean</p></td>
<td><p>outputTerminatedTracks: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>outputShadowTracks</p></td>
<td><p>Output shadow track state information to the tracker plugin for downstream usage</p></td>
<td><p>Boolean</p></td>
<td><p>outputShadowTracks: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>terminatedTrackFilename</p></td>
<td><p>File name prefix to save terminated tracks</p></td>
<td><p>String</p></td>
<td><p>terminatedTrackFilename: “”</p></td>
</tr>
<tr class="row-odd"><td><p>Trajectory Management</p></td>
<td><p>useUniqueID</p></td>
<td><p>Enable unique ID generation scheme</p></td>
<td><p>Boolean</p></td>
<td><p>useUniqueID: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>enableReAssoc</p></td>
<td><p>Enable motion-based target re-association</p></td>
<td><p>Boolean</p></td>
<td><p>enableReAssoc: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minMatchingScore4Overall</p></td>
<td><p>Min total score for re-association</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minMatchingScore4Overall: 0.4</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>minTrackletMatchingScore</p></td>
<td><p>Min tracklet similarity score for matching in terms of average IOU between tracklets</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minTrackletMatchingScore: 0.4</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minMatchingScore4ReidSimilarity</p></td>
<td><p>Min ReID score for re-association</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minMatchingScore4ReidSimilarity: 0.8</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>matchingScoreWeight4TrackletSimilarity</p></td>
<td><p>Weight for tracklet similarity term in re-assoc cost function</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>matchingScoreWeight4TrackletSimilarity: 1.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>matchingScoreWeight4ReidSimilarity</p></td>
<td><p>Weight for ReID similarity term in re-assoc cost function</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>matchingScoreWeight4ReidSimilarity: 0.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>minTrajectoryLength4Projection</p></td>
<td><p>Min tracklet length of a target (i.e., age) to perform trajectory projection [frames]</p></td>
<td><p>Integer, &gt;=0</p></td>
<td><p>minTrajectoryLength4Projection: 20</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>prepLength4TrajectoryProjection</p></td>
<td><p>Length of the trajectory during which the state estimator is updated to make projections [frames]</p></td>
<td><p>Integer, &gt;=0</p></td>
<td><p>prepLength4TrajectoryProjection: 10</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>trajectoryProjectionLength</p></td>
<td><p>Length of the projected trajectory [frames]</p></td>
<td><p>Integer, &gt;=0</p></td>
<td><p>trajectoryProjectionLength: 90</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>maxAngle4TrackletMatching</p></td>
<td><p>Max angle difference for tracklet matching [degree]</p></td>
<td><p>Integer, [0, 180]</p></td>
<td><p>maxAngle4TrackletMatching: 40</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>minSpeedSimilarity4TrackletMatching</p></td>
<td><p>Min speed similarity for tracklet matching</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minSpeedSimilarity4TrackletMatching: 0.3</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minBboxSizeSimilarity4TrackletMatching</p></td>
<td><p>Min bbox size similarity for tracklet matching</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minBboxSizeSimilarity4TrackletMatching: 0.6</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>maxTrackletMatchingTimeSearchRange</p></td>
<td><p>Search space in time for max tracklet similarity</p></td>
<td><p>Integer, &gt;=0</p></td>
<td><p>maxTrackletMatchingTimeSearchRange: 20</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>trajectoryProjectionProcessNoiseScale</p></td>
<td><p>Trajectory state estimator’s process noise scale</p></td>
<td><p>Float,
0.0 to inf</p></td>
<td><p>trajectoryProjectionProcessNoiseScale: 1.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>trajectoryProjectionMeasurement NoiseScale</p></td>
<td><p>Trajectory state estimator’s measurement noise scale</p></td>
<td><p>Float,
0.0 to inf</p></td>
<td><p>trajectoryProjectionMeasurement NoiseScale: 1.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>trackletSpacialSearchRegionScale</p></td>
<td><p>Re-association peer tracklet search region scale</p></td>
<td><p>Float,
0.0 to inf</p></td>
<td><p>trackletSpacialSearchRegionScale: 0.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>reidExtractionInterval</p></td>
<td><p>Frame interval to extract ReID features per target for re-association; -1 means only extracting the beginning frame per target</p></td>
<td><p>Integer, ≥-1</p></td>
<td><p>reidExtractionInterval: 0</p></td>
</tr>
<tr class="row-odd"><td><p>Data Associator</p></td>
<td><p>associationMatcherType</p></td>
<td><p>Type of matching algorithm { GREEDY=0, CASCADED=1 }</p></td>
<td><p>Integer, [0, 1]</p></td>
<td><p>associationMatcherType: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>checkClassMatch</p></td>
<td><p>Enable associating only the same-class objects</p></td>
<td><p>Boolean</p></td>
<td></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minMatchingScore4Overall</p></td>
<td><p>Min total score for valid matching</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minMatchingScore4Overall: 0.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>minMatchingScore4SizeSimilarity</p></td>
<td><p>Min bbox size similarity score for valid matching</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minMatchingScore4SizeSimilarity: 0.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minMatchingScore4Iou</p></td>
<td><p>Min IOU score for valid matching</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minMatchingScore4Iou: 0.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>matchingScoreWeight4SizeSimilarity</p></td>
<td><p>Weight for size similarity term in matching cost function</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>matchingScoreWeight4SizeSimilarity: 0.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>matchingScoreWeight4Iou</p></td>
<td><p>Weight for IOU term in matching cost function</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>matchingScoreWeight4Iou: 1.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>tentativeDetectorConfidence</p></td>
<td><p>If a detection’s confidence is lower than this but higher than minDetectorConfidence, then it’s considered as a tentative detection</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>tentativeDetectorConfidence: 0.5</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minMatchingScore4TentativeIou</p></td>
<td><p>Min iou threshold to match targets and tentative detection</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minMatchingScore4TentativeIou: 0.0</p></td>
</tr>
<tr class="row-even"><td><p>State Estimator</p></td>
<td><p>stateEstimatorType</p></td>
<td><p>Type of state estimator among { DUMMY=0, SIMPLE=1, REGULAR=2, SIMPLE_LOC=3 }</p></td>
<td><p>Integer, [0,3]</p></td>
<td><p>stateEstimatorType: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>processNoiseVar4Loc</p></td>
<td><p>Process noise variance for bbox center</p></td>
<td><p>Float,
0.0 to inf</p></td>
<td><p>processNoiseVar4Loc: 2.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>processNoiseVar4Size</p></td>
<td><p>Process noise variance for bbox size</p></td>
<td><p>Float,
0.0 to inf</p></td>
<td><p>processNoiseVar4Size: 1.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>processNoiseVar4Vel</p></td>
<td><p>Process noise variance for velocity</p></td>
<td><p>Float,
0.0 to inf</p></td>
<td><p>processNoiseVar4Vel: 0.1</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>measurementNoiseVar4Detector</p></td>
<td><p>Measurement noise variance for detector’s detection</p></td>
<td><p>Float,
0.0 to inf</p></td>
<td><p>measurementNoiseVar4Detector: 4.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>measurementNoiseVar4Tracker</p></td>
<td><p>Measurement noise variance for tracker’s localization</p></td>
<td><p>Float,
0.0 to inf</p></td>
<td><p>measurementNoiseVar4Tracker: 16.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>noiseWeightVar4Loc</p></td>
<td><p>Noise covariance weight for bbox location; if set, location noise will be proportional to box height</p></td>
<td><p>Float,
&gt;0.0 considered as set</p></td>
<td><p>noiseWeightVar4Loc: -0.1</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>noiseWeightVar4Vel</p></td>
<td><p>Noise covariance weight for bbox velocity; if set, location noise will be proportional to box height</p></td>
<td><p>Float,
&gt;0.0 considered as set</p></td>
<td><p>noiseWeightVar4Vel: -0.1</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>useAspectRatio</p></td>
<td><p>Use aspect ratio in Kalman Filter’s states</p></td>
<td><p>Boolean</p></td>
<td><p>useAspectRatio: 0</p></td>
</tr>
<tr class="row-odd"><td><p>Object Re-ID</p></td>
<td><p>reidType</p></td>
<td><p>The type of Re-ID network among { DUMMY=0, NvDEEPSORT=1, Reid based reassoc=2, both NvDEEPSORT and reid based reassoc=3 }</p></td>
<td><p>Integer,
[0, 3]</p></td>
<td><p>reidType: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>batchSize</p></td>
<td><p>Batch size of Re-ID network</p></td>
<td><p>Integer,
&gt;0</p></td>
<td><p>batchSize: 1</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>workspaceSize</p></td>
<td><p>Workspace size to be used by Re-ID TensorRT engine, in MB</p></td>
<td><p>Integer,
&gt;0</p></td>
<td><p>workspaceSize: 20</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>reidFeatureSize</p></td>
<td><p>Size of Re-ID feature</p></td>
<td><p>Integer,
&gt;0</p></td>
<td><p>reidFeatureSize: 128</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>reidHistorySize</p></td>
<td><p>Size of feature gallery, i.e. max number of Re-ID features kept for one tracker</p></td>
<td><p>Integer,
&gt;0</p></td>
<td><p>reidHistorySize: 100</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>inferDims</p></td>
<td><p>Re-ID network input dimension CHW or HWC based on inputOrder</p></td>
<td><p>Integer,
&gt;0</p></td>
<td><p>inferDims: [128, 64, 3]</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>inputOrder</p></td>
<td><p>Re-ID network input order {NCHW=0, NHWC=1}</p></td>
<td><p>Integer,
[0, 1]</p></td>
<td><p>inputOrder: 1</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>colorFormat</p></td>
<td><p>Re-ID network input color format among {RGB=0, BGR=1 }</p></td>
<td><p>Integer,
[0, 1]</p></td>
<td><p>colorFormat: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>networkMode</p></td>
<td><p>Re-ID network inference precision mode among {FP32=0, FP16=1, INT8=2 }</p></td>
<td><p>Integer,
[0, 1, 2]</p></td>
<td><p>networkMode: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>offsets</p></td>
<td><p>Array of values to be subtracted from each input channel, with length equal to number of channels</p></td>
<td><p>Comma delimited float array</p></td>
<td><p>offsets: [0.0, 0.0, 0.0]</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>netScaleFactor</p></td>
<td><p>Scaling factor for Re-ID network input after substracting offsets</p></td>
<td><p>Float,
&gt;0</p></td>
<td><p>netScaleFactor: 1.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>addFeatureNormalization</p></td>
<td><p>If Re-ID network’s output Re-ID feature vector is not l2 normalized, explicitly performs l2 normalization</p></td>
<td><p>Boolean</p></td>
<td><p>addFeatureNormalization: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>tltEncodedModel</p></td>
<td><p>Pathname of the TAO toolkit encoded model</p></td>
<td><p>String</p></td>
<td><p>tltEncodedModel: “”</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>tltModelKey</p></td>
<td><p>Key for the TAO toolkit encoded model</p></td>
<td><p>String</p></td>
<td><p>tltModelKey: “”</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>onnxFile</p></td>
<td><p>Pathname of the ONNX model file</p></td>
<td><p>String</p></td>
<td><p>onnxFile: “”</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>modelEngineFile</p></td>
<td><p>Absolute path to Re-ID engine file</p></td>
<td><p>String</p></td>
<td><p>modelEngineFile:””</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>calibrationTableFile</p></td>
<td><p>Absolute path to calibration table, required by INT8 only</p></td>
<td><p>String</p></td>
<td><p>calibrationTableFile:””</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>keepAspc</p></td>
<td><p>Whether to keep aspcect ratio when resizing input objects to Re-ID network</p></td>
<td><p>Boolean</p></td>
<td><p>keepAspc: 1</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>outputReidTensor</p></td>
<td><p>Output Re-ID features to user meta for downstream usage</p></td>
<td><p>Boolean</p></td>
<td><p>outputReidTensor: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>useVPICropScaler (Alpha feature)</p></td>
<td><p>Use NVIDIA’s <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a> Crop Scaler algorithm instead of built in implementation</p></td>
<td><p>Boolean</p></td>
<td><p>useVPICropScaler: 0</p></td>
</tr>
<tr class="row-odd"><td><p>Object Model Projection</p></td>
<td><p>cameraModelFilepath</p></td>
<td><p>A list of file paths to camera info files. A valid camera info file should be provided to each video stream</p></td>
<td><p>String</p></td>
<td><p>cameraModelFilepath: “”</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>outputVisibility</p></td>
<td><p>Output object visibility to object meta and file dump</p></td>
<td><p>Boolean</p></td>
<td><p>outputVisibility: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>outputFootLocation</p></td>
<td><p>Output object (especially for human) foot location to object meta and file dump</p></td>
<td><p>Boolean</p></td>
<td><p>outputFootLocation: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>outputConvexHull</p></td>
<td><p>Output projected object convex hull (especially cylinder for human) to object meta and file dump</p></td>
<td><p>Boolean</p></td>
<td><p>outputConvexHull: 1</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>maxConvexHullSize</p></td>
<td><p>Maximum number of points to consist an object convex hull</p></td>
<td><p>Integer,
&gt;0</p></td>
<td><p>maxConvexHullSize: 15</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="low-level-tracker-comparisons-and-tradeoffs">
<h2>Low-Level Tracker Comparisons and Tradeoffs<a class="headerlink" href="DS_plugin_gst-nvtracker.html#low-level-tracker-comparisons-and-tradeoffs" title="Link to this heading">#</a></h2>
<p>DeepStream SDK provides four reference low-level tracker libraries which have different resource requirements and performance characteristics, in terms of accuracy, robustness, and efficiency, allowing the users to choose the best tracker based on their use cases and requirements. See the following table for comparison.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id18">
<caption><span class="caption-text">Gst-nvtracker Tracker library comparison</span><a class="headerlink" href="DS_plugin_gst-nvtracker.html#id18" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 7.7%" />
<col style="width: 7.7%" />
<col style="width: 7.7%" />
<col style="width: 28.8%" />
<col style="width: 24.0%" />
<col style="width: 24.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Tracker Type</p></th>
<th class="head"><p>GPU
Compute</p></th>
<th class="head"><p>CPU
Compute</p></th>
<th class="head"><p>Pros</p></th>
<th class="head"><p>Cons</p></th>
<th class="head"><p>Best Use Cases</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>IOU</p></td>
<td><p>No</p></td>
<td><p>Very Low</p></td>
<td><ul class="simple">
<li><p>Light weight</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>No visual features for matching, so prone to frequent tracker ID switches and failures. Not suitable for fast moving scene.</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>Objects are sparsely located, with distinct sizes</p></li>
<li><p>Detector is expected to run every frame or very frequently (ex. every alternate frame)</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>NvSORT</p></td>
<td><p>No</p></td>
<td><p>Very Low</p></td>
<td><ul class="simple">
<li><p>Light weight</p></li>
<li><p>Motion estimation and prection with Kalman filter based state estimator</p></li>
<li><p>Enhanced data association with reasonable tracking accuracy with medium or high accuracy detectors</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>No visual features for matching, so prone to frequent tracker ID switches and failures.</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>Medium or high accuracy detectors such as PeopleNet and YOLO</p></li>
<li><p>Detector is expected to run every frame or very frequently (ex. every alternate frame)</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>NvDeepSORT</p></td>
<td><p>High</p></td>
<td><p>Low</p></td>
<td><ul class="simple">
<li><p>Allow custom Re-ID model for appearance matching</p></li>
<li><p>Less frequent ID switches</p></li>
<li><p>Highly discriminative on targets with similar appearance</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>Higher compute cost due to inferencing required for each object</p></li>
<li><p>Cannot easily adjust accuracy/perf level unless switching Re-ID model</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>Multi-object, complex scenes even with partial occlusion</p></li>
<li><p>Objects with similar appearance</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>NvDCF</p></td>
<td><p>Medium</p></td>
<td><p>Low</p></td>
<td><ul class="simple">
<li><p>Highly robust against partial occlusions, shadow, and other transient visual changes</p></li>
<li><p>Less frequent ID switches</p></li>
<li><p>Can be used with PGIE interval &gt; 0 without significant accuracy degradation</p></li>
<li><p>Output tracking confidence based on visual similarity</p></li>
<li><p>Easily adjust params for accuracy-perf tradeoff depending on application requirements</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>Slower than IOU, NvSORT due to increased computational complexity for visual feature extraction</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>Multi-object, complex scenes even with partial occlusion</p></li>
<li><p>PGIE interval &gt; 0</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="iou-tracker">
<h2>IOU Tracker<a class="headerlink" href="DS_plugin_gst-nvtracker.html#iou-tracker" title="Link to this heading">#</a></h2>
<p>The NvMultiObjectTracker library provides an object tracker that has only the essential and minimum set of functionalities for multi-object tracking, which is called the <em>IOU</em> tracker. IOU tracker performs only the following functionalities:</p>
<ul class="simple">
<li><p><strong>Greedy data association</strong> between the detector objects from a new video frame and the existing targets in the previous video frame</p></li>
<li><p><strong>Target management</strong> based on the data association results including the target state update and the creation and termination of targets</p></li>
</ul>
<p>The error handling mechanisms like Late Activation and Shadow Tracking are integral part of the target management module of the NvMultiObjectTracker library; thus, such features are inherently enabled in the IOU tracker.</p>
<p>IOU tracker can be used as a performance baseline as it consumes the minimum amount of computational resources. A sample configuration file <code class="docutils literal notranslate"><span class="pre">config_tracker_IOU.yml</span></code> is provided in DeepStream SDK package.</p>
</section>
<section id="nvsort-tracker">
<h2>NvSORT Tracker<a class="headerlink" href="DS_plugin_gst-nvtracker.html#nvsort-tracker" title="Link to this heading">#</a></h2>
<p>NvSORT tracker increases the tracking accuracy while maintaining the high performance on top of IOU tracker with the following improvements:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>State estimation</strong> with Kalman filter to better estimate and predict the states of the targets in the current frame.</p></li>
<li><p><strong>Cascaded data association</strong> to associate targets and detector objects in multiple stages based on their proximity and confidence, which is more accurate than the simple matching in original SORT tracker.</p></li>
</ul>
</div></blockquote>
<p>As it fully relies on the bbox attributes for data association, the NvSORT’s tracking accuracy is solely attributed to the detection accuracy. With a medium or high accuracy detector, NvSORT produces high quality tracking results with minimal computational resources. A sample configuration file <code class="docutils literal notranslate"><span class="pre">config_tracker_NvSORT.yml</span></code> is provided in DeepStream SDK package.</p>
</section>
<section id="nvdeepsort-tracker">
<h2>NvDeepSORT Tracker<a class="headerlink" href="DS_plugin_gst-nvtracker.html#nvdeepsort-tracker" title="Link to this heading">#</a></h2>
<p>NvDeepSORT tracker utilizes deep learning based object appearance information for accurate object matching in different frames and locations, resulting in enhanced robustness over occlusions and reduced ID switches. It applies a pre-trained re-identification (Re-ID) neural network to extract a feature vector for each object, compares the similarity between different objects using the extracted feature vector with a cosine distance metric, and combines it with a state estimator to perform the data association over frames. Before running NvDeepSORT, Re-ID model needs to be set up following <a class="reference internal" href="DS_plugin_gst-nvtracker.html#setup-sample-re-id-models">Setup Sample Re-ID Models</a>  and <a class="reference internal" href="DS_plugin_gst-nvtracker.html#customize-re-id-model">Customize Re-ID Model</a>.</p>
<section id="id5">
<h3>Data Association<a class="headerlink" href="DS_plugin_gst-nvtracker.html#id5" title="Link to this heading">#</a></h3>
<p>For the data association in the NvDeepSORT tracker, there are two metrics are used:</p>
<ul class="simple">
<li><p>Proximity</p></li>
<li><p>Re-ID based similarity</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For the proximity score, the <cite>Mahalanobis</cite> distance between the <cite>i-th</cite> detector object and the <cite>j-th</cite> target is calculated using the target’s predicted location and its associated uncertainty:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[dist_{ij}=(D_i-Y_j)^TS_j^{-1}(D_i-Y_j)\]</div>
</div></blockquote>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D_i\)</span> denotes the <cite>i-th</cite> detector object in <code class="docutils literal notranslate"><span class="pre">{x,</span> <span class="pre">y,</span> <span class="pre">a,</span> <span class="pre">h}</span></code> format.</p></li>
<li><p><span class="math notranslate nohighlight">\(Y_j\)</span> denotes the predicted states <code class="docutils literal notranslate"><span class="pre">{x',</span> <span class="pre">y',</span> <span class="pre">a',</span> <span class="pre">h'}</span></code> from state estimator for the <cite>j-th</cite> tracker.</p></li>
<li><p><span class="math notranslate nohighlight">\(S_j\)</span> denotes the predicted covariance from state estimator for the <cite>j-th</cite> tracker.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>In the original DeepSORT implementation, the maximum threshold of <cite>Mahalanobis</cite> distance for a valid pair of detector object and target is set <code class="docutils literal notranslate"><span class="pre">9.4877</span></code>, representing 95% confidence computed from the inverse Chi-square distribution. Note in NvDeepSORT, the value is configured by <code class="docutils literal notranslate"><span class="pre">thresholdMahalanobis</span></code> in tracker config to achieve higher accuracy for a particular detector model, such as the PeopleNet v2.6.2, so it may be different from the value in the original implementation.</p>
<p>After filtering out invalid pairs, the Re-ID similarity score is computed as the maximum cosine similarity between a detector object and a target. Then the cascaded data association algorithm is used for high accuracy multi-stage matching.</p>
</section>
<section id="id6">
<h3>Configuration Parameters<a class="headerlink" href="DS_plugin_gst-nvtracker.html#id6" title="Link to this heading">#</a></h3>
<p>A sample config file <code class="docutils literal notranslate"><span class="pre">config_tracker_NvDeepSORT.yml</span></code> is provided in DeepStream SDK package. The following table summarizes the configuration parameters for NvDeepSORT.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id19">
<caption><span class="caption-text">Gst-nvtracker NvDeepSORT low-level tracker configuration properties</span><a class="headerlink" href="DS_plugin_gst-nvtracker.html#id19" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 1.0%" />
<col style="width: 1.0%" />
<col style="width: 96.2%" />
<col style="width: 1.0%" />
<col style="width: 1.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>Property</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Default value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data Associator</p></td>
<td><p>thresholdMahalanobis</p></td>
<td><p>Max Mahalanobis distance based on Chi-square probabilities</p></td>
<td><p>Float,
&gt;0 considered as set</p></td>
<td><p>thresholdMahalanobis: -1.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minMatchingScore4ReidSimilarity</p></td>
<td><p>Min Re-ID threshold to match targets and tentative detection</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minMatchingScore4ReidSimilarity: 0.0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>matchingScoreWeight4ReIDSimilarity</p></td>
<td><p>Weight for Re-ID similarity term in matching cost function</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>matchingScoreWeight4ReIDSimilarity: 0.0</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="implementation-details-and-reference">
<h3>Implementation Details and Reference<a class="headerlink" href="DS_plugin_gst-nvtracker.html#implementation-details-and-reference" title="Link to this heading">#</a></h3>
<p>The difference between NvDeepSORT and the original implementation includes:</p>
<ul class="simple">
<li><p>For data association, the original implementation sorts the targets in an ascending order based on the tracking age and runs the matching algorithm for each age sequentially, while NvDeepSORT applies the cascaded data association algorithm with higher performance and accuracy.</p></li>
<li><p>NvDeepSORT implementation in the NvMultiObjectTracker library adopts the same target management policy as the NvDCF tracker, which is advanced to the original DeepSORT.</p></li>
<li><p>The cosine distance metric for two features is <span class="math notranslate nohighlight">\(score_{ij}=1-feature\_det_{i}\cdot feature\_track_{jk}\)</span>, with smaller values representing more similarity. By contrast, NvDeepSORT directly uses dot product for computational efficiency, so larger values means higher similarity.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>Reference</strong>: Wojke, Nicolai, Alex Bewley, and Dietrich Paulus. “Simple online and real-time tracking with a deep association metric.” <cite>2017 IEEE international conference on image processing (ICIP).</cite> IEEE, 2017. Check <a class="reference external" href="https://arxiv.org/abs/1703.07402">Paper</a> and <a class="reference external" href="https://github.com/nwojke/deep_sort">The original implementation on Github</a>.</p>
</section>
</section>
<section id="nvdcf-tracker">
<h2>NvDCF Tracker<a class="headerlink" href="DS_plugin_gst-nvtracker.html#nvdcf-tracker" title="Link to this heading">#</a></h2>
<p>NvDCF tracker employs a visual tracker that is based on the discriminative correlation filter (DCF) for learning a target-specific correlation filter and for localizing the same target in the next frames using the learned correlation filter. Such correlation filter learning and localization are usually carried out on <cite>per-object</cite> basis in a typical MOT implementation, creating a potentially large number of small CUDA kernel launches when processed on GPU. This inherently poses challenges in maximizing GPU utilization, especially when a large number of objects from multiple video streams are expected to be tracked on a single GPU.</p>
<p>To address such performance issues, the GPU-accelerated operations for the NvDCF tracker are designed to be executed in the <em>batch processing</em> mode to maximize the GPU utilization despite the nature of small CUDA kernels in per-object tracking model. The batch processing mode is applied in the entire tracking operations, including the <cite>bbox</cite> cropping and scaling, visual feature extraction, correlation filter learning, and localization. This can be viewed as a similar model to the batched cuFFT or batched cuBLAS calls, but it differs in that the batched MOT execution model spans many operations in a higher level. The batch processing capability is extended from multi-object batching to the batching of multiple streams for even greater efficiency and scalability.</p>
<p>Thanks to its visual tracking capability, the NvDCF tracker can localize and keep track of the targets even when the detector in PGIE misses them (i.e., false negatives) for potentially an extended period of time caused by partial or full occlusions, resulting in more robust tracking. The enhanced robustness characteristics allow users to use a higher <code class="docutils literal notranslate"><span class="pre">maxShadowTrackingAge</span></code> value for longer-term object tracking and also allows PGIE’s <code class="docutils literal notranslate"><span class="pre">interval</span></code> to be higher only at the cost of slight degradation in accuracy.</p>
<p>Unlike NvSORT and NvDeepSORT where the Kalman filter takes the detection bboxes as the only input, the Kalman filter in the NvDCF tracker also takes the localization results from the visual tracking module as an input as well. Once a target is being tracked, the visual tracker keeps trying to localize the same target in the next frames using the learned correlation filter, while there could be matched detector bboxes. The Kalman filter in NvDCF tracker fuses both the DCF-based localization results and the detection bboxes for better target state estimation and prediction.</p>
<section id="visual-tracking">
<h3>Visual Tracking<a class="headerlink" href="DS_plugin_gst-nvtracker.html#visual-tracking" title="Link to this heading">#</a></h3>
<p>For each tracked target, NvDCF tracker defines a search region around its <cite>predicted</cite> location in the next frame large enough for the same target to be detected in the search region. The location of a target on a new video frame is predicted by using the state estimator module. The <code class="docutils literal notranslate"><span class="pre">searchRegionPaddingScale</span></code> property determines the size of the search region as a multiple of the diagonal of the target’s bounding box. The size of the search region would be determined as:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}SearchRegion_{width}=w+searchRegionPaddingScale*\sqrt{w*h}\\SearchRegion_{height}=h+searchRegionPaddingScale*\sqrt{w*h}\end{aligned}\end{align} \]</div>
</div></blockquote>
<p>, where <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(h\)</span> are the width and height of the target’s bounding box, respectively.</p>
<p>Once the search region is defined for each target at its predicted location, the image patches from each of the search regions are cropped and scaled to a predefined feature image size, from which the visual features are extracted. The <code class="docutils literal notranslate"><span class="pre">featureImgSizeLevel</span></code> property defines the size of the feature image, and its range is from 1 to 5. Each level between 1 and 5 corresponds to 12x12, 18x18, 24x24, 30x30, and 36x36, respectively, for each feature channel. A lower value of <code class="docutils literal notranslate"><span class="pre">featureImgSizeLevel</span></code> causes NvDCF to use a smaller feature size, increasing GPU performance potentially yet at the cost of accuracy and robustness. Consider the relationship between <code class="docutils literal notranslate"><span class="pre">featureImgSizeLevel</span></code> and <code class="docutils literal notranslate"><span class="pre">searchRegionPaddingScale</span></code> when configuring the parameters. If <code class="docutils literal notranslate"><span class="pre">searchRegionPaddingScale</span></code> is increased while <code class="docutils literal notranslate"><span class="pre">featureImgSizeLevel</span></code> is fixed, the number of pixels corresponding to the target itself in the feature images will be effectively decreased.</p>
<p>For each cropped image patch, the visual appearance features such as ColorNames and/or Histogram-of-Oriented-Gradient (HOG) are extracted. The type of visual features to be used can be configured by setting <code class="docutils literal notranslate"><span class="pre">useColorNames</span></code> and/or <code class="docutils literal notranslate"><span class="pre">useHog</span></code>. The HOG features consist of 18 channels based on the number of bins for different orientations, while The ColorNames features have 10 channels. If both features are used (by setting <code class="docutils literal notranslate"><span class="pre">useColorNames:</span> <span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">useHog:</span> <span class="pre">1</span></code>), the total number of channels would then be 28. Therefore, if one uses both HOG and ColorNames with <code class="docutils literal notranslate"><span class="pre">featureImgSizeLevel:</span> <span class="pre">5</span></code>, the dimension of visual features that represents a target would be 28x48x48. The more channels of visual features are used, the higher the accuracy would be, but would increase the computational complexity and reduce the performance. The NvDCF tracker uses NVIDIA’s <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a> library for extracting those visual features.</p>
<p>The correlation filters are generated with an attention window (using a Hanning window) applied at the center of the target <cite>bbox</cite>. Users are allowed to move the center of the attention window in the vertical direction. For example, <code class="docutils literal notranslate"><span class="pre">featureFocusOffsetFactor_y:</span> <span class="pre">-0.2</span></code> would result in the center of the attention window to be at <code class="docutils literal notranslate"><span class="pre">y=-0.2</span></code> in the feature map, where the relative range of the height is <code class="docutils literal notranslate"><span class="pre">[-0.5,</span> <span class="pre">0.5]</span></code>. Consider that typical surveillance or CCTV cameras are mounted at a moderately high position to monitor a wide area of the environment, say, a retail store or a traffic intersection. From those vantage points, more occlusions can occur at the lower part of the body of persons or vehicles by other persons or vehicles. Moving the attention window up a bit may improve the accuracy and robustness for those use cases.</p>
<p>Once a correlation filter is generated for a target, typical DCF-based trackers usually employ an exponential moving average for temporal consistency when the optimal correlation filter is created and updated over consecutive frames. The learning rate for this moving average can be configured by <code class="docutils literal notranslate"><span class="pre">filterLr</span></code> and <code class="docutils literal notranslate"><span class="pre">filterChannelWeightsLr</span></code> for the correlation filters and their channel weights, respectively. The standard deviation for Gaussian for the desired response used when creating an optimal DCF filter can also be configured by <code class="docutils literal notranslate"><span class="pre">gaussianSigma</span></code>.</p>
<p><strong>Compute Backends</strong></p>
<p>Visual tracker module in NvDCF supports multiple compute backends: CUDA/GPU and PVA (<a class="reference external" href="https://docs.nvidia.com/vpi/architecture.html#autotoc_md11">Programmable Vision Accelerator</a>). So, the users are allowed to use different compute backends depending on the particular use cases.</p>
<p>PVA is an accelerator in Tegra SOC in NVIDIA’s Jetson devices, which is specialized for image processing and computer vision algorithms with extremely low power consumption. When running DeepStream-based pipeline with tracker on Jetson, it is recommended to use the PVA-backend for DCF operations in NvDCF for better power efficiency. Since the GPU-based processing for DCF operations is offloaded onto PVA, therefore, more GPU resources are made available to the users for any downstream or custom processing that requires GPU-based processing.</p>
<p>To employ PVA-backend, the visual tracker module leverages the APIs provided by NVIDIA’s <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a>, which can be enabled by setting <code class="docutils literal notranslate"><span class="pre">visualTrackerType:</span> <span class="pre">2</span></code> (Alpha feature) and <code class="docutils literal notranslate"><span class="pre">vpiBackend4DcfTracker:</span> <span class="pre">2</span> <span class="pre">#</span> <span class="pre">{CUDA=1,</span> <span class="pre">PVA=2}</span></code> on Jetson platforms. VPI also has CUDA-backend mode, which can configured by setting <code class="docutils literal notranslate"><span class="pre">vpiBackend4DcfTracker:</span> <span class="pre">1</span></code> on any supported HW platforms (Jetson or dGPU platforms).</p>
<p>The PVA-backend implementation of DCF operations by <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a> currently has the following limitations:</p>
<ul>
<li><p>The maximum number of objects that can be supported by a single instance of tracker library is 512. This translates to the following restrictions in DeepStream configuration:</p>
<ul>
<li><p>When sub-batching is not enabled, the total number of objects being tracked across all the streams (i.e., the number of streams in a batch *  <code class="docutils literal notranslate"><span class="pre">maxTargetsPerStream</span></code>) should be less than or equal to 512</p></li>
<li><p>When sub-batching is enabled, the total number of objects being tracked across all the streams in a sub-batch (i.e., the number of streams in a sub-batch *  <code class="docutils literal notranslate"><span class="pre">maxTargetsPerStream</span></code>) should be less than or equal to 512</p>
<blockquote>
<div><ul class="simple">
<li><p>If there are <cite>N</cite> sub-batches, the total number of objects that can be tracked in the pipeline is effectively <code class="docutils literal notranslate"><span class="pre">N*512</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</li>
<li><p>Supports only one combination of the these three configurations : <code class="docutils literal notranslate"><span class="pre">useColorNames:</span> <span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">useHog:</span> <span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">featureImgSizeLevel:</span> <span class="pre">3</span></code></p></li>
</ul>
<p>Another limitation of the DCF APIs provided by <a class="reference external" href="https://docs.nvidia.com/vpi/index.html">VPI™</a> is that, the max number of streams supported by a single library instance is 33. Hence, to run a DS application with higher batch size (&gt;33) using <code class="docutils literal notranslate"><span class="pre">visualTrackerType:</span> <span class="pre">2</span></code>, it is recommended to use <cite>Sub-batching</cite> feature in the tracker plugin such that each sub-batch is of size less than or equal to 33 streams.</p>
</section>
<section id="id11">
<h3>Data Association<a class="headerlink" href="DS_plugin_gst-nvtracker.html#id11" title="Link to this heading">#</a></h3>
<p>The association of target IDs across frames for robust tracking typically entails visual appearance-based similarity matching, for which the visual appearance features are extracted at each candidate location. Usually, this is a computationally expensive process and often plays as a performance bottleneck in object tracking. Unlike existing approaches that extract visual features from all the candidate locations and perform feature matching among all the candidate objects, the NvDCF tracker takes advantage of the correlation response (that is already obtained during target localization stage) as the tracking confidence map of each tracker over a search region and simply looks up the confidence values at each candidate location (i.e., the location of each detector object) to get the visual similarity without any explicit computation. By comparing those confidences between trackers, we can identify which tracker has a higher visual similarity to a particular detector object and use it as a part of the matching score for data association. Therefore, the visual similarity matching in the data association process can be carried out very efficiently through a simple look-up table (LUT) operation on existing correlation responses.</p>
<p>In the animated figure below, the left side shows the target within its search region, while the right side shows the correlation response map (where the deep red color indicates higher confidence and deep blue indicates lower confidence). In the confidence map, the yellow cross (i.e., <code class="docutils literal notranslate"><span class="pre">+</span></code>) around the center indicates the peak location of the correlation response, while the purple <code class="docutils literal notranslate"><span class="pre">x</span></code> indicate the center of nearby detector bboxes. The correlation response values at those purple <code class="docutils literal notranslate"><span class="pre">x</span></code> locations indicate the confidence score on how likely the same target exists at that location in terms of the visual similarity.</p>
<img alt="Correlation Response" class="align-center" src="../_images/NvDCF_RN10_lvl5_reassoc_CorrResp[187].gif" />
<p>If there are multiple detector bboxes (i.e., purple <code class="docutils literal notranslate"><span class="pre">x</span></code>) around the target like the one in the figure below, the data association module will take care of the matching based on the visual similairty score and the configured weight and minimum value, which are <code class="docutils literal notranslate"><span class="pre">matchingScoreWeight4VisualSimilarity</span></code> and <code class="docutils literal notranslate"><span class="pre">minMatchingScore4VisualSimilarity</span></code>, respectively.</p>
<img alt="Correlation Response" class="align-center" src="../_images/NvDCF_RN10_lvl5_CorrResp_Target_187_Frame_985.jpg" />
</section>
<section id="id12">
<h3>Configuration Parameters<a class="headerlink" href="DS_plugin_gst-nvtracker.html#id12" title="Link to this heading">#</a></h3>
<p>A few sample configuration files for the NvDCF tracker are provided as a part of DeepStream SDK package, which is named as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_max_perf.yml</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_perf.yml</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_accuracy.yml</span></code></p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The first <cite>max_perf</cite> config file is to configure the NvDCF tracker to consume the least amount of resources, while the second <cite>perf</cite> config file is for the use case where a decent balance between performance and accuracy is required. The last <cite>accuracy</cite> config file is to maximize the accuracy and robustness by enabling most of the features to their full capability, especially the target re-association.</p>
<p>The following table summarizes the configuration parameters used in the config files for the NvDCF low-level tracker (except the common modules and parameters already mentioned in an earlier section).</p>
<div class="pst-scrollable-table-container"><table class="table" id="id20">
<caption><span class="caption-text">Gst-nvtracker NvDCF low-level tracker configuration properties</span><a class="headerlink" href="DS_plugin_gst-nvtracker.html#id20" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 1.0%" />
<col style="width: 1.0%" />
<col style="width: 96.2%" />
<col style="width: 1.0%" />
<col style="width: 1.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>Property</p></th>
<th class="head"><p>Meaning</p></th>
<th class="head"><p>Type and Range</p></th>
<th class="head"><p>Default value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Visual Tracker</p></td>
<td><p>visualTrackerType</p></td>
<td><p>Type of visual tracker among { DUMMY=0, NvDCF=1, NvDCF_VPI=2 }</p>
<p>NvDCF_VPI is an Alpha feature.</p>
</td>
<td><p>Int, [0, 1, 2]</p></td>
<td><p>visualTrackerType: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>useColorNames</p></td>
<td><p>Use ColorNames feature</p></td>
<td><p>Boolean</p></td>
<td><p>useColorNames: 1</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>useHog</p></td>
<td><p>Use Histogram-of-Oriented-Gradient (HOG) feature</p></td>
<td><p>Boolean</p></td>
<td><p>useHog: 0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>featureImgSizeLevel</p></td>
<td><p>Size of a feature image</p></td>
<td><p>Integer,
1 to 5</p></td>
<td><p>featureImgSizeLevel: 2</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>featureFocusOffsetFactor_y</p></td>
<td><p>The offset for the center of hanning window relative to the feature height</p></td>
<td><p>Float,
-0.5 to 0.5</p></td>
<td><p>featureFocusOffsetFactor_y: 0.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>useHighPrecisionFeature</p></td>
<td><p>Whether to use 16 bit high precision feature; otherwise use 8 bit</p></td>
<td><p>Boolean</p></td>
<td><p>useHighPrecisionFeature: 0</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>filterLr</p></td>
<td><p>Learning rate for DCF filter in exponential moving average</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>filterLr: 0.075</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>filterChannelWeightsLr</p></td>
<td><p>Learning rate for weights for different feature channels in DCF</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>filterChannelWeightsLr: 0.1</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>gaussianSigma</p></td>
<td><p>Standard deviation for Gaussian for desired response</p></td>
<td><p>Float,
&gt;0.0</p></td>
<td><p>gaussianSigma: 0.75</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>vpiBackend4DcfTracker</p></td>
<td><p>Compute backend among {CUDA=1, PVA=2}</p>
<p>Valid when <cite>visualTrackerType: 2</cite></p>
</td>
<td><p>Int, [1,2]</p></td>
<td><p>vpiBackend4DcfTracker: 1</p></td>
</tr>
<tr class="row-even"><td><p>Target Management</p></td>
<td><p>searchRegionPaddingScale</p></td>
<td><p>Search region size</p></td>
<td><p>Integer,
1 to 3</p></td>
<td><p>searchRegionPaddingScale: 1</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>minTrackerConfidence</p></td>
<td><p>Minimum detector confidence for a valid target</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minTrackerConfidence: 0.6</p></td>
</tr>
<tr class="row-even"><td><p>Data Assoicator</p></td>
<td><p>minMatchingScore4
VisualSimilarity</p></td>
<td><p>Min visual similarity score for valid matching</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>minMatchingScore4
VisualSimilarity: 0.0</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>matchingScoreWeight4
VisualSimilarity</p></td>
<td><p>Weight for visual similarity term in matching cost function</p></td>
<td><p>Float,
0.0 to 1.0</p></td>
<td><p>matchingScoreWeight4
VisualSimilarity: 0.0</p></td>
</tr>
</tbody>
</table>
</div>
<p>See also the <a class="reference internal" href="DS_troubleshooting.html#nvdcf-param-troubleshooting-label"><span class="std std-ref">Troubleshooting in Tracker Setup and Parameter Tuning</span></a> section for solutions to common problems in tracker behavior and tuning.</p>
</section>
</section>
<section id="miscellaneous-data-output">
<h2>Miscellaneous Data Output<a class="headerlink" href="DS_plugin_gst-nvtracker.html#miscellaneous-data-output" title="Link to this heading">#</a></h2>
<p>Miscellaneous data provides a mechanism to return additional data to the user outside of the Gst Buffer or NvDsBatchMeta. The miscellaneous data buffer will only be populated when options are enabled for specific feature. Currently the supported types of miscellaneous data are:</p>
<ul class="simple">
<li><p>Terminated Track List</p></li>
<li><p>Shadow Tracking Target Data</p></li>
<li><p>Past-frame Target Data</p></li>
</ul>
<p>Each type of data has a unique output variable within the miscellaneous data; however, they do share a common data structure, using the unified <code class="docutils literal notranslate"><span class="pre">NvDsTargetMiscDataBatch</span></code> data structure. A buffer pool is used for its memory management, whose size can be set with <code class="docutils literal notranslate"><span class="pre">user-meta-pool-size</span></code>. When the latency for downstream plugins to release the buffers is too long, the buffer pool may be empty so tracker will skip reporting the miscellaneous data for next batch. A warning <code class="docutils literal notranslate"><span class="pre">gstnvtracker:</span> <span class="pre">Unable</span> <span class="pre">to</span> <span class="pre">acquire</span> <span class="pre">a</span> <span class="pre">user</span> <span class="pre">meta</span> <span class="pre">buffer</span></code> will be shown, and users can increase the pool size from default 32 to larger values like 64. Specifics for each type of miscellaneous data is defined below in the following sections.</p>
<section id="terminated-track-list">
<h3>Terminated Track List<a class="headerlink" href="DS_plugin_gst-nvtracker.html#terminated-track-list" title="Link to this heading">#</a></h3>
<p>Whenever a target is terminated, the full target trajectory data can be exported to the metadata as part of the miscellaneous data, which is populated in NvDsTargetMiscDataBatch data structure. This data not only informs of the termination event of a target, but also can be useful for a downstream module that performs trajectory-based analysis for each object.</p>
<p>The terminated track list can be saved in a file in either <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> or within the low-level tracker library:</p>
<ul class="simple">
<li><p>(Option 1) Saving output in <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code>:</p>
<ul>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">outputTerminatedTracks:</span> <span class="pre">1</span></code> in tracker config <code class="docutils literal notranslate"><span class="pre">TargetManagement</span></code> section</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">terminated-track-output-dir=&lt;dir</span> <span class="pre">name&gt;</span></code>  in <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> config’s application group properties</p></li>
<li><p>create the folder <code class="docutils literal notranslate"><span class="pre">&lt;dir</span> <span class="pre">name&gt;</span></code></p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> to save the terminated track history into text files in <code class="docutils literal notranslate"><span class="pre">&lt;dir</span> <span class="pre">name&gt;</span></code></p></li>
<li><p>Data format is defined below.</p></li>
</ul>
</li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><p>frame number</p></td>
<td><p>object unique id</p></td>
<td><p>class id</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>bbox left</p></td>
<td><p>bbox top</p></td>
<td><p>bbox right</p></td>
<td><p>bbox bottom</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>confidence</p></td>
<td><p>tracker state</p></td>
<td><p>visibility</p></td>
</tr>
<tr class="row-even"><td><p>unsigned int</p></td>
<td><p>long unsigned int</p></td>
<td><p>unsigned int</p></td>
<td><p>int</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>int</p></td>
<td><p>float</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><ul class="simple">
<li><p>A file will be created for each frame in each stream. Sample data is like:</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0 7 2 0 0.0 1535.194092 94.266541 1603.132812 301.653625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.000000 2 1.000000
1 7 2 0 0.0 1535.938232 94.234810 1603.121338 301.769501 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.982758 2 1.000000
...
</pre></div>
</div>
</div></blockquote>
<ul class="simple">
<li><p>(Option 2) Saving output directly from low level tracker:</p>
<ul>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">outputTerminatedTracks:</span> <span class="pre">1</span></code> in tracker config <code class="docutils literal notranslate"><span class="pre">TargetManagement</span></code> section</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">terminatedTrackFilename:</span> <span class="pre">&lt;file</span> <span class="pre">name</span> <span class="pre">prefix&gt;</span></code> in tracker config <code class="docutils literal notranslate"><span class="pre">TargetManagement</span></code> section. For example, set <code class="docutils literal notranslate"><span class="pre">terminatedTrackFilename:</span> <span class="pre">track_dump</span></code> and the saved file names will be <code class="docutils literal notranslate"><span class="pre">track_dump_0.txt</span></code>, <code class="docutils literal notranslate"><span class="pre">track_dump_1.txt</span></code>, etc.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> to save the terminated track information directly from the low-level tracker library</p></li>
<li><p>Data format is defined below. The foot location, and convex hull data are only useful if SV3DT output is enabled.</p></li>
</ul>
</li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><p>frame number(starting from 1)</p></td>
<td><p>object unique id</p></td>
<td><p>bbox left</p></td>
<td><p>bbox top</p></td>
<td><p>bbox right</p></td>
<td><p>bbox bottom</p></td>
<td><p>confidence</p></td>
<td><p>Foot World Position X</p></td>
<td><p>Foot World Position Y</p></td>
<td><p>blank</p></td>
<td><p>class id</p></td>
<td><p>tracker state</p></td>
<td><p>visibility</p></td>
<td><p>Foot Image Position X</p></td>
<td><p>Foot Image Position Y</p></td>
<td><p>ConvexHull Points (optional)</p></td>
</tr>
<tr class="row-even"><td><p>unsigned int</p></td>
<td><p>long unsigned int</p></td>
<td><p>int</p></td>
<td><p>int</p></td>
<td><p>int</p></td>
<td><p>int</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>int</p></td>
<td><p>unsigned int</p></td>
<td><p>int</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>int separated by vertical bar</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><ul class="simple">
<li><p>A file will be created for each stream. Sample data is like:</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># SV3DT output disabled
31,48,558,104,31,74,0.949,-1.000,-1.000,-1,0,0.994,-1,-1
32,48,558,104,31,74,0.951,-1.000,-1.000,-1,0,0.995,-1,-1
...

# SV3DT output enabled
31,48,558,104,31,74,0.949,1254.535,2962.867,-1,0,0.994,581,176,-15|-34|-14|-35|-13|-35|-10|-36|-6|-36|-3|-36|0|-36|1|-36|2|-35|16|35|15|35|13|36|9|37|6|37|3|37|0|37|0|36
32,48,558,104,31,74,0.951,1255.602,2968.294,-1,0,0.995,581,176,-14|-34|-14|-35|-13|-35|-10|-35|-6|-36|-3|-36|0|-36|1|-36|2|-35|15|35|15|35|13|36|9|37|6|37|3|37|0|37|0|36
...
</pre></div>
</div>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="shadow-tracking-target-data">
<h3>Shadow Tracking Target Data<a class="headerlink" href="DS_plugin_gst-nvtracker.html#shadow-tracking-target-data" title="Link to this heading">#</a></h3>
<p>As mentioned earlier, even when a target is not being associated with any of the detection bboxes, the target is still being tracked in a Shadow Tracking mode. When tracked in the Shadow Tracking mode, the target data is not being reported to the downstream because the target data may not be reliable.</p>
<p>However, the users are allowed to still report these shadow tracking target data as a part of the miscellaneous data, when <code class="docutils literal notranslate"><span class="pre">outputShadowTracks:</span> <span class="pre">1</span></code> is set under <code class="docutils literal notranslate"><span class="pre">TargetManagement</span></code> section in a tracker config file.</p>
<p>The shadow tracking data can be dumped in to a file when enabled in <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> like below:</p>
<ul class="simple">
<li><p>Add <code class="docutils literal notranslate"><span class="pre">shadow-track-output-dir=&lt;dir</span> <span class="pre">name&gt;</span></code>  in <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> config’s application group properties</p></li>
<li><p>create the folder <code class="docutils literal notranslate"><span class="pre">&lt;dir</span> <span class="pre">name&gt;</span></code></p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> to save the Shadow Track history into text files in <code class="docutils literal notranslate"><span class="pre">&lt;dir</span> <span class="pre">name&gt;</span></code></p></li>
<li><p>Data format is defined below.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><p>frame number</p></td>
<td><p>object unique id</p></td>
<td><p>class id</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>bbox left</p></td>
<td><p>bbox top</p></td>
<td><p>bbox right</p></td>
<td><p>bbox bottom</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>confidence</p></td>
<td><p>tracker state</p></td>
<td><p>visibility</p></td>
</tr>
<tr class="row-even"><td><p>unsigned int</p></td>
<td><p>long unsigned int</p></td>
<td><p>unsigned int</p></td>
<td><p>int</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>int</p></td>
<td><p>float</p></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>A file will be created for each frame in each stream. Sample data is like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1 11 2 0 0.0  296.346130 262.343445  333.428864  376.817291 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.407002 2 1.000000
1 22 2 0 0.0 1663.921875 857.167725 1752.483521 1049.053223 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.912138 2 1.000000
...
</pre></div>
</div>
</li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Both the terminated track data and the shadow tracking object data are not retained in the low-level tracker library when subsequent frames arrive. Thus, if users want to use these data, they should retrieve these miscellaneous data using the <code class="docutils literal notranslate"><span class="pre">NvMOT_RetrieveMiscData()</span></code> API at the end of every frame in the plugin. Otherwise, the data will be discarded within the tracker library.</p>
</div>
</section>
<section id="past-frame-target-data">
<h3>Past-frame Target Data<a class="headerlink" href="DS_plugin_gst-nvtracker.html#past-frame-target-data" title="Link to this heading">#</a></h3>
<p>Past-frame target data is always reported in miscellaneous data, and is appended to the current frame objects in tracker KITTI dump when enabled.</p>
<p>To enable tracker KITTI dump:</p>
<ul class="simple">
<li><p>Add <code class="docutils literal notranslate"><span class="pre">kitti-track-output-dir=&lt;dir</span> <span class="pre">name&gt;</span></code>  in <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> config’s application group properties</p></li>
<li><p>create the folder <code class="docutils literal notranslate"><span class="pre">&lt;dir</span> <span class="pre">name&gt;</span></code></p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> to save tracked objects files in <code class="docutils literal notranslate"><span class="pre">&lt;dir</span> <span class="pre">name&gt;</span></code>.</p></li>
<li><p>Data format is defined below following the KITTI format. The foot location and visibility data will append to the end of each line if SV3DT output is enabled.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><p>object Label</p></td>
<td><p>object Unique Id</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>bbox left</p></td>
<td><p>bbox top</p></td>
<td><p>bbox right</p></td>
<td><p>bbox bottom</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>blank</p></td>
<td><p>confidence</p></td>
<td><p>visibility (optional)</p></td>
<td><p>Foot Image Position X (optional)</p></td>
<td><p>Foot Image Position Y (optional)</p></td>
</tr>
<tr class="row-even"><td><p>string</p></td>
<td><p>long unsigned</p></td>
<td><p>float</p></td>
<td><p>int</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>A file will be created for each frame in each stream. Sample data is like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># SV3DT output disabled
person 0 0.0 0 0.0 1302.667236 135.852036 1340.975830 241.724579 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.917301
person 1 0.0 0 0.0 878.249023  195.080475  913.410950 320.695618 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.899029
...

# SV3DT output enabled
person 0 0.0 0 0.0 1302.667236 135.852036 1340.975830 241.724579 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.917301 0.966531 1314.492554 239.495193
person 1 0.0 0 0.0  878.249023 195.080475  913.410950 320.695618 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.899029 0.930824  899.187500 316.670013
...
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="setup-and-usage-of-re-id-model">
<h2>Setup and Usage of Re-ID Model<a class="headerlink" href="DS_plugin_gst-nvtracker.html#setup-and-usage-of-re-id-model" title="Link to this heading">#</a></h2>
<p>This section describes how to download and setup Re-ID models in different formats, perform INT8 calibration for performance optimization and output Re-ID features to downstream modules.</p>
<section id="setup-sample-re-id-models">
<h3>Setup Sample Re-ID Models<a class="headerlink" href="DS_plugin_gst-nvtracker.html#setup-sample-re-id-models" title="Link to this heading">#</a></h3>
<p>The supported Re-ID model formats are NVIDIA TAO and ONNX. Multiple ready-to-use sample models are listed below. Scripts and README file for users to setup the model are provided in <code class="docutils literal notranslate"><span class="pre">sources/tracker_ReID</span></code>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>UFF is no longer supported by TensorRT, so please migrate to TAO or ONNX models.</p>
</div>
</div></blockquote>
<section id="nvidia-tao-reidentificationnet">
<h4>NVIDIA TAO ReIdentificationNet<a class="headerlink" href="DS_plugin_gst-nvtracker.html#nvidia-tao-reidentificationnet" title="Link to this heading">#</a></h4>
<p>NVIDIA pre-trained <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/reidentificationnet">ReIdentificationNet</a> is a high accuracy ResNet-50 model with feature length 256. It can be downloaded and used directly with command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>/opt/nvidia/deepstream/deepstream/samples/models/Tracker/
wget<span class="w"> </span><span class="s1">&#39;https://api.ngc.nvidia.com/v2/models/nvidia/tao/reidentificationnet/versions/deployable_v1.0/files/resnet50_market1501.etlt&#39;</span><span class="w"> </span>-P<span class="w"> </span>/opt/nvidia/deepstream/deepstream/samples/models/Tracker/
</pre></div>
</div>
<p>The tracker config file supports this model by default. Note the raw output from this network is not L2 normalized, so <code class="docutils literal notranslate"><span class="pre">addFeatureNormalization:</span> <span class="pre">1</span></code> is set to add L2 normalization as a post processing.</p>
</section>
<section id="onnx-model">
<h4>ONNX Model<a class="headerlink" href="DS_plugin_gst-nvtracker.html#onnx-model" title="Link to this heading">#</a></h4>
<p>An open-sourced ONNX model sample is <a class="reference external" href="https://github.com/dvl-tum/GHOST">Simple Cues Lead to a Strong Multi-Object Tracker</a> , which proposes a new Re-ID model using on-the-fly domain adaptation. The network is based on ResNet-50 with feature length being 512.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>/opt/nvidia/deepstream/deepstream/samples/models/Tracker/
wget<span class="w"> </span><span class="s1">&#39;https://vision.in.tum.de/webshare/u/seidensc/GHOST/ghost_reid.onnx&#39;</span><span class="w"> </span>-P<span class="w"> </span>/opt/nvidia/deepstream/deepstream/samples/models/Tracker/
</pre></div>
</div>
<p>Then update below parameters in tracker config file <cite>ReID</cite> session (keep <cite>reidType</cite> unchanged).</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">ReID</span><span class="p">:</span>
<span class="w">  </span><span class="nt">batchSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">  </span><span class="nt">workspaceSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<span class="w">  </span><span class="nt">reidFeatureSize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512</span>
<span class="w">  </span><span class="nt">reidHistorySize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100</span>
<span class="w">  </span><span class="nt">inferDims</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">3</span><span class="p p-Indicator">,</span><span class="nv">384</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">128</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">networkMode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="c1"># [Input Preprocessing]</span>
<span class="w">  </span><span class="nt">inputOrder</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">  </span><span class="nt">colorFormat</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="w">  </span><span class="nt">offsets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">109.1250</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">102.6000</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">91.3500</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">netScaleFactor</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.01742919</span>
<span class="w">  </span><span class="nt">keepAspc</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="c1"># [Paths and Names]</span>
<span class="w">  </span><span class="nt">onnxFile</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;/opt/nvidia/deepstream/deepstream/samples/models/Tracker/ghost_reid.onnx&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="customize-re-id-model">
<h3>Customize Re-ID Model<a class="headerlink" href="DS_plugin_gst-nvtracker.html#customize-re-id-model" title="Link to this heading">#</a></h3>
<p>Users can also train a custom Re-ID model in ONNX format, whose output is a single vector for each object. Then the Re-ID similarity score will be computed based on the cosine metric and used to perform the data association in the same way as the official model. The steps are:</p>
<ul class="simple">
<li><p>Train a Re-ID network using deep learning frameworks such as PyTorch or TensorFlow.</p></li>
<li><p>Make sure the network layers are supported by TensorRT and convert the model into ONNX. Mixed precision inference is still supported, and a calibration cache is required for INT8 mode.</p></li>
<li><p>Specify the following parameters in tracker config file based on the custom model’s properties. Then run DeepStream SDK with the new Re-ID model.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">reidFeatureSize</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reidHistorySize</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inferDims</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">colorFormat</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">networkMode</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">offsets</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">netScaleFactor</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">addFeatureNormalization</span></code></p></li>
</ul>
</li>
<li><p>ONNX model must specify below parameters.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">onnxFile</span></code></p></li>
</ul>
</li>
</ul>
</section>
<section id="re-id-feature-output">
<h3>Re-ID Feature Output<a class="headerlink" href="DS_plugin_gst-nvtracker.html#re-id-feature-output" title="Link to this heading">#</a></h3>
<p>Objects’ Re-ID features can be accessed in the tracker plugin and downstream modules, which can be used for other tasks such as multi-target multi-camera tracking. Steps to retrieve those features using <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> are:</p>
<ul class="simple">
<li><p>Add <code class="docutils literal notranslate"><span class="pre">outputReidTensor:</span> <span class="pre">1</span></code> in tracker config <code class="docutils literal notranslate"><span class="pre">ReID</span></code> section. Add <code class="docutils literal notranslate"><span class="pre">reid-track-output-dir=&lt;dir</span> <span class="pre">name&gt;</span></code>  in <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> config’s application group properties and create the folder <code class="docutils literal notranslate"><span class="pre">&lt;dir</span> <span class="pre">name&gt;</span></code>.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> to save the Re-ID features in each frame into text files in <code class="docutils literal notranslate"><span class="pre">&lt;dir</span> <span class="pre">name&gt;</span></code>. In each text file, each line’s first integer is object id, and the remaining floats are its feature vector. Users can check <code class="docutils literal notranslate"><span class="pre">write_reid_track_output()</span></code> in <code class="docutils literal notranslate"><span class="pre">deepstream_app.c</span></code> to understand how these features are retrieved.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>This feature is supported whenever NvDeepSORT or Re-ID based re-association is used. To retrieve Re-ID features for every frame, make sure <code class="docutils literal notranslate"><span class="pre">interval=0</span></code> in PGIE config and <code class="docutils literal notranslate"><span class="pre">reidExtractionInterval:</span> <span class="pre">0</span></code> if re-association is enabled. Otherwise, the Re-ID features will be extracted at intervals only when PGIE generates bounding boxes and <code class="docutils literal notranslate"><span class="pre">reidExtractionInterval</span></code> is met.</p>
</section>
</section>
<section id="setup-and-usage-of-sub-batching-alpha">
<h2>Setup and Usage of Sub-batching (Alpha)<a class="headerlink" href="DS_plugin_gst-nvtracker.html#setup-and-usage-of-sub-batching-alpha" title="Link to this heading">#</a></h2>
<p>This section describes how to use the <code class="docutils literal notranslate"><span class="pre">sub-batching</span></code> feature with multiple low-level tracker config files. Two use cases are explained: the first one with varied tracker algorithms and the second one with varied compute backends.</p>
<p>Here, a <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is used as an example.</p>
<section id="use-case-1">
<h3>Use-case 1<a class="headerlink" href="DS_plugin_gst-nvtracker.html#use-case-1" title="Link to this heading">#</a></h3>
<p>This use-case illustrates an application with a batch size of 4 (i.e., 4 streams). The batch is split into 3 sub-batches: the first sub-batch of size 2 and the next two sub-batches of size 1 each. The first sub-batch uses NvDCF tracker, the second sub-batch uses NvSORT tracker, and the third sub-batch uses IOU tracker, respectively.
To achieve this, modify the <code class="docutils literal notranslate"><span class="pre">[tracker]</span></code> section in <code class="docutils literal notranslate"><span class="pre">source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt</span></code> configuration file that comes with DeepStream package as shown below :</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>tracker<span class="o">]</span>
<span class="nv">enable</span><span class="o">=</span><span class="m">1</span>
tracker-width<span class="o">=</span><span class="m">960</span>
tracker-height<span class="o">=</span><span class="m">544</span>
ll-lib-file<span class="o">=</span>/opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so

ll-config-file<span class="o">=</span>config_tracker_NvDCF_accuracy.yml<span class="p">;</span>config_tracker_NvSORT.yml<span class="p">;</span>config_tracker_IOU.yml
sub-batches<span class="o">=</span><span class="m">0</span>,1<span class="p">;</span><span class="m">2</span><span class="p">;</span><span class="m">3</span>
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">sub-batches</span></code> can also be configured using option 2 i.e. <code class="docutils literal notranslate"><span class="pre">sub-batches=2:1:1</span></code>. Refer <a class="reference internal" href="DS_plugin_gst-nvtracker.html#gst-properties">Gst Properties</a> for details.</p>
</section>
<section id="use-case-2">
<h3>Use-case 2<a class="headerlink" href="DS_plugin_gst-nvtracker.html#use-case-2" title="Link to this heading">#</a></h3>
<p>This use-case illustrates an application with a batch size of 4, where the batch is split into 2 sub-batches each of size 2: the first sub-batch uses NvDCF tracker with  setting <code class="docutils literal notranslate"><span class="pre">visualTrackerType:</span> <span class="pre">1</span></code> (i.e., existing DCF module). The second sub-batch uses NvDCF tracker with <code class="docutils literal notranslate"><span class="pre">visualTrackerType:</span> <span class="pre">2</span></code> and <code class="docutils literal notranslate"><span class="pre">vpiBackend4DcfTracker:</span> <span class="pre">2</span></code> (i.e. NvDCF_VPI tracker with PVA backend). Please note that, since this use-case configures PVA backend, it will run only on Jetson platforms.</p>
<p>Steps are :</p>
<ul>
<li><p>Create a copy of <code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_accuracy.yml</span></code> and name it as <code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_accuracy_PVA.yml</span></code></p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_accuracy_PVA.yml</span></code> modify the <code class="docutils literal notranslate"><span class="pre">VisualTracker:</span></code> section as follows :</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">VisualTracker</span><span class="p">:</span>
<span class="w">  </span><span class="nt">visualTrackerType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w">    </span><span class="c1"># the type of visual tracker among { DUMMY=0, NvDCF=1,  NvDCF_VPI=2 }</span>
<span class="w">  </span><span class="nt">vpiBackend4DcfTracker</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"> </span><span class="c1"># the type of compute backend among {CUDA=1, PVA=2}</span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">....</span>
</pre></div>
</div>
</li>
<li><p>Then modify the <code class="docutils literal notranslate"><span class="pre">[tracker]</span></code> section in <code class="docutils literal notranslate"><span class="pre">source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt</span></code> as follows :</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>tracker<span class="o">]</span>
<span class="nv">enable</span><span class="o">=</span><span class="m">1</span>
tracker-width<span class="o">=</span><span class="m">960</span>
tracker-height<span class="o">=</span><span class="m">544</span>
ll-lib-file<span class="o">=</span>/opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so

ll-config-file<span class="o">=</span>config_tracker_NvDCF_accuracy.yml<span class="p">;</span>config_tracker_NvDCF_accuracy_PVA.yml
sub-batches<span class="o">=</span><span class="m">0</span>,1<span class="p">;</span><span class="m">2</span>,3
</pre></div>
</div>
</li>
</ul>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">sub-batches</span></code> can also be configured using option 2 i.e. <code class="docutils literal notranslate"><span class="pre">sub-batches=2:2</span></code>. Refer <a class="reference internal" href="DS_plugin_gst-nvtracker.html#gst-properties">Gst Properties</a> for details.</p>
<p>For further details on implementation of sub-batches feature, please refer to a writeup at the end of <code class="docutils literal notranslate"><span class="pre">nvtracker_proc.cpp</span></code> in <code class="docutils literal notranslate"><span class="pre">nvtracker</span></code> plugin’s source code.</p>
<p>The most optimal <code class="docutils literal notranslate"><span class="pre">sub-batches</span></code> configuration for a pipeline depends on multiple factors like elements in the pipeline, configuration of each element, hardware configuration etc. Increasing the number of sub-batches parallelizes the processing of streams in a batch. But it also adds an overhead. Hence, the number of sub-batches to configure needs to be determined experimentally by comparing GPU/PVA utilization and performance across various <code class="docutils literal notranslate"><span class="pre">sub-batches</span></code> configurations. A thumb rule is to start with a single batch and keep splitting it into sub-batches until an optimal performance point is reached.</p>
</section>
</section>
<section id="setup-and-visualization-of-tracker-sample-pipelines">
<h2>Setup and Visualization of Tracker Sample Pipelines<a class="headerlink" href="DS_plugin_gst-nvtracker.html#setup-and-visualization-of-tracker-sample-pipelines" title="Link to this heading">#</a></h2>
<p>This section describes how to setup a multi-object tracking pipeline with various NVIDIA® pre-trained detector models and DeepStream multi-object trackers, and provides ready-to-use detector and tracker config files optimized for high accuracy tracking. The optimal tracker configs for People tracking (e.g., config_tracker_NvSORT.yml, config_tracker_NvDeepSORT.yml, config_tracker_NvDCF_accuracy.yml, etc.) are already provided in DeepStream release package, so here we present optimized detector parameters only. Then the visualization of some sample outputs and internal states (such as correlation responses for a few selected targets) are presented to help users to better understand how NvDsTracker works, especially on the visual tracker module. In addition, we present detector config params and tracker config params for vehicle tracking use-case as well.</p>
<section id="people-tracking">
<h3>People Tracking<a class="headerlink" href="DS_plugin_gst-nvtracker.html#people-tracking" title="Link to this heading">#</a></h3>
<p>NVIDIA® pre-trained <cite>PeopleNet</cite> detects person, bag, and face classes. The pre-trained model with ResNet-34 backbone is on <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet">NVIDIA NGC catalog</a>. It can be used in PGIE module in conjunction with various low level trackers for people tracking.</p>
<section id="setup">
<h4>Setup<a class="headerlink" href="DS_plugin_gst-nvtracker.html#setup" title="Link to this heading">#</a></h4>
<p>Here <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is used as an example, which can be naturally extended to other applications. Steps are:</p>
<ul class="simple">
<li><p>Download the detector model files <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet/files?version=deployable_quantized_onnx_v2.6.2">here</a>, and place the files under <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/samples/models/peoplenet</span></code></p></li>
<li><p>Copy detector config file in below sections <code class="docutils literal notranslate"><span class="pre">config_infer_primary_PeopleNet.txt</span></code> for the specific low level tracker (for example <a class="reference internal" href="DS_plugin_gst-nvtracker.html#peoplenet-nvdcf">PeopleNet + NvDCF</a>) into work directory: <code class="docutils literal notranslate"><span class="pre">cp</span> <span class="pre">config_infer_primary_PeopleNet.txt</span> <span class="pre">/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app</span></code></p></li>
<li><p>Assuming <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> config file is <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/deepstream_app_config.txt</span></code>, set PGIE and tracker config files in it:</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<details>
<summary><a>deepstream_app_config.txt</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">## Other groups</span>

<span class="o">[</span>primary-gie<span class="o">]</span>
<span class="c1">## Use PeopleNet as PGIE</span>
config-file<span class="o">=</span>config_infer_primary_PeopleNet.txt
<span class="c1">## Other [primary-gie] configs</span>

<span class="o">[</span>tracker<span class="o">]</span>
<span class="c1">## Specify the low level tracker (for example NvSORT)</span>
<span class="c1"># ll-config-file=config_tracker_IOU.yml</span>
ll-config-file<span class="o">=</span>config_tracker_NvSORT.yml
<span class="c1"># ll-config-file=config_tracker_NvDCF_perf.yml</span>
<span class="c1"># ll-config-file=config_tracker_NvDCF_accuracy.yml</span>
<span class="c1"># ll-config-file=config_tracker_NvDeepSORT.yml</span>
<span class="c1">## Other [tracker] configs</span>
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream</span></code> is the default DeepStream installation directory. The paths will be different if the user sets up in different directory.</p>
</section>
<section id="peoplenet-nvsort">
<h4>PeopleNet + NvSORT<a class="headerlink" href="DS_plugin_gst-nvtracker.html#peoplenet-nvsort" title="Link to this heading">#</a></h4>
<p>This pipeline performs high performance people tracking with reasonable accuracy. Such a <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is constructed with the following components:</p>
<ul class="simple">
<li><p><strong>Detector</strong>: PeopleNet v2.6.2 (w/ ResNet-34 as backbone)</p></li>
<li><p><strong>Post-processing</strong> algorithm for object detection: Hybrid clustering (i.e., DBSCAN + NMS)</p></li>
<li><p><strong>Tracker</strong>: NvSORT with <code class="docutils literal notranslate"><span class="pre">config_tracker_NvSORT.yml</span></code> configuration in DeepStream release</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>A set of recommended detector config params for PeopleNet v2.6.2 to be used with NvSORT tracker is:</p>
<details>
<summary><a>config_infer_primary_PeopleNet.txt</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>property<span class="o">]</span>
<span class="c1">## model-specific params. The paths will be different if the user sets up in different directory.</span>
int8-calib-file<span class="o">=</span>../../models/peoplenet/resnet34_peoplenet_int8.txt
labelfile-path<span class="o">=</span>../../models/peoplenet/labels.txt
onnx-file<span class="o">=</span>../../models/peoplenet/resnet34_peoplenet.onnx
tlt-model-key<span class="o">=</span>tlt_encode

gpu-id<span class="o">=</span><span class="m">0</span>
net-scale-factor<span class="o">=</span><span class="m">0</span>.00392156862745098
input-dims<span class="o">=</span><span class="m">3</span><span class="p">;</span><span class="m">544</span><span class="p">;</span><span class="m">960</span>
process-mode<span class="o">=</span><span class="m">1</span>
model-color-format<span class="o">=</span><span class="m">0</span>
<span class="c1">## 0=FP32, 1=INT8, 2=FP16 mode</span>
network-mode<span class="o">=</span><span class="m">1</span>
num-detected-classes<span class="o">=</span><span class="m">3</span>
<span class="nv">interval</span><span class="o">=</span><span class="m">0</span>
gie-unique-id<span class="o">=</span><span class="m">1</span>
<span class="c1">## 1=DBSCAN, 2=NMS, 3= DBSCAN+NMS Hybrid, 4 = None(No clustering)</span>
cluster-mode<span class="o">=</span><span class="m">3</span>
maintain-aspect-ratio<span class="o">=</span><span class="m">1</span>

<span class="o">[</span>class-attrs-all<span class="o">]</span>
pre-cluster-threshold<span class="o">=</span><span class="m">0</span>.1555
nms-iou-threshold<span class="o">=</span><span class="m">0</span>.3386
<span class="nv">minBoxes</span><span class="o">=</span><span class="m">2</span>
dbscan-min-score<span class="o">=</span><span class="m">1</span>.9224
<span class="nv">eps</span><span class="o">=</span><span class="m">0</span>.3596
detected-min-w<span class="o">=</span><span class="m">20</span>
detected-min-h<span class="o">=</span><span class="m">20</span>
</pre></div>
</div>
</details></section>
<section id="peoplenet-nvdeepsort">
<h4>PeopleNet + NvDeepSORT<a class="headerlink" href="DS_plugin_gst-nvtracker.html#peoplenet-nvdeepsort" title="Link to this heading">#</a></h4>
<p>This pipeline enables people Re-ID capability during tracking. Such a <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is constructed with the following components:</p>
<ul class="simple">
<li><p><strong>Detector</strong>: PeopleNet v2.6.2 (w/ ResNet-34 as backbone)</p></li>
<li><p><strong>Post-processing</strong> algorithm for object detection: Hybrid clustering (i.e., DBSCAN + NMS)</p></li>
<li><p><strong>Tracker</strong>: NvDeepSORT with <code class="docutils literal notranslate"><span class="pre">config_tracker_NvDeepSORT.yml</span></code> configuration in DeepStream release</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>A set of recommended detector config params for PeopleNet v2.6.2 to be used with NvDeepSORT tracker is:</p>
<details>
<summary><a>config_infer_primary_PeopleNet.txt</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>property<span class="o">]</span>
<span class="c1">## model-specific params. The paths will be different if the user sets up in different directory.</span>
int8-calib-file<span class="o">=</span>../../models/peoplenet/resnet34_peoplenet_int8.txt
labelfile-path<span class="o">=</span>../../models/peoplenet/labels.txt
onnx-file<span class="o">=</span>../../models/peoplenet/resnet34_peoplenet.onnx
tlt-model-key<span class="o">=</span>tlt_encode

gpu-id<span class="o">=</span><span class="m">0</span>
net-scale-factor<span class="o">=</span><span class="m">0</span>.00392156862745098
input-dims<span class="o">=</span><span class="m">3</span><span class="p">;</span><span class="m">544</span><span class="p">;</span><span class="m">960</span>
process-mode<span class="o">=</span><span class="m">1</span>
model-color-format<span class="o">=</span><span class="m">0</span>
<span class="c1">## 0=FP32, 1=INT8, 2=FP16 mode</span>
network-mode<span class="o">=</span><span class="m">1</span>
num-detected-classes<span class="o">=</span><span class="m">3</span>
<span class="nv">interval</span><span class="o">=</span><span class="m">0</span>
gie-unique-id<span class="o">=</span><span class="m">1</span>
<span class="c1">## 1=DBSCAN, 2=NMS, 3= DBSCAN+NMS Hybrid, 4 = None(No clustering)</span>
cluster-mode<span class="o">=</span><span class="m">3</span>
maintain-aspect-ratio<span class="o">=</span><span class="m">1</span>

<span class="o">[</span>class-attrs-all<span class="o">]</span>
pre-cluster-threshold<span class="o">=</span><span class="m">0</span>.1653
nms-iou-threshold<span class="o">=</span><span class="m">0</span>.5242
<span class="nv">minBoxes</span><span class="o">=</span><span class="m">2</span>
dbscan-min-score<span class="o">=</span><span class="m">1</span>.7550
<span class="nv">eps</span><span class="o">=</span><span class="m">0</span>.1702
detected-min-w<span class="o">=</span><span class="m">20</span>
detected-min-h<span class="o">=</span><span class="m">20</span>
</pre></div>
</div>
</details></section>
<section id="peoplenet-nvdcf">
<h4>PeopleNet + NvDCF<a class="headerlink" href="DS_plugin_gst-nvtracker.html#peoplenet-nvdcf" title="Link to this heading">#</a></h4>
<p>This pipeline performs more accurate people tracking. For the output visualization, a <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is first constructed with the following components:</p>
<ul class="simple">
<li><p><strong>Detector</strong>: PeopleNet v2.6.2 (w/ ResNet-34 as backbone)</p></li>
<li><p><strong>Post-processing</strong> algorithm for object detection: Hybrid clustering (i.e., DBSCAN + NMS)</p></li>
<li><p><strong>Tracker</strong>: NvDCF with <code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_accuracy.yml</span></code> configuration in DeepStream release</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For better visualization, the following changes were also made:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">featureImgSizeLevel:</span> <span class="pre">5</span></code> is set under <code class="docutils literal notranslate"><span class="pre">VisualTracker</span></code> section in <code class="docutils literal notranslate"><span class="pre">config_tracker_NvDCF_accuracy.yml</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tracker-height=960</span></code> and <code class="docutils literal notranslate"><span class="pre">tracker-width=544</span></code> under <code class="docutils literal notranslate"><span class="pre">[tracker]</span></code> section in the deepstream-app config file</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>A set of recommended detector config params for PeopleNet v2.6.2 to be used with NvDCF_accuracy tracker is:</p>
<details>
<summary><a>config_infer_primary_PeopleNet.txt</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>property<span class="o">]</span>
<span class="c1">## model-specific params. The paths will be different if the user sets up in different directory.</span>
int8-calib-file<span class="o">=</span>../../models/peoplenet/resnet34_peoplenet_int8.txt
labelfile-path<span class="o">=</span>../../models/peoplenet/labels.txt
onnx-file<span class="o">=</span>../../models/peoplenet/resnet34_peoplenet.onnx
tlt-model-key<span class="o">=</span>tlt_encode

gpu-id<span class="o">=</span><span class="m">0</span>
net-scale-factor<span class="o">=</span><span class="m">0</span>.00392156862745098
input-dims<span class="o">=</span><span class="m">3</span><span class="p">;</span><span class="m">544</span><span class="p">;</span><span class="m">960</span>
process-mode<span class="o">=</span><span class="m">1</span>
model-color-format<span class="o">=</span><span class="m">0</span>
<span class="c1">## 0=FP32, 1=INT8, 2=FP16 mode</span>
network-mode<span class="o">=</span><span class="m">1</span>
num-detected-classes<span class="o">=</span><span class="m">3</span>
<span class="nv">interval</span><span class="o">=</span><span class="m">0</span>
gie-unique-id<span class="o">=</span><span class="m">1</span>
<span class="c1">## 1=DBSCAN, 2=NMS, 3= DBSCAN+NMS Hybrid, 4 = None(No clustering)</span>
cluster-mode<span class="o">=</span><span class="m">3</span>
maintain-aspect-ratio<span class="o">=</span><span class="m">1</span>

<span class="o">[</span>class-attrs-all<span class="o">]</span>
pre-cluster-threshold<span class="o">=</span><span class="m">0</span>.1429
nms-iou-threshold<span class="o">=</span><span class="m">0</span>.4688
<span class="nv">minBoxes</span><span class="o">=</span><span class="m">3</span>
dbscan-min-score<span class="o">=</span><span class="m">0</span>.7726
<span class="nv">eps</span><span class="o">=</span><span class="m">0</span>.2538
detected-min-w<span class="o">=</span><span class="m">20</span>
detected-min-h<span class="o">=</span><span class="m">20</span>
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p>The resulting output video of the aforementioned pipeline with (PeopleNet + Hybrid clustering + NvDCF) is shown below, but please note that only ‘Person’-class objects are detected and shown in the video:</p>
<div style="width: 100%;display: inline-block;position: relative; padding:2px;border:2px solid white;">
     <div id="dummy" style="margin-top: 56%;"></div>
     <div align="center"></div>
       <div id="kaltura_player_9" style="position:absolute;top:0;left:0;left: 0;right: 0;bottom:0;"></div>
       <script type="text/javascript" src="https://cdnapisec.kaltura.com/p/2935771/embedPlaykitJs/uiconf_id/50468382"></script>
       <script type="text/javascript">
           try {
           var kalturaPlayer = KalturaPlayer.setup({
           targetId: "kaltura_player_9",
           provider:
           { partnerId: 2935771, uiConfId: 50468382 }
           });
           kalturaPlayer.loadMedia(
           {entryId: '1_krcd55yf'}
           );
           } catch (e)
           { console.error(e.message) }
       </script>
     </div><div class="line-block">
<div class="line"><br /></div>
</div>
<p>While the video above shows the <cite>per-stream</cite> output, each animated figure below shows (1) the cropped &amp; scaled image patch used for <cite>each target</cite> on the left side and (2) the corresponding correlation response map for the target on the right side. As mentioned earlier, the yellow <code class="docutils literal notranslate"><span class="pre">+</span></code> mark shows the peak location of the correlation response map generated by using the learned correlation filter, while the puple <code class="docutils literal notranslate"><span class="pre">x</span></code> marks show the the center of nearby detector objects.</p>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><p><strong>Person 1</strong> (w/ Blue hat + gray backpack)</p>
<img alt="../_images/NvDCF_PNv2.3_lvl5_reassoc_CorrResp%5B1%5D.gif" src="../_images/NvDCF_PNv2.3_lvl5_reassoc_CorrResp[1].gif" />
</td>
<td><p><strong>Person 6</strong> (w/ Red jacket + gray backpack)</p>
<img alt="../_images/NvDCF_PNv2.3_lvl5_reassoc_CorrResp%5B6%5D.gif" src="../_images/NvDCF_PNv2.3_lvl5_reassoc_CorrResp[6].gif" />
</td>
</tr>
<tr class="row-even"><td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><strong>Person 4</strong> (w/ Green jacket)</p>
<img alt="../_images/NvDCF_PNv2.3_lvl5_reassoc_CorrResp%5B4%5D.gif" src="../_images/NvDCF_PNv2.3_lvl5_reassoc_CorrResp[4].gif" />
</td>
<td><p><strong>Person 5</strong> (w/ Cyan jacket)</p>
<img alt="../_images/NvDCF_PNv2.3_lvl5_reassoc_CorrResp%5B5%5D.gif" src="../_images/NvDCF_PNv2.3_lvl5_reassoc_CorrResp[5].gif" />
</td>
</tr>
</tbody>
</table>
</div>
<p>The figures above show how the correlation responses progress over time for the cases of no occlusion, partial occlusion, and full occlusions happening. It can be seen that even when a target undergoes a full occlusion for a prolonged period, the NvDCF tracker is able to keep track of the targets in many cases.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">featureImgSizeLevel:</span> <span class="pre">3</span></code> is used instead for better performance, the resolution of the image patch used for each target would get lower like shown in the figure below.</p>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><p><strong>Person 1</strong> (w/ Blue hat + gray backpack)</p>
<img alt="../_images/NvDCF_PNv2.3_lvl3_reassoc_CorrResp%5B1%5D.gif" src="../_images/NvDCF_PNv2.3_lvl3_reassoc_CorrResp[1].gif" />
</td>
<td><p><strong>Person 6</strong> (w/ Red jacket + gray backpack)</p>
<img alt="../_images/NvDCF_PNv2.3_lvl3_reassoc_CorrResp%5B6%5D.gif" src="../_images/NvDCF_PNv2.3_lvl3_reassoc_CorrResp[6].gif" />
</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="vehicle-tracking">
<h3>Vehicle Tracking<a class="headerlink" href="DS_plugin_gst-nvtracker.html#vehicle-tracking" title="Link to this heading">#</a></h3>
<p>To perform tracking of vehicle and other types of targets (such as pedestrians, bicycles and road signs), a NVIDIA® pre-trained detector model is available in NGC:</p>
<ul class="simple">
<li><p><cite>TrafficCamNet</cite>: A newer model based on ResNet-18 backbone with higher detection accuracy on <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/trafficcamnet">NVIDIA NGC</a>.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The following samples demonstrate using <cite>TrafficCamNet</cite> and various trackers with different detection intervals for performance and accuracy tradeoff.</p>
<section id="id13">
<h4>Setup<a class="headerlink" href="DS_plugin_gst-nvtracker.html#id13" title="Link to this heading">#</a></h4>
<p>Here <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is used as an example, which can be naturally extended to other applications. Steps to setup a pipeline are:</p>
<ul class="simple">
<li><p>For <cite>TrafficCamNet</cite>, download the detector model files <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/trafficcamnet/files?version=pruned_onnx_v1.0.3">here</a>, and place the files under <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/samples/models/trafficcamnet</span></code></p></li>
<li><p>Copy detector and tracker config files in below sections for the specific tracker type (for example <a class="reference internal" href="DS_plugin_gst-nvtracker.html#trafficcamnet-nvdcf">TrafficCamNet + NvDCF</a>) into work directory: <code class="docutils literal notranslate"><span class="pre">cp</span> <span class="pre">config_infer_primary_TrafficCamNet.txt</span> <span class="pre">/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app</span></code></p></li>
<li><p>Assuming <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> config file is <code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream/deepstream/samples/configs/deepstream-app/deepstream_app_config.txt</span></code>, set detector and tracker config files in it like the following:</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<details>
<summary><a>deepstream_app_config.txt</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">## Other groups</span>

<span class="o">[</span>primary-gie<span class="o">]</span>
<span class="c1">## Use TrafficCamNet as PGIE</span>
config-file<span class="o">=</span>config_infer_primary_TrafficCamNet.txt
<span class="c1">## Other [primary-gie] configs</span>

<span class="o">[</span>tracker<span class="o">]</span>
<span class="c1">## Specify the low level tracker (for example NvDCF_accuracy)</span>
<span class="c1"># ll-config-file=config_tracker_IOU.yml</span>
<span class="c1"># ll-config-file=config_tracker_NvSORT.yml</span>
<span class="c1"># ll-config-file=config_tracker_NvDCF_perf.yml</span>
ll-config-file<span class="o">=</span>config_tracker_NvDCF_accuracy.yml
<span class="c1"># ll-config-file=config_tracker_NvDeepSORT.yml</span>
<span class="c1">## Other [tracker] configs</span>
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">/opt/nvidia/deepstream</span></code> is the default DeepStream installation directory. The paths will be different if the user sets up in different directory.</p>
</section>
<section id="trafficcamnet-nvsort">
<h4>TrafficCamNet + NvSORT<a class="headerlink" href="DS_plugin_gst-nvtracker.html#trafficcamnet-nvsort" title="Link to this heading">#</a></h4>
<p>This pipeline performs high performance vehicle tracking with reasonable accuracy. Such a <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is constructed with the following components:</p>
<ul class="simple">
<li><p><strong>Detector</strong>: TrafficCamNet v1.0.3 (w/ ResNet-18 as backbone)</p></li>
<li><p><strong>Post-processing</strong> algorithm for object detection: Hybrid clustering (i.e., DBSCAN + NMS)</p></li>
<li><p><strong>Tracker</strong>: NvSORT with configuration as below</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The detector and tracker config files:</p>
<details>
<summary><a>config_infer_primary_TrafficCamNet.txt</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>property<span class="o">]</span>
<span class="c1">## model-specific params. The paths will be different if the user sets up in different directory.</span>
int8-calib-file<span class="o">=</span>../../models/trafficcamnet/resnet18_trafficcamnet_pruned_int8.txt
labelfile-path<span class="o">=</span>../../models/trafficcamnet/labels.txt
onnx-file<span class="o">=</span>../../models/trafficcamnet/resnet18_trafficcamnet_pruned.onnx
tlt-model-key<span class="o">=</span>tlt_encode

gpu-id<span class="o">=</span><span class="m">0</span>
net-scale-factor<span class="o">=</span><span class="m">0</span>.00392156862745098
input-dims<span class="o">=</span><span class="m">3</span><span class="p">;</span><span class="m">544</span><span class="p">;</span><span class="m">960</span>
process-mode<span class="o">=</span><span class="m">1</span>
model-color-format<span class="o">=</span><span class="m">0</span>
<span class="c1">## 0=FP32, 1=INT8, 2=FP16 mode</span>
network-mode<span class="o">=</span><span class="m">1</span>
num-detected-classes<span class="o">=</span><span class="m">4</span>
<span class="nv">interval</span><span class="o">=</span><span class="m">0</span>
gie-unique-id<span class="o">=</span><span class="m">1</span>
<span class="c1">## 1=DBSCAN, 2=NMS, 3= DBSCAN+NMS Hybrid, 4 = None(No clustering)</span>
cluster-mode<span class="o">=</span><span class="m">3</span>
maintain-aspect-ratio<span class="o">=</span><span class="m">0</span>

<span class="o">[</span>class-attrs-all<span class="o">]</span>
pre-cluster-threshold<span class="o">=</span><span class="m">0</span>.2327
nms-iou-threshold<span class="o">=</span><span class="m">0</span>.1760
<span class="nv">minBoxes</span><span class="o">=</span><span class="m">2</span>
dbscan-min-score<span class="o">=</span><span class="m">0</span>.7062
<span class="nv">eps</span><span class="o">=</span><span class="m">0</span>.4807
detected-min-w<span class="o">=</span><span class="m">20</span>
detected-min-h<span class="o">=</span><span class="m">20</span>
</pre></div>
</div>
</details><details>
<summary><a>config_tracker_NvSORT.yml</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%YAML:1.0

BaseConfig:
<span class="w">  </span>minDetectorConfidence:<span class="w"> </span><span class="m">0</span>.0415
TargetManagement:
<span class="w">  </span>enableBboxUnClipping:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>maxTargetsPerStream:<span class="w"> </span><span class="m">300</span>
<span class="w">  </span>minIouDiff4NewTarget:<span class="w"> </span><span class="m">0</span>.6974
<span class="w">  </span>minTrackerConfidence:<span class="w"> </span><span class="m">0</span>.8049
<span class="w">  </span>probationAge:<span class="w"> </span><span class="m">5</span>
<span class="w">  </span>maxShadowTrackingAge:<span class="w"> </span><span class="m">42</span>
<span class="w">  </span>earlyTerminationAge:<span class="w"> </span><span class="m">1</span>
TrajectoryManagement:
<span class="w">  </span>useUniqueID:<span class="w"> </span><span class="m">0</span>
DataAssociator:
<span class="w">  </span>dataAssociatorType:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>associationMatcherType:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>checkClassMatch:<span class="w"> </span><span class="m">1</span>
minMatchingScore4Overall:<span class="w"> </span><span class="m">0</span>.2042
<span class="w">  </span>minMatchingScore4SizeSimilarity:<span class="w"> </span><span class="m">0</span>.2607
<span class="w">  </span>minMatchingScore4Iou:<span class="w"> </span><span class="m">0</span>.3708
<span class="w">  </span>matchingScoreWeight4SizeSimilarity:<span class="w"> </span><span class="m">0</span>.2639
<span class="w">  </span>matchingScoreWeight4Iou:<span class="w"> </span><span class="m">0</span>.4384
<span class="w">  </span>tentativeDetectorConfidence:<span class="w"> </span><span class="m">0</span>.1054
<span class="w">  </span>minMatchingScore4TentativeIou:<span class="w"> </span><span class="m">0</span>.4953
<span class="w">  </span>usePrediction4Assoc:<span class="w"> </span><span class="m">1</span>
StateEstimator:
<span class="w">  </span>stateEstimatorType:<span class="w"> </span><span class="m">2</span>
<span class="w">  </span>noiseWeightVar4Loc:<span class="w"> </span><span class="m">0</span>.0853
<span class="w">  </span>noiseWeightVar4Vel:<span class="w"> </span><span class="m">0</span>.0061
<span class="w">  </span>useAspectRatio:<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</details></section>
<section id="trafficcamnet-nvdeepsort">
<h4>TrafficCamNet + NvDeepSORT<a class="headerlink" href="DS_plugin_gst-nvtracker.html#trafficcamnet-nvdeepsort" title="Link to this heading">#</a></h4>
<p>This pipeline enables vehicle Re-ID capability during tracking. Such a <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is constructed with the following components:</p>
<ul class="simple">
<li><p><strong>Detector</strong>: TrafficCamNet v1.0.3 (w/ ResNet-18 as backbone)</p></li>
<li><p><strong>Post-processing</strong> algorithm for object detection: Hybrid clustering (i.e., DBSCAN + NMS)</p></li>
<li><p><strong>Tracker</strong>: NvDeepSORT with configuration as below. No vehicle Re-ID models are included in DeepStream so far, so the user needs to follow <a class="reference internal" href="DS_plugin_gst-nvtracker.html#customize-re-id-model">Customize Re-ID Model</a> to setup a vehicle Re-ID model and change <cite>ReID</cite> section in tracker config.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The detector and tracker config files:</p>
<details>
<summary><a>config_infer_primary_TrafficCamNet.txt</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>property<span class="o">]</span>
<span class="c1">## model-specific params. The paths will be different if the user sets up in different directory.</span>
int8-calib-file<span class="o">=</span>../../models/trafficcamnet/resnet18_trafficcamnet_pruned_int8.txt
labelfile-path<span class="o">=</span>../../models/trafficcamnet/labels.txt
onnx-file<span class="o">=</span>../../models/trafficcamnet/resnet18_trafficcamnet_pruned.onnx
tlt-model-key<span class="o">=</span>tlt_encode

gpu-id<span class="o">=</span><span class="m">0</span>
net-scale-factor<span class="o">=</span><span class="m">0</span>.00392156862745098
input-dims<span class="o">=</span><span class="m">3</span><span class="p">;</span><span class="m">544</span><span class="p">;</span><span class="m">960</span>
process-mode<span class="o">=</span><span class="m">1</span>
model-color-format<span class="o">=</span><span class="m">0</span>
<span class="c1">## 0=FP32, 1=INT8, 2=FP16 mode</span>
network-mode<span class="o">=</span><span class="m">1</span>
num-detected-classes<span class="o">=</span><span class="m">4</span>
<span class="nv">interval</span><span class="o">=</span><span class="m">0</span>
gie-unique-id<span class="o">=</span><span class="m">1</span>
<span class="c1">## 1=DBSCAN, 2=NMS, 3= DBSCAN+NMS Hybrid, 4 = None(No clustering)</span>
cluster-mode<span class="o">=</span><span class="m">3</span>
maintain-aspect-ratio<span class="o">=</span><span class="m">0</span>

<span class="o">[</span>class-attrs-all<span class="o">]</span>
pre-cluster-threshold<span class="o">=</span><span class="m">0</span>.1654
nms-iou-threshold<span class="o">=</span><span class="m">0</span>.7614
<span class="nv">minBoxes</span><span class="o">=</span><span class="m">3</span>
dbscan-min-score<span class="o">=</span><span class="m">2</span>.4240
<span class="nv">eps</span><span class="o">=</span><span class="m">0</span>.3615
detected-min-w<span class="o">=</span><span class="m">20</span>
detected-min-h<span class="o">=</span><span class="m">20</span>
</pre></div>
</div>
</details><details>
<summary><a>config_tracker_NvDeepSORT.yml</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%YAML:1.0

BaseConfig:
<span class="w">  </span>minDetectorConfidence:<span class="w"> </span><span class="m">0</span>.0451
TargetManagement:
<span class="w">  </span>preserveStreamUpdateOrder:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>maxTargetsPerStream:<span class="w"> </span><span class="m">150</span>
<span class="w">  </span>minIouDiff4NewTarget:<span class="w"> </span><span class="m">0</span>.0602
<span class="w">  </span>minTrackerConfidence:<span class="w"> </span><span class="m">0</span>.7312
<span class="w">  </span>probationAge:<span class="w"> </span><span class="m">9</span>
<span class="w">  </span>maxShadowTrackingAge:<span class="w"> </span><span class="m">59</span>
<span class="w">  </span>earlyTerminationAge:<span class="w"> </span><span class="m">1</span>
TrajectoryManagement:
<span class="w">  </span>useUniqueID:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>DataAssociator:
<span class="w">  </span>dataAssociatorType:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>associationMatcherType:<span class="w"> </span><span class="m">1</span>
checkClassMatch:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>thresholdMahalanobis:<span class="w"> </span><span class="m">34</span>.3052
<span class="w">  </span>minMatchingScore4Overall:<span class="w"> </span><span class="m">0</span>.0231
<span class="w">  </span>minMatchingScore4SizeSimilarity:<span class="w"> </span><span class="m">0</span>.3104
<span class="w">  </span>minMatchingScore4Iou:<span class="w"> </span><span class="m">0</span>.3280
<span class="w">  </span>minMatchingScore4ReidSimilarity:<span class="w"> </span><span class="m">0</span>.6805
<span class="w">  </span>matchingScoreWeight4SizeSimilarity:<span class="w"> </span><span class="m">0</span>.7103
<span class="w">  </span>matchingScoreWeight4Iou:<span class="w"> </span><span class="m">0</span>.5429
<span class="w">  </span>matchingScoreWeight4ReidSimilarity:<span class="w"> </span><span class="m">0</span>.6408
<span class="w">  </span>tentativeDetectorConfidence:<span class="w"> </span><span class="m">0</span>.0483
<span class="w">  </span>minMatchingScore4TentativeIou:<span class="w"> </span><span class="m">0</span>.5093
StateEstimator:
<span class="w">  </span>stateEstimatorType:<span class="w"> </span><span class="m">2</span>
<span class="w">  </span>noiseWeightVar4Loc:<span class="w"> </span><span class="m">0</span>.0739
<span class="w">  </span>noiseWeightVar4Vel:<span class="w"> </span><span class="m">0</span>.0097
<span class="w">  </span>useAspectRatio:<span class="w"> </span><span class="m">1</span>
ReID:<span class="w"> </span><span class="c1"># need customization</span>
<span class="w">  </span>reidType:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>batchSize:<span class="w"> </span><span class="m">100</span>
<span class="w">  </span>workspaceSize:<span class="w"> </span><span class="m">1000</span>
<span class="w">  </span>reidFeatureSize:<span class="w"> </span><span class="m">128</span>
<span class="w">  </span>reidHistorySize:<span class="w"> </span><span class="m">100</span>
<span class="w">  </span>inferDims:<span class="w"> </span><span class="o">[</span><span class="m">128</span>,<span class="w"> </span><span class="m">64</span>,<span class="w"> </span><span class="m">3</span><span class="o">]</span>
<span class="w">  </span>networkMode:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>inputOrder:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>colorFormat:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>offsets:<span class="w"> </span><span class="o">[</span><span class="m">0</span>.0,<span class="w"> </span><span class="m">0</span>.0,<span class="w"> </span><span class="m">0</span>.0<span class="o">]</span>
<span class="w">  </span>netScaleFactor:<span class="w"> </span><span class="m">1</span>.0000
<span class="w">  </span>keepAspc:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span><span class="c1"># custom Re-ID model path</span>
</pre></div>
</div>
</details></section>
<section id="trafficcamnet-nvdcf">
<h4>TrafficCamNet + NvDCF<a class="headerlink" href="DS_plugin_gst-nvtracker.html#trafficcamnet-nvdcf" title="Link to this heading">#</a></h4>
<p>This pipeline performs more accurate vehicle tracking. For the output visualization, a <code class="docutils literal notranslate"><span class="pre">deepstream-app</span></code> pipeline is first constructed with the following components:</p>
<ul class="simple">
<li><p><strong>Detector</strong>: TrafficCamNet v1.0.3 (w/ ResNet-18 as backbone)</p></li>
<li><p><strong>Post-processing</strong> algorithm for object detection: Hybrid clustering (i.e., DBSCAN + NMS)</p></li>
<li><p><strong>Tracker</strong>: NvDCF with configuration as below. No vehicle Re-ID models are included in DeepStream so far, so the user needs to follow <a class="reference internal" href="DS_plugin_gst-nvtracker.html#customize-re-id-model">Customize Re-ID Model</a> to setup a vehicle Re-ID model and change <cite>ReID</cite> section in tracker config.</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The detector and tracker config files:</p>
<details>
<summary><a>config_infer_primary_TrafficCamNet.txt</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>property<span class="o">]</span>
<span class="c1">## model-specific params. The paths will be different if the user sets up in different directory.</span>
int8-calib-file<span class="o">=</span>../../models/trafficcamnet/trafficcamnet_int8.txt
labelfile-path<span class="o">=</span>../../models/trafficcamnet/labels.txt
tlt-encoded-model<span class="o">=</span>../../models/trafficcamnet/resnet18_trafficcamnet_pruned.etlt
tlt-model-key<span class="o">=</span>tlt_encode

gpu-id<span class="o">=</span><span class="m">0</span>
net-scale-factor<span class="o">=</span><span class="m">0</span>.00392156862745098
input-dims<span class="o">=</span><span class="m">3</span><span class="p">;</span><span class="m">544</span><span class="p">;</span><span class="m">960</span>
uff-input-blob-name<span class="o">=</span>input_1
process-mode<span class="o">=</span><span class="m">1</span>
model-color-format<span class="o">=</span><span class="m">0</span>
<span class="c1">## 0=FP32, 1=INT8, 2=FP16 mode</span>
network-mode<span class="o">=</span><span class="m">1</span>
num-detected-classes<span class="o">=</span><span class="m">4</span>
<span class="nv">interval</span><span class="o">=</span><span class="m">0</span>
gie-unique-id<span class="o">=</span><span class="m">1</span>
output-blob-names<span class="o">=</span>output_cov/Sigmoid<span class="p">;</span>output_bbox/BiasAdd
<span class="c1">## 1=DBSCAN, 2=NMS, 3= DBSCAN+NMS Hybrid, 4 = None(No clustering)</span>
cluster-mode<span class="o">=</span><span class="m">3</span>
maintain-aspect-ratio<span class="o">=</span><span class="m">0</span>

<span class="o">[</span>class-attrs-all<span class="o">]</span>
pre-cluster-threshold<span class="o">=</span><span class="m">0</span>.3034
nms-iou-threshold<span class="o">=</span><span class="m">0</span>.5002
<span class="nv">minBoxes</span><span class="o">=</span><span class="m">3</span>
dbscan-min-score<span class="o">=</span><span class="m">1</span>.2998
<span class="nv">eps</span><span class="o">=</span><span class="m">0</span>.1508
detected-min-w<span class="o">=</span><span class="m">20</span>
detected-min-h<span class="o">=</span><span class="m">20</span>
</pre></div>
</div>
</details><details>
<summary><a>config_tracker_NvDCF_accuracy.yml</a></summary><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%YAML:1.0

BaseConfig:
<span class="w">  </span>minDetectorConfidence:<span class="w"> </span><span class="m">0</span>.0382
TargetManagement:
<span class="w">  </span>enableBboxUnClipping:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>preserveStreamUpdateOrder:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>maxTargetsPerStream:<span class="w"> </span><span class="m">150</span>
<span class="w">  </span>minIouDiff4NewTarget:<span class="w"> </span><span class="m">0</span>.1356
<span class="w">  </span>minTrackerConfidence:<span class="w"> </span><span class="m">0</span>.2136
<span class="w">  </span>probationAge:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>maxShadowTrackingAge:<span class="w"> </span><span class="m">49</span>
<span class="w">  </span>earlyTerminationAge:<span class="w"> </span><span class="m">1</span>
TrajectoryManagement:
<span class="w">  </span>useUniqueID:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>enableReAssoc:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>minMatchingScore4Overall:<span class="w"> </span><span class="m">0</span>.0324
<span class="w">  </span>minTrackletMatchingScore:<span class="w"> </span><span class="m">0</span>.2979
<span class="w">  </span>minMatchingScore4ReidSimilarity:<span class="w"> </span><span class="m">0</span>.4329
<span class="w">  </span>matchingScoreWeight4TrackletSimilarity:<span class="w"> </span><span class="m">0</span>.5117
<span class="w">  </span>matchingScoreWeight4ReidSimilarity:<span class="w"> </span><span class="m">0</span>.8356
<span class="w">  </span>minTrajectoryLength4Projection:<span class="w"> </span><span class="m">14</span>
<span class="w">  </span>prepLength4TrajectoryProjection:<span class="w"> </span><span class="m">50</span>
<span class="w">  </span>trajectoryProjectionLength:<span class="w"> </span><span class="m">116</span>
<span class="w">  </span>maxAngle4TrackletMatching:<span class="w"> </span><span class="m">180</span>
<span class="w">  </span>minSpeedSimilarity4TrackletMatching:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>minBboxSizeSimilarity4TrackletMatching:<span class="w"> </span><span class="m">0</span>.2154
<span class="w">  </span>maxTrackletMatchingTimeSearchRange:<span class="w"> </span><span class="m">16</span>
<span class="w">  </span>trajectoryProjectionProcessNoiseScale:<span class="w"> </span><span class="m">0</span>.0100
<span class="w">  </span>trajectoryProjectionMeasurementNoiseScale:<span class="w"> </span><span class="m">100</span>
<span class="w">  </span>trackletSpacialSearchRegionScale:<span class="w"> </span><span class="m">0</span>.0742
<span class="w">  </span>reidExtractionInterval:<span class="w"> </span><span class="m">2</span>
<span class="w">  </span>enableVanishingTrackletReconstruction:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>minInclusionRatio4DuplicateTrackletRemoval:<span class="w"> </span><span class="m">0</span>.5705
<span class="w">  </span>minIou4DuplicateTrackletRemoval:<span class="w"> </span><span class="m">0</span>.5260
<span class="w">  </span>minMatchRatio4ValidTrackletDetermination:<span class="w"> </span><span class="m">0</span>.4385
<span class="w">  </span>minVisibility4VanishingTrackletReconstruction:<span class="w"> </span><span class="m">0</span>.3485
<span class="w">  </span>visibilityThreshold4VanishingTrackletDetection:<span class="w"> </span><span class="m">0</span>.5817
<span class="w"> </span>DataAssociator:
<span class="w">  </span>dataAssociatorType:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>associationMatcherType:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>checkClassMatch:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>minMatchingScore4Overall:<span class="w"> </span><span class="m">0</span>.1103
<span class="w">  </span>minMatchingScore4SizeSimilarity:<span class="w"> </span><span class="m">0</span>.0392
<span class="w">  </span>minMatchingScore4Iou:<span class="w"> </span><span class="m">0</span>.0980
<span class="w">  </span>minMatchingScore4VisualSimilarity:<span class="w"> </span><span class="m">0</span>.3234
<span class="w">  </span>matchingScoreWeight4VisualSimilarity:<span class="w"> </span><span class="m">0</span>.4223
<span class="w">  </span>matchingScoreWeight4SizeSimilarity:<span class="w"> </span><span class="m">0</span>.8416
<span class="w">  </span>matchingScoreWeight4Iou:<span class="w"> </span><span class="m">0</span>.6517
<span class="w">  </span>tentativeDetectorConfidence:<span class="w"> </span><span class="m">0</span>.0198
<span class="w">  </span>minMatchingScore4TentativeIou:<span class="w"> </span><span class="m">0</span>.1844
StateEstimator:
<span class="w">  </span>stateEstimatorType:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>processNoiseVar4Loc:<span class="w"> </span><span class="m">374</span>.6508
<span class="w">  </span>processNoiseVar4Size:<span class="w"> </span><span class="m">3364</span>.1350
<span class="w">  </span>processNoiseVar4Vel:<span class="w"> </span><span class="m">3</span>.6082
<span class="w">  </span>measurementNoiseVar4Detector:<span class="w"> </span><span class="m">164</span>.4517
<span class="w">  </span>measurementNoiseVar4Tracker:<span class="w"> </span><span class="m">3439</span>.5683
VisualTracker:
<span class="w">  </span>visualTrackerType:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>useColorNames:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>useHog:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>featureImgSizeLevel:<span class="w"> </span><span class="m">4</span>
<span class="w">  </span>featureFocusOffsetFactor_y:<span class="w"> </span><span class="m">0</span>.0652
<span class="w">  </span>filterLr:<span class="w"> </span><span class="m">0</span>.0993
<span class="w">  </span>filterChannelWeightsLr:<span class="w"> </span><span class="m">0</span>.0549
<span class="w">  </span>gaussianSigma:<span class="w"> </span><span class="m">0</span>.9047
ReID:<span class="w"> </span><span class="c1"># need customization</span>
<span class="w">  </span>reidType:<span class="w"> </span><span class="m">2</span>
<span class="w">  </span>batchSize:<span class="w"> </span><span class="m">100</span>
<span class="w">  </span>workspaceSize:<span class="w"> </span><span class="m">1000</span>
<span class="w">  </span>reidFeatureSize:<span class="w"> </span><span class="m">128</span>
<span class="w">  </span>reidHistorySize:<span class="w"> </span><span class="m">148</span>
<span class="w">  </span>inferDims:<span class="w"> </span><span class="o">[</span><span class="m">128</span>,<span class="w"> </span><span class="m">64</span>,<span class="w"> </span><span class="m">3</span><span class="o">]</span>
<span class="w">  </span>networkMode:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>inputOrder:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span>colorFormat:<span class="w"> </span><span class="m">0</span>
<span class="w">  </span>offsets:<span class="w"> </span><span class="o">[</span><span class="m">0</span>.0,<span class="w"> </span><span class="m">0</span>.0,<span class="w"> </span><span class="m">0</span>.0<span class="o">]</span>
<span class="w">  </span>netScaleFactor:<span class="w"> </span><span class="m">1</span>.0000
<span class="w">  </span>keepAspc:<span class="w"> </span><span class="m">1</span>
<span class="w">  </span><span class="c1"># onnxFile: customize_onnx_path</span>
</pre></div>
</div>
</details><p>Below is a side-by-side comparison of TrafficCamNet with different trackers on a challenging scene with lots of occlusions. From the top-left corner in the clockwise direction are detection only, NvSORT, NvDeepSORT, and NvDCF results. NvDCF has the highest tracking accuracy and robustness to occlusion.</p>
<blockquote>
<div><div style="width: 100%;display: inline-block;position: relative; padding:2px;border:2px solid white;">
   <div id="dummy" style="margin-top: 56%;"></div>
   <div align="center">
     <div id="kaltura_player_8" style="position:absolute;top:0;left:0;left: 0;right: 0;bottom:0;"></div>
     <script type="text/javascript" src="https://cdnapisec.kaltura.com/p/2935771/embedPlaykitJs/uiconf_id/50468382"></script>
     <script type="text/javascript">
         try {
         var kalturaPlayer = KalturaPlayer.setup({
         targetId: "kaltura_player_8",
         provider:
         { partnerId: 2935771, uiConfId: 50468382 }
         });
         kalturaPlayer.loadMedia(
         {entryId: '1_px9j7b7p'}
         );
         } catch (e)
         { console.error(e.message) }
     </script>
     </div></div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
</section>
<section id="tracker-accuracy-tuning">
<h2>Tracker Accuracy Tuning<a class="headerlink" href="DS_plugin_gst-nvtracker.html#tracker-accuracy-tuning" title="Link to this heading">#</a></h2>
<p>Tracker sample pipelines demonstrated in above section include lots of detector and tracker parameters optimized for people tracking. When users deploy such pipelines for other scenarios, such as traffic, animals, etc., a pain point is how to find the optimal parameters with the highest accuracy KPI for each use case. Manual parameter tuning requires in-depth knowledge on the tracker algorithm and how each parameter would affect the functionality. Given the large number of parameters, the complexity of such process would increase exponentially.</p>
<p>Starting from DeepStream 7.0, a new tool <strong>PipeTuner</strong> is released to allow automatic accuracy tuning. It efficiently explores the (potentially very high-dimensional) parameter space and automatically finds the optimal parameters for the pipelines, which yields the highest KPI on the dataset. Base on their use case, users can tune the tracker on public multi-object tracking datasets, such as <a class="reference external" href="https://motchallenge.net/">MOT Challenge</a>, <a class="reference external" href="https://www.cvlibs.net/datasets/kitti/">KITTI</a>, or create their own dataset with sample videos and ground truth (bounding box and object ID). Users can select common tracking accuracy metrics including <a class="reference external" href="https://link.springer.com/article/10.1007/s11263-020-01375-2">HOTA</a>, <a class="reference external" href="https://link.springer.com/article/10.1155/2008/246309">MOTA</a> and <a class="reference external" href="https://arxiv.org/abs/1609.01775">IDF1</a> as the KPI. Visit <a class="reference internal" href="DS_Accuracy.html"><span class="doc">Accuracy Tuning Tools</span></a> on how to setup PipeTuner and start DeepStream tracker accuracy tuning.</p>
</section>
<section id="how-to-implement-a-custom-low-level-tracker-library">
<h2>How to Implement a Custom Low-Level Tracker Library<a class="headerlink" href="DS_plugin_gst-nvtracker.html#how-to-implement-a-custom-low-level-tracker-library" title="Link to this heading">#</a></h2>
<p>To write a custom low-level tracker library, users are expected to implement the API defined in <code class="docutils literal notranslate"><span class="pre">sources/includes/nvdstracker.h</span></code>, which is covered in an earlier section on <cite>NvDsTracker API</cite> , and parts of the API refer to <code class="docutils literal notranslate"><span class="pre">sources/includes/nvbufsurface.h</span></code>. Thus, the users would need to include <code class="docutils literal notranslate"><span class="pre">nvdstracker.h</span></code> to implement the API:</p>
<blockquote>
<div><div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;nvdstracker.h&quot;</span>
</pre></div>
</div>
</div></blockquote>
<p>Below is a sample implementation of each API. Assuming that the low-level tracker library defines and implements a custom class (e.g., <code class="docutils literal notranslate"><span class="pre">NvMOTContext</span></code> class in the sample code below) to perform actual operations corresponding to each API call. Below is a sample code for initialization and de-initialization APIs:</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The sample code below have some skeletons only. Users are expected to add proper error handling and additional codes as needed</p>
</div>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_Init</span><span class="p">(</span><span class="n">NvMOTConfig</span><span class="w"> </span><span class="o">*</span><span class="n">pConfigIn</span><span class="p">,</span>
<span class="w">                         </span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="o">*</span><span class="n">pContextHandle</span><span class="p">,</span>
<span class="w">                         </span><span class="n">NvMOTConfigResponse</span><span class="w"> </span><span class="o">*</span><span class="n">pConfigResponse</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="k">if</span><span class="p">(</span><span class="n">pContextHandle</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span>
<span class="w">     </span><span class="p">{</span>
<span class="w">          </span><span class="n">NvMOT_DeInit</span><span class="p">(</span><span class="o">*</span><span class="n">pContextHandle</span><span class="p">);</span>
<span class="w">     </span><span class="p">}</span>

<span class="w">     </span><span class="c1">/// User-defined class for the context</span>
<span class="w">     </span><span class="n">NvMOTContext</span><span class="w"> </span><span class="o">*</span><span class="n">pContext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>

<span class="w">     </span><span class="c1">/// Instantiate the user-defined context</span>
<span class="w">     </span><span class="n">pContext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">NvMOTContext</span><span class="p">(</span><span class="o">*</span><span class="n">pConfigIn</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">pConfigResponse</span><span class="p">);</span>

<span class="w">     </span><span class="c1">/// Pass the pointer as the context handle</span>
<span class="w">     </span><span class="o">*</span><span class="n">pContextHandle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pContext</span><span class="p">;</span>

<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * return NvMOTStatus_Error if something is wrong</span>
<span class="cm">      * return NvMOTStatus_OK if everything went well</span>
<span class="cm">      */</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * This is a sample code for the constructor of `NvMOTContext`</span>
<span class="cm"> * to show what may need to happen when NvMOTContext is instantiated in the above code for `NvMOT_Init` API</span>
<span class="cm"> */</span>
<span class="n">NvMOTContext</span><span class="o">::</span><span class="n">NvMOTContext</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">NvMOTConfig</span><span class="w"> </span><span class="o">&amp;</span><span class="n">config</span><span class="p">,</span><span class="w"> </span><span class="n">NvMOTConfigResponse</span><span class="o">&amp;</span><span class="w"> </span><span class="n">configResponse</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="c1">// Set CUDA device as needed</span>
<span class="w">     </span><span class="n">cudaSetDevice</span><span class="p">(</span><span class="n">m_Config</span><span class="p">.</span><span class="n">miscConfig</span><span class="p">.</span><span class="n">gpuId</span><span class="p">)</span>

<span class="w">     </span><span class="c1">// Instantiate an appropriate localizer/tracker implementation</span>
<span class="w">     </span><span class="c1">// Load and parse the config file for the low-level tracker using the path to a config file</span>
<span class="w">     </span><span class="n">m_pLocalizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">LocalizerFactory</span><span class="o">::</span><span class="n">getInstance</span><span class="p">().</span><span class="n">makeLocalizer</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">customConfigFilePath</span><span class="p">);</span>

<span class="w">     </span><span class="c1">// Set max # of streams to be supported</span>
<span class="w">     </span><span class="c1">// ex) uint32_t maxStreams = config.maxStreams;</span>

<span class="w">     </span><span class="c1">// Use the video frame info</span>
<span class="w">     </span><span class="k">for</span><span class="p">(</span><span class="n">uint</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">m_Config</span><span class="p">.</span><span class="n">numTransforms</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="w">     </span><span class="p">{</span>
<span class="w">          </span><span class="c1">// Use the expected color format from the input source images</span>
<span class="w">          </span><span class="n">NvBufSurfaceColorFormat</span><span class="w"> </span><span class="n">configColorFormat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">NvBufSurfaceColorFormat</span><span class="p">)</span><span class="n">m_Config</span><span class="p">.</span><span class="n">perTransformBatchConfig</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">colorFormat</span><span class="p">;</span>

<span class="w">          </span><span class="c1">// Use the frame width, height, and pitch as needed</span>
<span class="w">          </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">frameHeight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">m_Config</span><span class="p">.</span><span class="n">perTransformBatchConfig</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">maxHeight</span><span class="p">;</span>
<span class="w">          </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">frameWidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">m_Config</span><span class="p">.</span><span class="n">perTransformBatchConfig</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">maxWidth</span><span class="p">;</span>
<span class="w">          </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">framePitch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">m_Config</span><span class="p">.</span><span class="n">perTransformBatchConfig</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">maxPitch</span><span class="p">;</span>

<span class="w">          </span><span class="cm">/* Add here to pass the frame info to the low-level tracker */</span>
<span class="w">     </span><span class="p">}</span>

<span class="w">     </span><span class="c1">// Set if everything goes well</span>
<span class="w">     </span><span class="n">configResponse</span><span class="p">.</span><span class="n">summaryStatus</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NvMOTConfigStatus_OK</span><span class="p">;</span>

<span class="p">}</span>
</pre></div>
</div>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">NvMOT_DeInit</span><span class="p">(</span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="c1">/// Destroy the context handle</span>
<span class="w">     </span><span class="k">delete</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div></blockquote>
<p>During the initialization stage (when <code class="docutils literal notranslate"><span class="pre">NvMOT_Init()</span></code> is called), the context for the low-level tracker is expected to be instantiated, and its pointer is passed as the context handle (i.e., <code class="docutils literal notranslate"><span class="pre">pContextHandle</span></code>) as the output as well as the output status in <code class="docutils literal notranslate"><span class="pre">pConfigResponse</span></code>. Users may allocate memories based on the information about the video frames (e.g., width, height, pitch, and colorFormat) and streams (e.g., max # of streams) from the input <code class="docutils literal notranslate"><span class="pre">NvMOTConfig</span> <span class="pre">*pConfigIn</span></code>, where the definition of the struct <code class="docutils literal notranslate"><span class="pre">NvMOTConfig</span></code> can be found in <code class="docutils literal notranslate"><span class="pre">nvdstracker.h</span></code>. The path to the config file for the low-level tracker library in <code class="docutils literal notranslate"><span class="pre">pConfigIn-&gt;customConfigFilePath</span></code> can be also used to parse the config file to initialize the low-level tracker library.</p>
<p>After initialization is finished, the tracker plugin queries parameters needed from low-level tracker library. The query function needs to be implemented like below:</p>
<blockquote>
<div><div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_Query</span><span class="p">(</span><span class="kt">uint16_t</span><span class="w"> </span><span class="n">customConfigFilePathSize</span><span class="p">,</span>
<span class="w">                         </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">pCustomConfigFilePath</span><span class="p">,</span>
<span class="w">                         </span><span class="n">NvMOTQuery</span><span class="w"> </span><span class="o">*</span><span class="n">pQuery</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * Users can parse the low-level config file in pCustomConfigFilePath to check</span>
<span class="cm">      * the low-level tracker&#39;s requirements</span>
<span class="cm">      */</span>

<span class="w">     </span><span class="cm">/** An optional function queryParams(NvMOTQuery&amp;) can be implemented in context handle to fill query params. */</span>
<span class="w">     </span><span class="cm">/*</span>
<span class="cm">     if (pQuery-&gt;contextHandle)</span>
<span class="cm">     {</span>
<span class="cm">          pQuery-&gt;contextHandle-&gt;queryParams(*pQuery);</span>
<span class="cm">     }</span>
<span class="cm">     */</span>

<span class="w">     </span><span class="cm">/**  Required configs for all custom trackers. */</span>
<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">computeConfig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NVMOTCOMP_GPU</span><span class="p">;</span><span class="w">       </span><span class="c1">// among {NVMOTCOMP_GPU, NVMOTCOMP_CPU}</span>
<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">numTransforms</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w">                   </span><span class="c1">// 0 for IOU and NvSORT tracker, 1 for NvDCF or NvDeepSORT tracker as they require the video frames</span>
<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">supportPastFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">             </span><span class="c1">// Set true only if the low-level tracker supports the past-frame data</span>
<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">batchMode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NvMOTBatchMode_Batch</span><span class="p">;</span><span class="w">    </span><span class="c1">// batchMode must be set as NvMOTBatchMode_Batch</span>
<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">colorFormats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NVBUF_COLOR_FORMAT_NV12</span><span class="p">;</span><span class="w"> </span><span class="c1">// among {NVBUF_COLOR_FORMAT_NV12, NVBUF_COLOR_FORMAT_RGBA}</span>

<span class="w">     </span><span class="cp">#ifdef __aarch64__</span>
<span class="w">          </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">memType</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NVBUF_MEM_DEFAULT</span><span class="p">;</span>
<span class="w">     </span><span class="cp">#else</span>
<span class="w">          </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">memType</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NVBUF_MEM_CUDA_DEVICE</span><span class="p">;</span>
<span class="w">     </span><span class="cp">#endif</span>
<span class="w">     </span><span class="c1">// among {NVBUF_MEM_DEFAULT, NVBUF_MEM_CUDA_DEVICE, NVBUF_MEM_CUDA_UNIFIED, NVBUF_MEM_CUDA_PINNED, ... }</span>

<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">maxTargetsPerStream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">150</span><span class="p">;</span><span class="w">           </span><span class="c1">// Max number of targets stored for each stream</span>

<span class="w">     </span><span class="cm">/** Optional configs to set for additional features. */</span>
<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">maxShadowTrackingAge</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">30</span><span class="p">;</span><span class="w">           </span><span class="c1">// Maximum length of shadow tracking, required if supportPastFrame is true</span>
<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">outputReidTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">             </span><span class="c1">// Set true only if the low-level tracker supports outputting reid feature</span>
<span class="w">     </span><span class="n">pQuery</span><span class="o">-&gt;</span><span class="n">reidFeatureSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">256</span><span class="p">;</span><span class="w">               </span><span class="c1">// Size of Re-ID feature, required if outputReidTensor is true</span>

<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * return NvMOTStatus_Error if something is wrong</span>
<span class="cm">      * return NvMOTStatus_OK if everything went well</span>
<span class="cm">      */</span>
<span class="p">}</span>
</pre></div>
</div>
</div></blockquote>
<p>Once the low-level tracker library creates the tracker context and performs query during the initialization stage, it needs to implement a function to process each frame batch, which is <code class="docutils literal notranslate"><span class="pre">NvMOT_Process()</span></code>. Make sure to set the stream ID properly in the output so that <code class="docutils literal notranslate"><span class="pre">pParams-&gt;frameList[i].streamID</span></code> matches with <code class="docutils literal notranslate"><span class="pre">pTrackedObjectsBatch-&gt;list[j].streamID</span></code> if they are for the same stream, regardless of <code class="docutils literal notranslate"><span class="pre">i</span></code> and <code class="docutils literal notranslate"><span class="pre">j</span></code>. The method <code class="docutils literal notranslate"><span class="pre">NvMOTContext::processFrame()</span></code> in the sample code below is expected to perform the required multi-object tracking operations with the input data of the video frames and the detector object information, while reporting the tracking outputs in <code class="docutils literal notranslate"><span class="pre">NvMOTTrackedObjBatch</span> <span class="pre">*pTrackedObjectsBatch</span></code>.</p>
<p>Users can refer to <a class="reference external" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_sample_custom_gstream.html#accessing-nvbufsurface-memory-in-opencv">Accessing NvBufSurface memory in OpenCV</a> to know more about how to access the pixel data in the video frames.</p>
<blockquote>
<div><div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_Process</span><span class="p">(</span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">,</span>
<span class="w">                         </span><span class="n">NvMOTProcessParams</span><span class="w"> </span><span class="o">*</span><span class="n">pParams</span><span class="p">,</span>
<span class="w">                         </span><span class="n">NvMOTTrackedObjBatch</span><span class="w"> </span><span class="o">*</span><span class="n">pTrackedObjectsBatch</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="c1">/// Process the given video frame using the user-defined method in the context, and generate outputs</span>
<span class="w">     </span><span class="n">contextHandle</span><span class="o">-&gt;</span><span class="n">processFrame</span><span class="p">(</span><span class="n">pParams</span><span class="p">,</span><span class="w"> </span><span class="n">pTrackedObjectsBatch</span><span class="p">);</span>

<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * return NvMOTStatus_Error if something is wrong</span>
<span class="cm">      * return NvMOTStatus_OK if everything went well</span>
<span class="cm">      */</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * This is a sample code for the method of `NvMOTContext::processFrame()`</span>
<span class="cm"> * to show what may need to happen when it is called in the above code for `NvMOT_Process` API</span>
<span class="cm"> */</span>
<span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOTContext::processFrame</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">NvMOTProcessParams</span><span class="w"> </span><span class="o">*</span><span class="n">params</span><span class="p">,</span>
<span class="w">                                        </span><span class="n">NvMOTTrackedObjBatch</span><span class="w"> </span><span class="o">*</span><span class="n">pTrackedObjectsBatch</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="c1">// Make sure the input frame is valid according to the MOT Config used to create this context</span>
<span class="w">     </span><span class="k">for</span><span class="p">(</span><span class="n">uint</span><span class="w"> </span><span class="n">streamInd</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">streamInd</span><span class="o">&lt;</span><span class="n">params</span><span class="o">-&gt;</span><span class="n">numFrames</span><span class="p">;</span><span class="w"> </span><span class="n">streamInd</span><span class="o">++</span><span class="p">)</span>
<span class="w">     </span><span class="p">{</span>
<span class="w">          </span><span class="n">NvMOTFrame</span><span class="w"> </span><span class="o">*</span><span class="n">motFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">params</span><span class="o">-&gt;</span><span class="n">frameList</span><span class="p">[</span><span class="n">streamInd</span><span class="p">];</span>
<span class="w">          </span><span class="k">for</span><span class="p">(</span><span class="n">uint</span><span class="w"> </span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">&lt;</span><span class="n">motFrame</span><span class="o">-&gt;</span><span class="n">numBuffers</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="w">          </span><span class="p">{</span>
<span class="w">               </span><span class="cm">/* Add something here to check the validity of the input using the following info*/</span>
<span class="w">               </span><span class="n">motFrame</span><span class="o">-&gt;</span><span class="n">bufferList</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">width</span>
<span class="w">               </span><span class="n">motFrame</span><span class="o">-&gt;</span><span class="n">bufferList</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">height</span>
<span class="w">               </span><span class="n">motFrame</span><span class="o">-&gt;</span><span class="n">bufferList</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">pitch</span>
<span class="w">               </span><span class="n">motFrame</span><span class="o">-&gt;</span><span class="n">bufferList</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">colorFormat</span>
<span class="w">          </span><span class="p">}</span>
<span class="w">     </span><span class="p">}</span>

<span class="w">     </span><span class="c1">// Construct the mot input frames</span>
<span class="w">     </span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">NvMOTStreamId</span><span class="p">,</span><span class="w"> </span><span class="n">NvMOTFrame</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">nvFramesInBatch</span><span class="p">;</span>
<span class="w">     </span><span class="k">for</span><span class="p">(</span><span class="n">NvMOTStreamId</span><span class="w"> </span><span class="n">streamInd</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">streamInd</span><span class="o">&lt;</span><span class="n">params</span><span class="o">-&gt;</span><span class="n">numFrames</span><span class="p">;</span><span class="w"> </span><span class="n">streamInd</span><span class="o">++</span><span class="p">)</span>
<span class="w">     </span><span class="p">{</span>
<span class="w">          </span><span class="n">NvMOTFrame</span><span class="w"> </span><span class="o">*</span><span class="n">motFrame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">params</span><span class="o">-&gt;</span><span class="n">frameList</span><span class="p">[</span><span class="n">streamInd</span><span class="p">];</span>
<span class="w">          </span><span class="n">nvFramesInBatch</span><span class="p">[</span><span class="n">motFrame</span><span class="o">-&gt;</span><span class="n">streamID</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">motFrame</span><span class="p">;</span>
<span class="w">     </span><span class="p">}</span>

<span class="w">     </span><span class="k">if</span><span class="p">(</span><span class="n">nvFramesInBatch</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span>
<span class="w">     </span><span class="p">{</span>
<span class="w">          </span><span class="c1">// Perform update and construct the output data inside</span>
<span class="w">          </span><span class="n">m_pLocalizer</span><span class="o">-&gt;</span><span class="n">update</span><span class="p">(</span><span class="n">nvFramesInBatch</span><span class="p">,</span><span class="w"> </span><span class="n">pTrackedObjectsBatch</span><span class="p">);</span>

<span class="w">          </span><span class="cm">/**</span>
<span class="cm">           * The call m_pLocalizer-&gt;update() is expected to properly populate the ouput (i.e., `pTrackedObjectsBatch`).</span>
<span class="cm">           *</span>
<span class="cm">           * One thing to not forget is to fill `pTrackedObjectsBatch-&gt;list[i].list[j].associatedObjectIn`, where</span>
<span class="cm">           * `i` and `j` are indices for stream and targets in the list, respectively.</span>
<span class="cm">           * If the `j`th target was associated/matched with a detector object,</span>
<span class="cm">           * then `associatedObjectIn` is supposed to have the pointer to the associated detector object.</span>
<span class="cm">           * Otherwise, `associatedObjectIn` shall be set NULL.</span>
<span class="cm">           */</span>
<span class="w">     </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div></blockquote>
<p>The low-level tracker can send miscellaneous data to the tracker plugin by <code class="docutils literal notranslate"><span class="pre">NvMOT_RetrieveMiscData()</span></code> API. Here past frame data is used as an example. Other types of customized miscellaneous data can be added in <code class="docutils literal notranslate"><span class="pre">NvMOTTrackerMiscData</span></code> struct and also be outputted in <code class="docutils literal notranslate"><span class="pre">retrieveMiscData</span></code>.</p>
<blockquote>
<div><div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_RetrieveMiscData</span><span class="p">(</span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">,</span>
<span class="w">                              </span><span class="n">NvMOTProcessParams</span><span class="w"> </span><span class="o">*</span><span class="n">pParams</span><span class="p">,</span>
<span class="w">                              </span><span class="n">NvMOTTrackerMiscData</span><span class="w"> </span><span class="o">*</span><span class="n">pTrackerMiscData</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="c1">/// Retrieve the past-frame data if there are</span>
<span class="w">     </span><span class="n">contextHandle</span><span class="o">-&gt;</span><span class="n">retrieveMiscData</span><span class="p">(</span><span class="n">pParams</span><span class="p">,</span><span class="w"> </span><span class="n">pTrackerMiscData</span><span class="p">);</span>

<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * return NvMOTStatus_Error if something is wrong</span>
<span class="cm">      * return NvMOTStatus_OK if everything went well</span>
<span class="cm">      */</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * This is a sample code for the method of `NvMOTContext::processFramePast()`</span>
<span class="cm"> * to show what may need to happen when it is called in the above code for `NvMOT_ProcessPast` API</span>
<span class="cm"> */</span>
<span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOTContext::retrieveMiscData</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">NvMOTProcessParams</span><span class="w"> </span><span class="o">*</span><span class="n">params</span><span class="p">,</span>
<span class="w">                                             </span><span class="n">NvMOTTrackerMiscData</span><span class="w"> </span><span class="o">*</span><span class="n">pTrackerMiscData</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="n">std</span><span class="o">::</span><span class="n">set</span><span class="o">&lt;</span><span class="n">NvMOTStreamId</span><span class="o">&gt;</span><span class="w"> </span><span class="n">videoStreamIdList</span><span class="p">;</span>

<span class="w">     </span><span class="c1">///\ Indiate what streams we want to fetch past-frame data</span>
<span class="w">     </span><span class="k">for</span><span class="p">(</span><span class="n">NvMOTStreamId</span><span class="w"> </span><span class="n">streamInd</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">streamInd</span><span class="o">&lt;</span><span class="n">params</span><span class="o">-&gt;</span><span class="n">numFrames</span><span class="p">;</span><span class="w"> </span><span class="n">streamInd</span><span class="o">++</span><span class="p">)</span>
<span class="w">     </span><span class="p">{</span>
<span class="w">          </span><span class="n">videoStreamIdList</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">params</span><span class="o">-&gt;</span><span class="n">frameList</span><span class="p">[</span><span class="n">streamInd</span><span class="p">].</span><span class="n">streamID</span><span class="p">);</span>
<span class="w">     </span><span class="p">}</span>

<span class="w">     </span><span class="c1">///\ Retrieve past frame data</span>
<span class="w">     </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">pTrackerMiscData</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">pTrackerMiscData</span><span class="o">-&gt;</span><span class="n">pPastFrameObjBatch</span><span class="p">)</span>
<span class="w">           </span><span class="p">{</span>
<span class="w">                </span><span class="n">m_pLocalizer</span><span class="o">-&gt;</span><span class="n">outputPastFrameObjs</span><span class="p">(</span><span class="n">videoStreamIdList</span><span class="p">,</span><span class="w"> </span><span class="n">pTrackerMiscData</span><span class="o">-&gt;</span><span class="n">pPastFrameObjBatch</span><span class="p">);</span>
<span class="w">           </span><span class="p">}</span>

<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * Add other types of miscellaneous data here</span>
<span class="cm">      */</span>
<span class="p">}</span>
</pre></div>
</div>
</div></blockquote>
<p>For the cases where the video stream sources are dynamically removed and added, the API call <code class="docutils literal notranslate"><span class="pre">NvMOT_RemoveStreams()</span></code> can be implemented to clean-up the resources no longer needed.</p>
<blockquote>
<div><div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOT_RemoveStreams</span><span class="p">(</span><span class="n">NvMOTContextHandle</span><span class="w"> </span><span class="n">contextHandle</span><span class="p">,</span>
<span class="w">                                   </span><span class="n">NvMOTStreamId</span><span class="w"> </span><span class="n">streamIdMask</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="c1">/// Remove the specified video stream from the low-level tracker context</span>
<span class="w">     </span><span class="n">contextHandle</span><span class="o">-&gt;</span><span class="n">removeStream</span><span class="p">(</span><span class="n">streamIdMask</span><span class="p">);</span>

<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * return NvMOTStatus_Error if something is wrong</span>
<span class="cm">      * return NvMOTStatus_OK if everything went well</span>
<span class="cm">      */</span>
<span class="p">}</span>

<span class="cm">/**</span>
<span class="cm"> * This is a sample code for the method of `NvMOTContext::removeStream()`</span>
<span class="cm"> * to show what may need to happen when it is called in the above code for `NvMOT_RemoveStreams` API</span>
<span class="cm"> */</span>
<span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">NvMOTContext::removeStream</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">NvMOTStreamId</span><span class="w"> </span><span class="n">streamIdMask</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">     </span><span class="n">m_pLocalizer</span><span class="o">-&gt;</span><span class="n">deleteRemovedStreamTrackers</span><span class="p">(</span><span class="n">streamIdMask</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</div></blockquote>
<p>In sum, to work with the <cite>NvDsTracker</cite> APIs, users may want to define <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">NvMOTContext</span></code> like below to implement the methods in the code above. The actual implementation of each method may differ depending on the tracking algorithm the user choose to implement.</p>
<blockquote>
<div><div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cm">/**</span>
<span class="cm">* @brief Context for input video streams</span>
<span class="cm">*</span>
<span class="cm">* The stream context holds all necessary state to perform multi-object tracking</span>
<span class="cm">* within the stream.</span>
<span class="cm">*</span>
<span class="cm">*/</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NvMOTContext</span>
<span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">     </span><span class="n">NvMOTContext</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">NvMOTConfig</span><span class="w"> </span><span class="o">&amp;</span><span class="n">configIn</span><span class="p">,</span><span class="w"> </span><span class="n">NvMOTConfigResponse</span><span class="o">&amp;</span><span class="w"> </span><span class="n">configResponse</span><span class="p">);</span>
<span class="w">     </span><span class="o">~</span><span class="n">NvMOTContext</span><span class="p">();</span>

<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * @brief Process a batch of frames</span>
<span class="cm">      *</span>
<span class="cm">      * Internal implementation of NvMOT_Process()</span>
<span class="cm">      *</span>
<span class="cm">      * @param [in] pParam Pointer to parameters for the frame to be processed</span>
<span class="cm">      * @param [out] pTrackedObjectsBatch Pointer to object tracks output</span>
<span class="cm">      */</span>
<span class="w">     </span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">processFrame</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">NvMOTProcessParams</span><span class="w"> </span><span class="o">*</span><span class="n">params</span><span class="p">,</span>
<span class="w">                                   </span><span class="n">NvMOTTrackedObjBatch</span><span class="w"> </span><span class="o">*</span><span class="n">pTrackedObjectsBatch</span><span class="p">);</span>
<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * @brief Output the miscellaneous data if there are</span>
<span class="cm">      *</span>
<span class="cm">      *  Internal implementation of retrieveMiscData()</span>
<span class="cm">      *</span>
<span class="cm">      * @param [in] pParam Pointer to parameters for the frame to be processed</span>
<span class="cm">      * @param [out] pTrackerMiscData Pointer to miscellaneous data output</span>
<span class="cm">      */</span>
<span class="w">     </span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">retrieveMiscData</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">NvMOTProcessParams</span><span class="w"> </span><span class="o">*</span><span class="n">params</span><span class="p">,</span>
<span class="w">                                   </span><span class="n">NvMOTTrackerMiscData</span><span class="w"> </span><span class="o">*</span><span class="n">pTrackerMiscData</span><span class="p">);</span>
<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * @brief Terminate trackers and release resources for a stream when the stream is removed</span>
<span class="cm">      *</span>
<span class="cm">      *  Internal implementation of NvMOT_RemoveStreams()</span>
<span class="cm">      *</span>
<span class="cm">      * @param [in] streamIdMask removed stream ID</span>
<span class="cm">      */</span>
<span class="w">     </span><span class="n">NvMOTStatus</span><span class="w"> </span><span class="nf">removeStream</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">NvMOTStreamId</span><span class="w"> </span><span class="n">streamIdMask</span><span class="p">);</span>

<span class="k">protected</span><span class="o">:</span>

<span class="w">     </span><span class="cm">/**</span>
<span class="cm">      * Users can include an actual tracker implementation here as a member</span>
<span class="cm">      * `IMultiObjectTracker` can be assumed to an user-defined interface class</span>
<span class="cm">      */</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">IMultiObjectTracker</span><span class="o">&gt;</span><span class="w"> </span><span class="n">m_pLocalizer</span><span class="p">;</span>

<span class="p">};</span>
</pre></div>
</div>
</div></blockquote>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="DS_plugin_gst-nvinferserver.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Gst-nvinferserver</p>
      </div>
    </a>
    <a class="right-next"
       href="DS_plugin_gst-nvstreammux.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gst-nvstreammux</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#sub-batching-alpha">Sub-batching (Alpha)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#inputs-and-outputs">Inputs and Outputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#gst-properties">Gst Properties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#nvdstracker-api-for-low-level-tracker-library">NvDsTracker API for Low-Level Tracker Library</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#nvmultiobjecttracker-a-reference-low-level-tracker-library"><em>NvMultiObjectTracker</em> : A Reference Low-Level Tracker Library</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#unified-tracker-architecture-for-composable-multi-object-tracker">Unified Tracker Architecture for Composable Multi-Object Tracker</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#workflow-and-core-modules-in-the-nvmultiobjecttracker-library">Workflow and Core Modules in The <em>NvMultiObjectTracker</em> Library</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#data-association">Data Association</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#target-management-and-error-handling">Target Management and Error Handling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#state-estimation">State Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#object-re-identification">Object Re-Identification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#target-re-association">Target Re-Association</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#bounding-box-unclipping">Bounding-box Unclipping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#single-view-3d-tracking-alpha">Single-View 3D Tracking (Alpha)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#the-3x4-camera-projection-matrix">The 3x4 Camera Projection Matrix</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#configuration-parameters">Configuration Parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#low-level-tracker-comparisons-and-tradeoffs">Low-Level Tracker Comparisons and Tradeoffs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#iou-tracker">IOU Tracker</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#nvsort-tracker">NvSORT Tracker</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#nvdeepsort-tracker">NvDeepSORT Tracker</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#id5">Data Association</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#id6">Configuration Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#implementation-details-and-reference">Implementation Details and Reference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#nvdcf-tracker">NvDCF Tracker</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#visual-tracking">Visual Tracking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#id11">Data Association</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#id12">Configuration Parameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#miscellaneous-data-output">Miscellaneous Data Output</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#terminated-track-list">Terminated Track List</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#shadow-tracking-target-data">Shadow Tracking Target Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#past-frame-target-data">Past-frame Target Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#setup-and-usage-of-re-id-model">Setup and Usage of Re-ID Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#setup-sample-re-id-models">Setup Sample Re-ID Models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#nvidia-tao-reidentificationnet">NVIDIA TAO ReIdentificationNet</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#onnx-model">ONNX Model</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#customize-re-id-model">Customize Re-ID Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#re-id-feature-output">Re-ID Feature Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#setup-and-usage-of-sub-batching-alpha">Setup and Usage of Sub-batching (Alpha)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#use-case-1">Use-case 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#use-case-2">Use-case 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#setup-and-visualization-of-tracker-sample-pipelines">Setup and Visualization of Tracker Sample Pipelines</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#people-tracking">People Tracking</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#setup">Setup</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#peoplenet-nvsort">PeopleNet + NvSORT</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#peoplenet-nvdeepsort">PeopleNet + NvDeepSORT</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#peoplenet-nvdcf">PeopleNet + NvDCF</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#vehicle-tracking">Vehicle Tracking</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#id13">Setup</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#trafficcamnet-nvsort">TrafficCamNet + NvSORT</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#trafficcamnet-nvdeepsort">TrafficCamNet + NvDeepSORT</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#trafficcamnet-nvdcf">TrafficCamNet + NvDCF</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#tracker-accuracy-tuning">Tracker Accuracy Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="DS_plugin_gst-nvtracker.html#how-to-implement-a-custom-low-level-tracker-library">How to Implement a Custom Low-Level Tracker Library</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js%3Fdigest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="../_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="../_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">



  <p class="copyright">
    
      Copyright © 2024-2025, NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item"><p class="last-updated">
  Last updated on Sep 15, 2025.
  <br/>
</p></div>
      
        <div class="footer-item">
<div class="extra_footer">
  
      <script type="text/javascript">if (typeof _satellite !== "undefined") {_satellite.pageBottom();}</script>
    
  
</div></div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>