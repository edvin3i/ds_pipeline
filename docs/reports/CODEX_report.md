# CODEX CPU Load Assessment

The following high-load paths were identified while reviewing the DeepStream pipeline codebase (excluding `Performance_report.MD`). Each section links the observed issue to NVIDIA documentation (`ds_doc/`) and the Jetson Orin NX platform description to emphasize why the current approach is risky for CPU budget on this SoC.

## 1. Deep copies of every NVMM frame in `BufferManager`
*Evidence:* `BufferManager.on_new_sample()` clones each `GstBuffer` with `copy_deep()` before enqueuing it (`new_week/pipeline/buffer_manager.py`, lines 123-158). The playback side later copies audio chunks as well (`lines 321-359`).

*Impact:* On Jetson Orin NX the CPU and GPU share the same 16 GB LPDDR5 pool and any extra DRAM copy competes with GPU workloads for bandwidth.【F:new_week/pipeline/buffer_manager.py†L123-L205】【F:nvidia_jetson_orin_nx_16GB_super_arch.txt†L4-L78】 DeepStream docs warn against heavy buffer work inside pipeline callbacks because they sit on the critical path; deep-copying every frame inside the appsink callback contradicts that guidance.【F:ds_doc/7.1/text/DS_service_maker_python_advanced_features.html†L641-L658】 Aside from the DRAM bandwidth hit, `copy_deep()` forces the Cortex-A78AE cluster (~2 GHz) to touch every pixel of the 5700×1900 panorama defined across the pipeline modules, so a single copy burns ~43 MB of bandwidth and can monopolize multiple CPU cores.【F:new_week/pipeline/pipeline_builder.py†L21-L72】


## 2. O(n) scans per frame when feeding the playback `appsrc`
*Evidence:* `_on_appsrc_need_data()` linearly walks the entire `frame_buffer` deque on every `need-data` signal to find the next timestamp (`lines 223-244`). `_push_audio_for_timestamp()` performs another linear scan through all audio chunks to locate the closest timestamp (`lines 330-359`).
*Impact:* With a 7-second buffer at 30 FPS (~210 frames) every outgoing frame performs ≥210 dictionary lookups and Python comparisons. Those scans run under the GStreamer streaming thread, so they add latency jitter and fight for CPU cycles needed by TensorRT post-processing. This violates the documentation guidance that time-consuming work should not run inside probes or streaming callbacks.【F:new_week/pipeline/buffer_manager.py†L223-L359】【F:ds_doc/7.1/text/DS_service_maker_python_advanced_features.html†L641-L658】

## 3. Heavy Python/Numpy post-processing inside the analysis pad probe
*Evidence:* `AnalysisProbeHandler.handle_analysis_probe()` executes large Python loops for every inference batch (lines 180-521): per-frame dictionaries, repeated list comprehensions, multiple `apply_nms()` calls (`new_week/utils/nms.py`, lines 1-49) and adaptive filters in pure Python. Each tensor layer is also copied into NumPy via `get_tensor_as_numpy()` which clones the full buffer (`new_week/processing/tensor_processor.py`, lines 16-88 & 91-119).
*Impact:* All of this executes inside the nvinfer pad probe—the most timing-sensitive point of the pipeline—contrary to DeepStream’s recommendation to avoid time-consuming work in probes.【F:new_week/processing/analysis_probe.py†L150-L537】【F:new_week/utils/nms.py†L1-L49】【F:new_week/processing/tensor_processor.py†L16-L119】【F:ds_doc/7.1/text/DS_service_maker_python_advanced_features.html†L641-L658】 Each copy drags the tensor out of NVMM into CPU DRAM, which on Orin NX consumes the unified memory bandwidth the GPU needs for inference.【F:nvidia_jetson_orin_nx_16GB_super_arch.txt†L31-L78】 The repeated Python filtering (confidence, mask, ban lists, shape, adaptive distance, priority ranking) scales with the number of tiles and detections, so worst-case CPU load spikes during crowded frames.

## 4. Expensive history searches and sorts for every detection request
*Evidence:* `HistoryManager.get_detection_for_timestamp()` rebuilds and sorts the entire combined history for each lookup (`new_week/core/history_manager.py`, lines 120-207). `_process_future_history()` merges dictionaries, runs trajectory filtering, and interpolates whenever new detections arrive, calling into `TrajectoryFilter.detect_and_remove_false_trajectories()` whose nested loops compute square roots for every pair of points (`new_week/core/trajectory_filter.py`, lines 63-210).
*Impact:* These operations are O(n log n) or worse per detection, executed on the same CPU cores that must also keep the DeepStream pipeline fed. The heavy math inside `TrajectoryFilter` (multiple passes, 5-point sliding windows, distance matrices) magnifies load whenever the ball path is noisy. Because CPU and GPU compete for the same DRAM, this extra work can directly throttle inference throughput on Orin NX.【F:new_week/core/history_manager.py†L120-L344】【F:new_week/core/trajectory_filter.py†L60-L212】【F:nvidia_jetson_orin_nx_16GB_super_arch.txt†L31-L78】

## 5. Rendering probe recomputes historical aggregates every frame
*Evidence:* `DisplayProbeHandler._compute_smoothed_center_of_mass()` re-lists all timestamps, sorts them, computes medians, filters outliers, and applies weighted averages for up to 7 seconds of history on every rendered frame (`new_week/rendering/display_probe.py`, lines 75-198). The same probe also enumerates metadata and draws boxes (`lines 200-398`).
*Impact:* nvdsosd sink pad probes are in the display branch’s hot path; recomputing medians and weights for potentially hundreds of timestamps every 33 ms can saturate one CPU core. DeepStream documentation again cautions against time-consuming work in probes, and doing so here risks starving the playback branch and increasing end-to-end latency.【F:new_week/rendering/display_probe.py†L75-L398】【F:ds_doc/7.1/text/DS_service_maker_python_advanced_features.html†L641-L658】

## 6. Disk I/O and NumPy reshapes inside the panorama tile saver
*Evidence:* `PanoramaTileSaver.on_new_sample()` pulls each appsink buffer into a NumPy array, reshapes it, and writes full-resolution JPEGs plus six 1024×1024 tiles every N frames (`sliser/panorama_tiles_saver.py`, lines 65-170). Each write happens under the GLib main loop.

*Impact:* NumPy reshaping and JPEG compression of 5700×1900 panoramas (43 MB RGBA frames) are CPU-heavy. On Jetson’s 8-core CPU those synchronous operations can block the GLib loop, which also handles bus messages and timers for the stitching pipeline. When combined with the shared-memory architecture, such CPU spikes compete with GPU inference for DRAM bandwidth.【F:sliser/panorama_tiles_saver.py†L26-L177】【F:new_week/pipeline/pipeline_builder.py†L21-L72】【F:nvidia_jetson_orin_nx_16GB_super_arch.txt†L4-L78】

## Summary
Across the pipeline, several components perform large CPU-bound tasks directly inside GStreamer probes or streaming callbacks. NVIDIA’s DeepStream documentation explicitly warns that probes should avoid time-consuming work, and Jetson Orin NX’s unified memory amplifies the cost of every CPU copy. Refactoring these hotspots (e.g., zero-copy metadata access, batched history maintenance, moving heavy math off the hot path) is critical to keep inference and rendering within the real-time CPU budget.
