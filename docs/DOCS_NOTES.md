# Documentation Notes - GPU Pipeline Optimization Research

**Date:** 2025-11-17
**Topic:** High GPU/RAM loading analysis and optimization strategies
**Current State:** GPU 94-99%, RAM 13.2G/15.3G, micro freezes in playback

---

## Root Cause Analysis

### Critical Bottleneck Identified: GPU‚ÜîCPU Memory Copies

**Location:** `new_week/pipeline/pipeline_builder.py:228-230`

Current display branch pipeline:
```python
nvvideoconvert name=display_convert compute-hw=1 !
capsfilter caps="video/x-raw,format=RGB" !
appsink name=display_sink emit-signals=true sync=false drop=false max-buffers=60
```

**Data Flow Problem:**
```
Analysis Branch (Real-time, stays in GPU):
  nvdsstitch (NVMM) ‚Üí tee ‚Üí nvtilebatcher (NVMM) ‚Üí nvinfer (NVMM) ‚Üí analysis_probe

Display Branch (7s lag, GPU‚ÜíCPU‚ÜíGPU roundtrip):
  nvdsstitch (NVMM) ‚Üí tee ‚Üí nvvideoconvert (NVMM‚ÜíCPU RGB) ‚Üí appsink (CPU)
     ‚Üì
  BufferManager (CPU deque with deep copies, ~6.8GB)
     ‚Üì
  appsrc (CPU) ‚Üí nvvideoconvert (CPU‚ÜíNVMM) ‚Üí nvdsvirtualcam (NVMM)
```

### Performance Impact

**Memory Bandwidth Saturation:**
- Panorama: 5700√ó1900 RGBA = 43.3 MB (NVMM)
- RGB conversion: 5700√ó1900 RGB = 32.5 MB (CPU)
- Buffer size: 210 frames @ 30fps √ó 32.5 MB = **6.8 GB in CPU RAM**
- Each frame copied **twice**: GPU‚ÜíCPU (nvvideoconvert) + CPU deep copy (BufferManager:147)

**Measured Impact:**
- RAM: 13.2G / 15.3G (86% usage, mostly buffer)
- GPU: 94-99% (saturated from copies + inference + encoding)
- Memory bandwidth: 102 GB/s shared between CPU/GPU (fully utilized)
- Result: Micro freezes from bandwidth contention

---

## Research Findings

### 1. DLA (Deep Learning Accelerator) Support

**Question:** Can YOLOv11 use DLA to reduce GPU load?

**Answer:** ‚ùå **NOT PRACTICAL** for this use case

**Reasons:**
- YOLOv11 contains layers NOT supported by DLA (SiLU/Swish activations, certain upsample ops)
- Hybrid GPU+DLA execution adds context switching overhead
- GitHub issue #18012: Users struggle to get YOLO11 working on DLA
- DLA only supports FP16/INT8 (already using FP16)
- **Main bottleneck is memory bandwidth, not GPU compute** - DLA won't help

**Source:**
- https://github.com/ultralytics/ultralytics/issues/18012
- NVIDIA Developer Blog: YOLOv5 on DLA (partial support only)
- Current config: `config_infer.txt` line 6: `network-mode=2` (FP16)

**Conclusion:** User's intuition is **CORRECT** - DLA not useful here.

---

### 2. appsink with NVMM Memory

**Question:** Can appsink work with NVMM to avoid CPU copies?

**Answer:** ‚ö†Ô∏è **PARTIALLY** - but cannot easily access buffer data in Python

**Key Findings:**
- appsink CAN receive NVMM buffers if caps set to `video/x-raw(memory:NVMM)`
- **BUT** `pyds.get_nvds_buf_surface()` does NOT support NVMM memory mapping in Python
- `NvBufSurfaceMap()` only works for `NVBUF_MEM_CUDA_UNIFIED` (dGPU) and `NVBUF_MEM_SURFACE_ARRAY` (Jetson)
- For Jetson: Must call `pyds.unmap_nvds_buf_surface()` to prevent memory leaks

**Source:**
- NVIDIA Developer Forums: "NvBufSurface doesn't support NVMM memory"
- DeepStream Python API docs: `get_nvds_buf_surface()` limitations

**Implication:** We don't need to ACCESS pixel data for buffering - only need to REFERENCE buffers and control timing!

---

### 3. GStreamer Tee and Zero-Copy

**Question:** Does tee copy buffers when splitting streams?

**Answer:** ‚úÖ **NO** - Tee uses reference counting (zero-copy)

**Key Findings:**
- GStreamer Tee increments reference count and pushes **pointers** to same buffer
- No data copying occurs at tee split
- All branches share same NVMM buffers
- nvvideoconvert supports NVMM‚ÜíNVMM (zero-copy on Jetson)

**Source:**
- Medium article: "Optimising Performance in NVIDIA DeepStream Pipelines"
- DeepStream 6.3 Pipeline Best Practices (RidgeRun)

**Implication:** Can implement buffering by holding buffer references in GPU memory!

---

### 4. Buffer Pool Management

**Best Practices from DeepStream Documentation:**

1. **Buffer Pool Exhaustion:**
   - Default pool size: 32 buffers
   - Can increase with `num-extra-surfaces` property
   - Set on source group or `nvv4l2decoder`

2. **Batch Size Optimization:**
   - nvstreammux batch-size = number of sources OR primary nvinfer batch-size
   - Current: batch-size=2 (2 cameras) for mux, batch-size=6 for nvinfer (6 tiles) ‚úÖ

3. **Memory Type:**
   - `nvbuf-memory-type=0`: Default (CPU)
   - `nvbuf-memory-type=3`: NVMM (GPU) - **should use this**

**Source:**
- DeepStream FAQ: Batch size configuration
- DeepStream Troubleshooting docs

---

## Optimization Strategies

### Strategy 1: Eliminate appsink - Use Probe-Based NVMM Buffering ‚≠ê **RECOMMENDED**

**Approach:**
1. Remove appsink + nvvideoconvert from display branch
2. Add probe callback on main_tee src pad (before playback branch)
3. Store buffer **references** (not copies) with timestamps using `gst_buffer_ref()`
4. Implement delayed playback by holding refs for 7 seconds
5. Release old buffer refs with `gst_buffer_unref()` to free pool

**Modified Pipeline:**
```python
# Display branch - NO appsink, NO CPU conversion!
main_tee. !
queue name=display_queue max-size-buffers=240 !  # 7s @ 30fps + margin
appsrc name=playback_src format=time is-live=true !
video/x-raw(memory:NVMM),format=RGBA !  # ‚Üê NVMM throughout!
nvdsvirtualcam name=vcam ...
```

**BufferManager Refactor:**
```python
class NVMMBufferManager:
    """Manages NVMM buffer references (not copies) for 7s delay."""

    def __init__(self, buffer_duration=7.0, framerate=30):
        self.buffer_refs = deque(maxlen=int(buffer_duration * framerate))
        self.buffer_lock = threading.RLock()

    def on_buffer_probe(self, pad, info):
        """Probe callback - store buffer reference."""
        buf = info.get_buffer()
        if buf:
            timestamp = float(buf.pts) / Gst.SECOND

            with self.buffer_lock:
                # Increment ref count - keeps buffer alive
                buf_ref = buf.ref()  # or use Gst.Buffer.copy() for NVMM-to-NVMM
                self.buffer_refs.append({
                    'timestamp': timestamp,
                    'buffer': buf_ref  # Reference, not deep copy!
                })

        return Gst.PadProbeReturn.OK

    def get_buffer_for_playback(self, target_timestamp):
        """Retrieve buffer ref for playback."""
        with self.buffer_lock:
            for item in self.buffer_refs:
                if item['timestamp'] >= target_timestamp:
                    return item['buffer']
        return None

    def cleanup_old_buffers(self, current_timestamp):
        """Unref old buffers to free pool."""
        threshold = current_timestamp - 1.0
        with self.buffer_lock:
            while self.buffer_refs and self.buffer_refs[0]['timestamp'] < threshold:
                old_item = self.buffer_refs.popleft()
                old_item['buffer'].unref()  # Release reference
```

**Pros:**
- ‚úÖ Eliminates GPU‚ÜíCPU‚ÜíGPU copies (saves ~65 GB/s bandwidth)
- ‚úÖ Reduces RAM usage: ~6.8GB ‚Üí ~1GB (NVMM stays in GPU)
- ‚úÖ No deep copies - only reference counting overhead
- ‚úÖ Maintains exact same 7s delay functionality
- ‚úÖ Compatible with existing pipeline architecture

**Cons:**
- ‚ö†Ô∏è Requires careful buffer lifecycle management (ref/unref balance)
- ‚ö†Ô∏è May need to increase `num-extra-surfaces` to prevent pool exhaustion
- ‚ö†Ô∏è Moderate implementation complexity (refactor BufferManager)

**Risk:** MEDIUM - Requires careful testing but well-documented approach

---

### Strategy 2: H.264 Encoding for Buffer (Compression) üîÑ **ALTERNATIVE**

**Approach:**
1. Encode panorama to H.264 on GPU (nvv4l2h264enc)
2. Buffer compressed stream (~1MB vs 32MB per frame)
3. Decode for playback (nvv4l2decoder)

**Modified Pipeline:**
```python
# Display branch with encoding
main_tee. !
queue !
nvv4l2h264enc bitrate=50000000 !  # 50 Mbps
h264parse !
appsink name=h264_sink  # Much smaller buffers!

# Playback
appsrc name=h264_src !
h264parse !
nvv4l2decoder !
video/x-raw(memory:NVMM) !
nvdsvirtualcam ...
```

**Pros:**
- ‚úÖ Massive memory reduction: 6.8GB ‚Üí ~0.21GB (32√ó compression)
- ‚úÖ Less bandwidth for buffering
- ‚úÖ Simpler implementation than NVMM ref management

**Cons:**
- ‚ùå Adds encode/decode latency (~10-20ms each = +20-40ms total)
- ‚ùå Compression artifacts (lossy)
- ‚ùå Additional GPU load for codec (10-15%)
- ‚ùå May not solve bandwidth issue if GPU already saturated

**Risk:** MEDIUM-HIGH - Adds latency, may impact quality

---

### Strategy 3: Reduce Inference Load (Tile Optimization) üìä **SUPPLEMENTARY**

**Current:** 6√ó 1024√ó1024 tiles processed every frame

**Options:**
1. **Spatial skip:** Process only 3-4 center tiles (ball rarely at edges)
2. **Temporal skip:** Already doing frame skip (every 5th frame) - could increase to 10
3. **Dynamic batching:** Process fewer tiles when ball detected (adaptive)

**Pros:**
- ‚úÖ Reduces GPU inference load (20ms ‚Üí 10-15ms)
- ‚úÖ Frees GPU bandwidth for other tasks
- ‚úÖ Simple to implement (modify tile configs)

**Cons:**
- ‚ö†Ô∏è Potential detection quality loss at field edges
- ‚ö†Ô∏è May miss fast ball movement if skip too aggressive

**Risk:** LOW - Easy to revert if quality degrades

---

### Strategy 4: Optimize Probe Callbacks (Minor Gains) üîß **SUPPLEMENTARY**

**Current Issues:**
- Pure Python NMS (O(n¬≤)) in `analysis_probe.py:24-86`
- Field mask lookup per detection
- Nested loops for filtering

**Optimizations:**
1. Use `cv2.dnn.NMSBoxes()` instead of pure Python NMS
2. Vectorize field mask checks with NumPy
3. Pre-filter detections before expensive operations

**Pros:**
- ‚úÖ Reduces CPU load in callbacks (minor)
- ‚úÖ Low risk - easy to benchmark

**Cons:**
- ‚è∫Ô∏è Minimal impact (not the main bottleneck)

**Risk:** VERY LOW - Incremental improvement

---

## Recommended Implementation Plan

### Phase 1: Primary Optimization - NVMM Buffer References ‚≠ê

**Goal:** Eliminate GPU‚ÜîCPU copies in display branch

1. **Modify pipeline_builder.py (Display Branch):**
   - Remove: `nvvideoconvert compute-hw=1 ! capsfilter caps="video/x-raw,format=RGB"`
   - Change appsink to use NVMM caps OR remove appsink entirely
   - Add probe callback on main_tee src pad

2. **Refactor BufferManager:**
   - Rename to `NVMMBufferManager`
   - Replace deep copies with `gst_buffer_ref()`
   - Implement proper `gst_buffer_unref()` cleanup
   - Test with small buffer (2s) first, then scale to 7s

3. **Modify playback_builder.py:**
   - Change appsrc caps from `video/x-raw,format=RGB` to `video/x-raw(memory:NVMM),format=RGBA`
   - Remove first nvvideoconvert (CPU‚ÜíNVMM)
   - Keep rest of pipeline as-is

4. **Tune Buffer Pool:**
   - Add `num-extra-surfaces=64` to nvdsstitch
   - Monitor buffer pool exhaustion warnings

**Expected Impact:**
- RAM: 13.2G ‚Üí ~6-7G (save ~6.8GB)
- GPU: 94-99% ‚Üí 70-80% (save ~15-20% from reduced copies)
- Bandwidth: Massive reduction in GPU‚ÜîCPU traffic
- Micro freezes: **ELIMINATED** (root cause removed)

**Timeline:** 1-2 days implementation + testing

---

### Phase 2: Supplementary Optimizations üìä

**If Phase 1 doesn't fully resolve (or for extra headroom):**

1. **Increase analysis frame skip:** 5 ‚Üí 10 (halve inference load)
2. **Process 4 center tiles only** (skip edge tiles 0 and 5)
3. **Optimize NMS:** Use cv2.dnn.NMSBoxes()

**Expected Additional Impact:**
- GPU: 70-80% ‚Üí 60-70%
- Minimal quality impact (ball rarely at edges)

**Timeline:** 1 day

---

### Phase 3: Monitoring & Validation ‚úÖ

1. **Performance Metrics:**
   - Monitor: `tegrastats` for GPU/RAM/bandwidth
   - Target: GPU <75%, RAM <12G, no playback freezes
   - Validate: 7s delay accuracy, no buffer drops

2. **Quality Validation:**
   - Ball detection accuracy (compare before/after)
   - Virtual camera smoothness
   - No visual artifacts

---

## Decision: DLA Not Recommended ‚ùå

**Reasons:**
1. YOLOv11 layers incompatible with DLA
2. Main bottleneck is **memory bandwidth**, not GPU compute
3. Hybrid execution overhead negates benefits
4. Already using FP16 (DLA requirement met, no further gain)

**Source:** User's intuition confirmed by research

---

## References

1. **NVIDIA Developer Forums:**
   - "Get GPU memory buffer from GStreamer without copying to CPU"
   - "Passing NVMM frames to GStreamer appsink"

2. **Medium Articles:**
   - "Optimising Performance in NVIDIA DeepStream Pipelines"
   - "Memory Management in NVIDIA DeepStream Pipelines"

3. **DeepStream Documentation:**
   - `/ds_doc/7.1/text/DS_FAQ.html` - Batch size, buffer pools
   - Python API: NvBufSurface limitations

4. **Codebase Analysis:**
   - `new_week/pipeline/pipeline_builder.py:228-230` - Bottleneck location
   - `new_week/pipeline/buffer_manager.py:147` - Deep copy issue
   - `new_week/pipeline/playback_builder.py:89` - CPU memory appsrc

---

## Next Steps

1. ‚úÖ Research complete - root cause identified
2. ‚è≠Ô∏è Present optimization plan to user for approval
3. ‚è≠Ô∏è Implement Phase 1 (NVMM buffer references)
4. ‚è≠Ô∏è Test and validate
5. ‚è≠Ô∏è Implement Phase 2 if needed

---

**Last Updated:** 2025-11-17
**Status:** Analysis complete, awaiting implementation approval

---

# CUDA 12.6 Best Practices for Jetson Orin

**Date:** 2025-11-19
**Source:** NVIDIA CUDA Best Practices Guide, CUDA C++ Programming Guide
**Platform:** Jetson Orin NX 16GB with CUDA 12.6, Compute Capability SM87
**Local Docs:** docs/cuda-12.6.0-docs/ (HTML and PDF)

## Memory Access Patterns

### 1. Coalesced Global Memory Access ‚≠ê

**Key Principle:** "Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions."

**For Compute Capability 6.0+ (Jetson Orin is SM87):**
- Concurrent accesses of warp threads coalesce into transactions equal to number of 32-byte segments needed
- Sequential threads accessing adjacent words = optimal performance
- Misaligned accesses increase transaction counts (e.g., 5 segments instead of 4)

**Best Practices:**
```cuda
// ‚úÖ CORRECT: Coalesced access - all threads read consecutive addresses
__global__ void process_rgba_good(uint8_t* input, uint8_t* output, int width, int height)
{
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x >= width || y >= height) return;

    int idx = (y * width + x) * 4;  // RGBA stride

    // Coalesced 4-byte read (vectorized)
    uchar4 pixel = *((uchar4*)&input[idx]);

    // Process...
    pixel.x = min(pixel.x + 10, 255);

    // Coalesced 4-byte write
    *((uchar4*)&output[idx]) = pixel;
}

// ‚ùå INCORRECT: Strided access pattern (kills bandwidth)
__global__ void process_rgba_bad(uint8_t* input, uint8_t* output, int width, int height)
{
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x >= width || y >= height) return;

    // BAD: Channels stored separately (SoA layout for RGB = scattered reads)
    int idx = y * width + x;
    output[idx] = input[idx];                              // R
    output[idx + width*height] = input[idx + width*height];  // G - different memory region!
    output[idx + 2*width*height] = input[idx + 2*width*height];  // B - scattered access

    // Threads access non-consecutive locations = 25% bandwidth efficiency
}
```

**Memory Alignment:**
- `cudaMalloc()` guarantees 256-byte alignment
- Use thread block sizes as multiples of warp size (32)
- Pitch/stride should be aligned to 64 bytes on Jetson

### 2. Shared Memory Bank Conflicts

**Architecture:** Shared memory divided into 32 banks (4-byte width per bank per cycle)

**Conflict Rules:**
- Multiple accesses to same bank (except same address) serialize
- Performance reduction proportional to max conflicts

**Best Practices:**
```cuda
// ‚úÖ CORRECT: Bank-conflict-free shared memory access
__global__ void transpose_no_conflict(float* output, float* input, int width)
{
    __shared__ float tile[32][33];  // ‚Üê Note: 33 columns to avoid bank conflicts!

    int x = blockIdx.x * 32 + threadIdx.x;
    int y = blockIdx.y * 32 + threadIdx.y;

    // Load tile (coalesced)
    tile[threadIdx.y][threadIdx.x] = input[y * width + x];
    __syncthreads();

    // Transpose (still no conflicts due to padding)
    output[x * width + y] = tile[threadIdx.x][threadIdx.y];
}

// ‚ùå INCORRECT: Bank conflicts on transpose
__shared__ float tile[32][32];  // Will have conflicts!
```

### 3. Unified Memory on Jetson (cudaMallocManaged)

**Jetson-Specific:** CPU and GPU share same physical LPDDR5 (16GB unified)

**Key Points:**
- `cudaMallocManaged()` leverages unified memory architecture
- CPU allocations impact GPU available memory (shared pool)
- Avoids explicit `cudaMemcpy()` but NOT zero-copy (page migration overhead)
- Best for irregular access patterns or prototyping

**Best Practices:**
```cuda
// ‚úÖ CORRECT: Unified memory for CPU+GPU access
float* data;
cudaMallocManaged(&data, N * sizeof(float));

// Accessible from both CPU and GPU
init_on_cpu(data, N);
kernel<<<blocks, threads>>>(data, N);
cudaDeviceSynchronize();
verify_on_cpu(data, N);

cudaFree(data);
```

**When NOT to use:**
- Streaming video data (use NVMM buffers instead)
- High-frequency CPU‚ÜîGPU transfers (use pinned memory + explicit copies)

---

## Kernel Optimization

### 4. Occupancy and Register Pressure

**Occupancy:** Ratio of active warps to maximum possible warps

**Goal:** Maximize occupancy to hide memory latency

**Tools:**
- `__launch_bounds__(maxThreadsPerBlock, minBlocksPerSM)` attribute
- CUDA Occupancy Calculator
- `nvprof --metrics achieved_occupancy`

**Best Practices:**
```cuda
// ‚úÖ CORRECT: Explicit occupancy control
__global__ void
__launch_bounds__(256, 4)  // 256 threads/block, min 4 blocks per SM
my_kernel(float* data, int N)
{
    // Compiler optimizes register usage to achieve 4 blocks/SM
}
```

**Register Usage:**
- Jetson Orin: 65,536 registers per SM
- Fewer registers per thread ‚Üí more concurrent blocks ‚Üí higher occupancy
- Monitor with `nvcc --ptxas-options=-v`

### 5. Warp Divergence Avoidance ‚ö†Ô∏è

**Critical Rule:** Avoid control flow divergence within warps (32 threads)

**Why:** Divergent branches serialize execution (both paths executed, results masked)

**Best Practices:**
```cuda
// ‚úÖ CORRECT: Warp-aligned branching
__global__ void process_even_odd_good(int* data, int N)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid >= N) return;

    // Good: All threads in warp take same branch (if tid is warp-aligned)
    int warp_id = tid / 32;
    if (warp_id % 2 == 0) {
        data[tid] = data[tid] * 2;  // All threads in even warps
    } else {
        data[tid] = data[tid] + 1;  // All threads in odd warps
    }
}

// ‚ùå INCORRECT: Thread-level divergence
__global__ void process_even_odd_bad(int* data, int N)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid >= N) return;

    // BAD: Each warp has 50% divergence (16 threads one way, 16 the other)
    if (tid % 2 == 0) {
        data[tid] = data[tid] * 2;  // Even threads
    } else {
        data[tid] = data[tid] + 1;  // Odd threads - DIVERGES!
    }
}
```

**Mitigation:**
- Design algorithms to align branches with warp boundaries
- Use predication instead of branches where possible
- Reorder data to group similar operations

### 6. Dynamic Allocation Forbidden in Kernels ‚ùå

**Rule:** NO `malloc()` or `new` inside kernels

**Reason:**
- Heap allocation is slow and non-deterministic
- Limited heap size on device
- Can cause deadlocks or failures

**Best Practices:**
```cuda
// ‚úÖ CORRECT: Pre-allocated shared memory
__global__ void process_with_shared(float* input, float* output, int N)
{
    __shared__ float scratch[1024];  // Compile-time allocation

    int tid = threadIdx.x;
    scratch[tid] = input[blockIdx.x * blockDim.x + tid];
    __syncthreads();

    // Use scratch...
    output[blockIdx.x * blockDim.x + tid] = scratch[tid] * 2.0f;
}

// ‚úÖ CORRECT: Dynamic shared memory (specified at kernel launch)
__global__ void process_with_dynamic(float* input, float* output, int N)
{
    extern __shared__ float scratch[];  // Size specified at launch

    int tid = threadIdx.x;
    scratch[tid] = input[blockIdx.x * blockDim.x + tid];
    __syncthreads();

    output[blockIdx.x * blockDim.x + tid] = scratch[tid];
}

// Launch with: kernel<<<blocks, threads, sharedMemSize>>>(...)

// ‚ùå INCORRECT: Dynamic allocation in kernel
__global__ void process_bad(float* input, float* output, int N)
{
    float* temp = (float*)malloc(1024 * sizeof(float));  // ‚ùå FORBIDDEN!
    // ...
    free(temp);  // Even if freed, still bad!
}
```

---

## Streams and Asynchronous Operations

### 7. Overlap Computation with Transfers

**Key Capability:** Jetson Orin can overlap kernel execution with memory transfers

**Requirements:**
- Use non-default streams
- Pinned host memory (`cudaHostAlloc()` or `cudaMallocHost()`)
- `cudaMemcpyAsync()` instead of `cudaMemcpy()`

**Best Practices:**
```cuda
// ‚úÖ CORRECT: Overlapped execution
cudaStream_t stream1, stream2;
cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);

float *d_data1, *d_data2, *h_data1, *h_data2;
cudaMalloc(&d_data1, size);
cudaMalloc(&d_data2, size);
cudaHostAlloc(&h_data1, size, cudaHostAllocDefault);  // Pinned!
cudaHostAlloc(&h_data2, size, cudaHostAllocDefault);

// Stream 1: Transfer + Compute
cudaMemcpyAsync(d_data1, h_data1, size, cudaMemcpyHostToDevice, stream1);
kernel<<<blocks, threads, 0, stream1>>>(d_data1);
cudaMemcpyAsync(h_data1, d_data1, size, cudaMemcpyDeviceToHost, stream1);

// Stream 2: Concurrent transfer + compute
cudaMemcpyAsync(d_data2, h_data2, size, cudaMemcpyHostToDevice, stream2);
kernel<<<blocks, threads, 0, stream2>>>(d_data2);
cudaMemcpyAsync(h_data2, d_data2, size, cudaMemcpyDeviceToHost, stream2);

cudaStreamSynchronize(stream1);
cudaStreamSynchronize(stream2);

// ‚ùå INCORRECT: Default stream serializes everything
cudaMemcpy(d_data1, h_data1, size, cudaMemcpyHostToDevice);  // Blocks
kernel<<<blocks, threads>>>(d_data1);  // Default stream = serialized
cudaMemcpy(h_data1, d_data1, size, cudaMemcpyDeviceToHost);  // Blocks
```

### 8. Event-Based Timing and Synchronization

**Best Practices:**
```cuda
// ‚úÖ CORRECT: GPU timing with events
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord(start, stream);
kernel<<<blocks, threads, 0, stream>>>(data);
cudaEventRecord(stop, stream);

cudaEventSynchronize(stop);

float milliseconds = 0;
cudaEventElapsedTime(&milliseconds, start, stop);
printf("Kernel time: %.3f ms\n", milliseconds);

cudaEventDestroy(start);
cudaEventDestroy(stop);
```

---

## Error Handling ‚ö†Ô∏è

### 9. Mandatory Error Checking

**Rule:** Check ALL CUDA API return values in production code

**Best Practices:**
```cuda
// ‚úÖ CORRECT: Comprehensive error checking
#define CUDA_CHECK(call)                                                      \
do {                                                                          \
    cudaError_t err = call;                                                   \
    if (err != cudaSuccess) {                                                 \
        fprintf(stderr, "CUDA error in %s:%d: %s (%s)\n",                    \
                __FILE__, __LINE__,                                           \
                cudaGetErrorString(err), cudaGetErrorName(err));              \
        exit(EXIT_FAILURE);                                                   \
    }                                                                         \
} while(0)

// Usage:
CUDA_CHECK(cudaMalloc(&d_data, size));
kernel<<<blocks, threads>>>(d_data);
CUDA_CHECK(cudaGetLastError());  // Check kernel launch errors
CUDA_CHECK(cudaDeviceSynchronize());  // Check kernel execution errors

// ‚ùå INCORRECT: Ignoring errors
cudaMalloc(&d_data, size);  // Return value ignored!
kernel<<<blocks, threads>>>(d_data);
// No error checking = silent failures!
```

**Kernel Launch Errors:**
- `cudaGetLastError()` returns most recent error
- `cudaPeekAtLastError()` returns error without resetting
- Always check after kernel launch AND after synchronization

---

## Jetson-Specific Optimizations

### 10. Unified Memory Architecture

**Jetson Orin Specifics:**
- 16 GB LPDDR5 shared between CPU/GPU (102 GB/s bandwidth)
- No PCIe bus overhead (integrated SoC)
- Memory bandwidth is THE critical resource

**Optimization Strategies:**
1. **Minimize CPU‚ÜîGPU data movement** (even more critical than dGPU)
2. **Use NVMM buffers** for video processing (GStreamer, DeepStream)
3. **Prefer GPU-resident data** throughout pipeline
4. **Avoid unnecessary cudaMemcpy()** even within GPU memory

### 11. Power and Thermal Constraints

**Jetson Orin NX Modes:**
- 10W mode: Lower clocks, better efficiency
- 25W mode: Balanced (recommended for DeepStream)
- 40W mode: Maximum performance (requires active cooling)

**Monitor:**
```bash
sudo tegrastats  # Real-time GPU/CPU/RAM/power monitoring
sudo jetson_clocks  # Lock clocks to max (for benchmarking)
```

### 12. FP16 Tensor Cores

**Jetson Orin:** 32 Tensor Cores (Ampere architecture)

**Optimization:**
- Use FP16 for inference (already doing this in project)
- TensorRT automatically uses Tensor Cores for FP16 ops
- Avoid FP32 for large matrix operations

---

## Performance Optimization Priority (APOD Workflow)

**From NVIDIA Best Practices Guide:**

1. **Assess:** Profile to find bottlenecks
   - Use `nsys` (Nsight Systems) for system-level profiling
   - Use `nvprof` for kernel-level profiling
   - Identify memory vs compute bottlenecks

2. **Parallelize:** Leverage libraries first
   - cuBLAS, cuFFT, Thrust (GPU STL)
   - DeepStream plugins for video
   - Only write custom kernels when necessary

3. **Optimize:** Apply targeted improvements
   - High Priority: Minimize transfers, coalesced access
   - Medium Priority: Shared memory, occupancy
   - Low Priority: Instruction-level micro-optimizations

4. **Deploy:** Iterative improvements
   - Partial optimizations are better than perfect rewrites
   - Measure impact of each change
   - Release incremental improvements

---

## Common Pitfalls on Jetson ‚ö†Ô∏è

1. **Exceeding 16GB unified memory** ‚Üí Out of memory errors
2. **High CPU usage** ‚Üí Reduces GPU memory availability
3. **Memory bandwidth saturation** ‚Üí GPU stalls (102 GB/s shared limit)
4. **Thermal throttling** ‚Üí Performance degradation (monitor temp)
5. **Swapping to disk** ‚Üí Catastrophic performance (avoid at all costs)

---

**References:**
- NVIDIA CUDA C++ Best Practices Guide (Local: docs/cuda-12.6.0-docs/cuda-c-best-practices-guide.html)
- NVIDIA CUDA C++ Programming Guide (Local: docs/cuda-12.6.0-docs/cuda-c-programming-guide.html)
- Jetson Orin Developer Guide (Local: docs/hw_arch/nvidia_jetson_orin_nx_16GB_super_arch.pdf)
- "CUDA for Tegra" Documentation (Local: docs/cuda-12.6.0-docs/cuda-for-tegra.html)
- Online: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html
- Online: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html

**Last Updated:** 2025-11-19
**Note:** Download CUDA 12.6.0 documentation to docs/cuda-12.6.0-docs/ for offline reference

---

# DeepStream 7.1 Best Practices

**Date:** 2025-11-19
**Source:** DeepStream 7.1 Documentation, NVIDIA Developer Forums
**Platform:** DeepStream SDK 7.1 on Jetson Orin NX 16GB

## Metadata Handling

### 1. NvDsBatchMeta Structure ‚≠ê

**Core Concept:** DeepStream attaches metadata to GstBuffer via NvDsBatchMeta

**Hierarchy:**
```
GstBuffer
  ‚îî‚îÄ NvDsBatchMeta (batch-level, created by nvstreammux)
      ‚îú‚îÄ NvDsFrameMeta (per frame in batch)
      ‚îÇ   ‚îú‚îÄ NvDsObjectMeta (per detected object)
      ‚îÇ   ‚îÇ   ‚îú‚îÄ rect_params (bbox)
      ‚îÇ   ‚îÇ   ‚îú‚îÄ class_id, confidence
      ‚îÇ   ‚îÇ   ‚îî‚îÄ NvDsUserMeta (custom object metadata)
      ‚îÇ   ‚îú‚îÄ NvDsDisplayMeta (for nvdsosd rendering)
      ‚îÇ   ‚îî‚îÄ NvDsUserMeta (custom frame metadata)
      ‚îî‚îÄ NvDsUserMeta (custom batch metadata)
```

**Extracting Metadata:**
```python
# ‚úÖ CORRECT: Safe metadata extraction
def probe_callback(pad, info, u_data):
    gst_buffer = info.get_buffer()
    if not gst_buffer:
        return Gst.PadProbeReturn.OK

    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))
    if not batch_meta:
        return Gst.PadProbeReturn.OK

    # Iterate frame metadata list
    try:
        l_frame = batch_meta.frame_meta_list
        while l_frame is not None:
            try:
                frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)
            except StopIteration:
                break

            # Process frame_meta...

            try:
                l_frame = l_frame.next
            except StopIteration:
                break

    except Exception as e:
        logging.error(f"Metadata iteration error: {e}")

    return Gst.PadProbeReturn.OK

# ‚ùå INCORRECT: No StopIteration handling
def probe_bad(pad, info, u_data):
    gst_buffer = info.get_buffer()
    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))

    l_frame = batch_meta.frame_meta_list
    while l_frame:  # ‚ùå Will crash when list ends!
        frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)  # ‚ùå No try/except
        # ...
        l_frame = l_frame.next  # ‚ùå CRASH on last iteration

    return Gst.PadProbeReturn.OK
```

### 2. User Metadata Attachment

**Rule:** Must acquire from pool, set copy/release functions

**Best Practices:**
```python
# ‚úÖ CORRECT: Proper user metadata lifecycle
def attach_user_meta(frame_meta, custom_data):
    """Attach custom data to frame metadata."""

    # Acquire from pool
    user_meta = pyds.nvds_acquire_user_meta_from_pool(frame_meta.base_meta.batch_meta)
    if not user_meta:
        logging.warning("Failed to acquire user meta from pool")
        return

    # Set metadata
    user_meta.user_meta_data = custom_data
    user_meta.base_meta.meta_type = pyds.NvDsMetaType.NVDSINFER_TENSOR_OUTPUT_META

    # CRITICAL: Set copy and release functions
    user_meta.base_meta.copy_func = pyds.my_copy_func
    user_meta.base_meta.release_func = pyds.my_release_func

    # Attach to frame
    pyds.nvds_add_user_meta_to_frame(frame_meta, user_meta)

# ‚ùå INCORRECT: Missing copy/release functions
def attach_user_meta_bad(frame_meta, custom_data):
    user_meta = pyds.nvds_acquire_user_meta_from_pool(frame_meta.base_meta.batch_meta)
    user_meta.user_meta_data = custom_data
    # ‚ùå Missing: copy_func, release_func ‚Üí Memory leaks!
    pyds.nvds_add_user_meta_to_frame(frame_meta, user_meta)
```

### 3. Display Metadata (nvdsosd)

**Jetson Limitation:** Maximum 16 objects rendered by nvdsosd on Jetson

**Best Practices:**
```python
# ‚úÖ CORRECT: Limit objects to 16, prioritize important ones
def add_display_meta(frame_meta, detections):
    """Add display metadata with Jetson constraint."""

    # Sort by priority (ball > players > others)
    detections_sorted = sorted(detections, key=lambda d: (d['class_id'], -d['confidence']))

    display_meta = pyds.nvds_acquire_display_meta_from_pool(frame_meta.base_meta.batch_meta)

    # CRITICAL: Limit to 16 objects
    max_objects = min(len(detections_sorted), 16)

    for i in range(max_objects):
        det = detections_sorted[i]

        rect_params = display_meta.rect_params[display_meta.num_rects]
        rect_params.left = det['x']
        rect_params.top = det['y']
        rect_params.width = det['w']
        rect_params.height = det['h']
        rect_params.border_width = 3 if det['class_id'] == 0 else 2  # Ball thicker
        rect_params.border_color.set(1.0, 0.0, 0.0, 1.0)  # Red

        display_meta.num_rects += 1

    pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta)

# ‚ùå INCORRECT: Exceeds 16 objects limit
def add_display_meta_bad(frame_meta, detections):
    display_meta = pyds.nvds_acquire_display_meta_from_pool(frame_meta.base_meta.batch_meta)

    # ‚ùå Will crash or corrupt if > 16 objects!
    for i, det in enumerate(detections):  # Could be 50+ detections
        rect_params = display_meta.rect_params[i]  # ‚ùå Array overflow!
        # ...
```

---

## Buffer Management

### 4. NVMM Buffer Lifecycle

**Key Principle:** Use NVMM buffers throughout pipeline (zero-copy)

**Best Practices:**
```cpp
// ‚úÖ CORRECT: NVMM buffer mapping in GStreamer plugin
GstFlowReturn
gst_my_plugin_chain (GstPad *pad, GstObject *parent, GstBuffer *buf)
{
    GstMapInfo in_map_info;

    // Map buffer for read
    if (!gst_buffer_map(buf, &in_map_info, GST_MAP_READ)) {
        GST_ERROR("Failed to map input buffer");
        return GST_FLOW_ERROR;
    }

    NvBufSurface *surf = (NvBufSurface *)in_map_info.data;

    // Validate surface
    if (surf->surfaceList[0].dataSize == 0) {
        GST_WARNING("Empty surface");
        gst_buffer_unmap(buf, &in_map_info);
        return GST_FLOW_OK;
    }

    // Map to CUDA/EGL for GPU access
    CUgraphicsResource pResource = NULL;
    CUeglFrame eglFrame;

    cuGraphicsEGLRegisterImage(&pResource,
                               surf->surfaceList[0].mappedAddr.eglImage,
                               CU_GRAPHICS_MAP_RESOURCE_FLAGS_NONE);

    cuGraphicsResourceGetMappedEglFrame(&eglFrame, pResource, 0, 0);

    // CUDA processing using eglFrame...
    my_cuda_kernel<<<grid, block>>>(eglFrame.frame.pPitch[0], ...);

    // Unmap in reverse order
    cuGraphicsUnregisterResource(pResource);
    gst_buffer_unmap(buf, &in_map_info);

    return GST_FLOW_OK;
}

// ‚ùå INCORRECT: Forgot to unmap buffer
GstFlowReturn bad_chain(GstPad *pad, GstObject *parent, GstBuffer *buf)
{
    GstMapInfo in_map_info;
    gst_buffer_map(buf, &in_map_info, GST_MAP_READ);

    NvBufSurface *surf = (NvBufSurface *)in_map_info.data;
    // Process...

    // ‚ùå Missing: gst_buffer_unmap() ‚Üí Memory leak!
    return GST_FLOW_OK;
}
```

### 5. Buffer Pool Configuration

**Properties:**
- `nvbuf-memory-type`: 0=CPU, 3=NVMM (use 3!)
- `num-extra-surfaces`: Increase pool size (default 32)
- `batch-size`: Must match source count or nvinfer batch size

**Best Practices:**
```python
# ‚úÖ CORRECT: Configure buffer pools properly
def create_pipeline():
    # nvstreammux configuration
    streammux = Gst.ElementFactory.make("nvstreammux", "stream-muxer")
    streammux.set_property("width", 1920)
    streammux.set_property("height", 1080)
    streammux.set_property("batch-size", 2)  # 2 cameras
    streammux.set_property("batched-push-timeout", 4000000)
    streammux.set_property("nvbuf-memory-type", 3)  # ‚úÖ NVMM!

    # Custom plugin with buffer pool
    my_plugin = Gst.ElementFactory.make("nvdsstitch", "stitcher")
    my_plugin.set_property("num-extra-surfaces", 64)  # ‚úÖ Increase pool

    # nvinfer configuration
    nvinfer = Gst.ElementFactory.make("nvinfer", "primary-inference")
    nvinfer.set_property("batch-size", 6)  # 6 tiles
    nvinfer.set_property("config-file-path", "config_infer.txt")

    # ...

# ‚ùå INCORRECT: Wrong memory type
streammux.set_property("nvbuf-memory-type", 0)  # ‚ùå CPU memory = copies!
```

---

## Pipeline Configuration

### 6. Probe Return Values

**Critical:** Always return `Gst.PadProbeReturn.OK` to continue pipeline

**Best Practices:**
```python
# ‚úÖ CORRECT: Proper probe return handling
def my_probe(pad, info, u_data):
    """
    Probe callback for metadata processing.

    Returns:
        Gst.PadProbeReturn.OK - Always return OK to allow buffer to continue
                                downstream. Returning other values can:
                                - DROP: Discard buffer (causes frame drops)
                                - REMOVE: Remove probe (one-shot probe)
                                - HANDLED: Don't call default handler
    """
    try:
        # Process metadata...
        pass
    except Exception as e:
        logging.error(f"Probe error: {e}")

    # ‚úÖ CRITICAL: Return OK to continue pipeline
    return Gst.PadProbeReturn.OK

# ‚ùå INCORRECT: Wrong return value
def bad_probe(pad, info, u_data):
    # Process...
    return Gst.PadProbeReturn.DROP  # ‚ùå Drops every buffer!
```

### 7. Config File Best Practices

**nvinfer config_infer.txt:**
```ini
# ‚úÖ CORRECT: Complete configuration
[property]
gpu-id=0
net-scale-factor=0.0039215697906911373
model-color-format=0
model-engine-file=../models/yolo11n_mixed_finetune_v9.engine
labelfile-path=labels.txt
batch-size=6
network-mode=2  # 0=FP32, 1=INT8, 2=FP16
num-detected-classes=5  # ‚úÖ Must match model output!
interval=0
gie-unique-id=1
output-blob-names=output0

# ‚úÖ CRITICAL: Define ALL classes (0 through num-detected-classes-1)
[class-attrs-0]
pre-cluster-threshold=0.25
nms-iou-threshold=0.45
topk=100

[class-attrs-1]
pre-cluster-threshold=0.40
nms-iou-threshold=0.45
topk=100

[class-attrs-2]
pre-cluster-threshold=0.40
nms-iou-threshold=0.45
topk=100

[class-attrs-3]
pre-cluster-threshold=0.40
nms-iou-threshold=0.45
topk=100

[class-attrs-4]  # ‚úÖ MUST HAVE section for last class
pre-cluster-threshold=0.40
nms-iou-threshold=0.45
topk=100

# ‚ùå INCORRECT: Missing class sections
[property]
num-detected-classes=5

[class-attrs-0]
pre-cluster-threshold=0.25

# ‚ùå Missing: [class-attrs-1] through [class-attrs-4] ‚Üí Undefined behavior!
```

---

## Performance Optimization

### 8. Batch Size Tuning

**Rule:** nvstreammux batch-size = number of sources OR nvinfer batch-size

**Best Practices:**
```
# ‚úÖ CORRECT: Matched batch sizes
nvstreammux batch-size=2 (2 cameras)
  ‚Üí my_steach (stitches to 1 panorama)
  ‚Üí my_tile_batcher (creates batch of 6 tiles)
  ‚Üí nvinfer batch-size=6 (processes 6 tiles together)

# ‚ùå INCORRECT: Mismatched batch sizes
nvstreammux batch-size=6  # ‚ùå Only 2 sources!
nvinfer batch-size=2      # ‚ùå Receiving 6-tile batch!
```

### 9. Memory Bandwidth Optimization

**Jetson Constraint:** 102 GB/s shared bandwidth

**Strategies:**
1. Avoid CPU‚ÜîGPU copies (use NVMM throughout)
2. Use hardware-accelerated conversions (nvvideoconvert compute-hw=1)
3. Avoid redundant format conversions
4. Process in native camera format when possible

---

## Error Handling

### 10. GStreamer State Management

**Best Practices:**
```python
# ‚úÖ CORRECT: Proper state transitions
def stop_pipeline(pipeline):
    """Safely stop GStreamer pipeline."""

    # Send EOS
    pipeline.send_event(Gst.Event.new_eos())

    # Wait for EOS message
    bus = pipeline.get_bus()
    msg = bus.timed_pop_filtered(
        5 * Gst.SECOND,
        Gst.MessageType.EOS | Gst.MessageType.ERROR
    )

    if msg:
        if msg.type == Gst.MessageType.ERROR:
            err, debug = msg.parse_error()
            logging.error(f"Pipeline error: {err.message}")

    # Set to NULL state
    pipeline.set_state(Gst.State.NULL)

    # Wait for state change
    ret, state, pending = pipeline.get_state(5 * Gst.SECOND)
    if ret != Gst.StateChangeReturn.SUCCESS:
        logging.error("Failed to stop pipeline cleanly")

# ‚ùå INCORRECT: Abrupt stop
def stop_bad(pipeline):
    pipeline.set_state(Gst.State.NULL)  # ‚ùå No EOS, no cleanup!
```

---

**References:**
- DeepStream 7.1 Plugin Development Guide
- DeepStream 7.1 Python API Reference
- DeepStream FAQ (docs/ds_doc/7.1/text/DS_FAQ.html)
- NVIDIA Developer Forums

**Last Updated:** 2025-11-19
